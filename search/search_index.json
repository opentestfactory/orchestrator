{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"OpenTestFactory Orchestrator Documentation \u00b6 Automate, customize, and execute your testing workflows right in your repository with the OpenTestFactory orchestrator. You can discover, create, and share plugins to perform any job you\u2019d like, and combine plugins in a completely customized workflow. Quickstart Reference guides Getting started \u00b6 About OpenTestFactory orchestrator \u00b6 The orchestrator enables you to create custom software development life cycle (SDLC) workflows directly in your source code repository. About OpenTestFactory orchestrator plugins \u00b6 Generators and providers plugins are individual tasks that you can combine to create jobs and customize your workflow. You can create your own generators and providers plugins or use and customize plugins shared by the OpenTestFactory community. Popular articles \u00b6 Installation \u00b6 The OpenTestFactory orchestrator is a set of services running together. They may or may not run on the same machine, and they may or may not be started at the same time. Workflow Syntax \u00b6 A workflow is a configurable automated process made up of one or more jobs. You must create a YAML file to define your workflow configuration. Context and expressions syntax \u00b6 You can access context information and evaluate expressions in workflows and plugins.","title":"Home"},{"location":"#opentestfactory-orchestrator-documentation","text":"Automate, customize, and execute your testing workflows right in your repository with the OpenTestFactory orchestrator. You can discover, create, and share plugins to perform any job you\u2019d like, and combine plugins in a completely customized workflow. Quickstart Reference guides","title":"OpenTestFactory Orchestrator Documentation"},{"location":"#getting-started","text":"","title":"Getting started"},{"location":"#about-opentestfactory-orchestrator","text":"The orchestrator enables you to create custom software development life cycle (SDLC) workflows directly in your source code repository.","title":"About OpenTestFactory orchestrator"},{"location":"#about-opentestfactory-orchestrator-plugins","text":"Generators and providers plugins are individual tasks that you can combine to create jobs and customize your workflow. You can create your own generators and providers plugins or use and customize plugins shared by the OpenTestFactory community.","title":"About OpenTestFactory orchestrator plugins"},{"location":"#popular-articles","text":"","title":"Popular articles"},{"location":"#installation","text":"The OpenTestFactory orchestrator is a set of services running together. They may or may not run on the same machine, and they may or may not be started at the same time.","title":"Installation"},{"location":"#workflow-syntax","text":"A workflow is a configurable automated process made up of one or more jobs. You must create a YAML file to define your workflow configuration.","title":"Workflow Syntax"},{"location":"#context-and-expressions-syntax","text":"You can access context information and evaluate expressions in workflows and plugins.","title":"Context and expressions syntax"},{"location":"installation/","text":"Orchestrator Installation Documentation \u00b6 Getting started \u00b6 The OpenTestFactory orchestrator is a set of services running together. They may or may not run on the same machine, and they may or may not be started at the same time. The only prerequisite is that the EventBus , the service they use to communicate together, is available when they launch. To ease the installation of the orchestrator, an \u2018all-in-one\u2019 docker image is provided. It contains all core services. The most current stable image is opentestfactory/operator:latest . If you want a specific version, you can use a specific tag such as 0.10.3-master . To get the latest image, use the following command: docker pull opentestfactory/operator:latest Configuring the \u2018all-in-one\u2019 image \u00b6 Three things must be configured if you want to use the \u2018all-in-one\u2019 image: Startup Trusted Keys Plugins Once this configuration is done, you can run the image and the orchestrator will be ready to use. Here is a simple command that will start the orchestrator so that it can use one existing execution environment , with self-generated trusted keys (which is not recommended in a production setup): docker run -d \\ --name orchestrator \\ -p 7774 :7774 \\ -p 7775 :7775 \\ -p 7776 :7776 \\ -p 38368 :38368 \\ -e SSH_CHANNEL_HOST = the_environment_ip_or_hostname \\ -e SSH_CHANNEL_USER = user \\ -e SSH_CHANNEL_PASSWORD = secret \\ -e SSH_CHANNEL_TAGS = ssh,linux,robotframework \\ opentestfactory/orchestrator:latest It exposes the following services on the corresponding ports: receptionist (port 7774) observer (port 7775) killswitch (port 7776) eventbus (port 38368) The orchestrator runs until one service fails or ends. Startup \u00b6 By default, the \u2018all-in-one\u2019 image will start the core services and all plugins it can find in the core and in the /app/plugins directory (and sub-directories). Some demonstration plugins that are part of the core are disabled by default: dummyee HelloWorld localpublisher You can override those defaults by providing an alternative /app/squashtf.yaml configuration file. Here is the default one: # squashtf.yaml eventbus : python3 -m squashtf.core.eventbus services : - ${{ CORE }}/core plugins : - ${{ CORE }}/plugins - /app/plugins disabled : - dummyee - HelloWorld - localpublisher If you have plugins installed in another location, add this location in the plugins section. If you want to enable or disable some of those plugins, remove or add their name in the disabled section. Trusted Keys \u00b6 To send requests to the exposed services, you need a signed JWT token. You can either provide your trusted key(s) or let the orchestrator create a JWT token for you. (It is not recommended to use this created JWT token in a production environment.) Getting the created JWT token \u00b6 A unique JWT token is created only if no trusted keys are provided. The orchestrator will generate a temporary private key, create and sign a JWT token using it, and then use the corresponding public key as its only trusted key. The temporary private key is not kept anywhere. The created JWT token is displayed in the logs of the orchestrator. Look for \"Creating temporary JWT token\" : docker logs orchestrator | grep --after-context = 10 \"Creating temporary JWT token\" A different JWT token will be created each time the orchestrator is started. It is not recommended to use this created JWT token in a production environment. Providing your own trusted keys \u00b6 You can either use existing private key(s) or generate new private key(s) to create and sign your JWT tokens. You provide your public key(s) in the /etc/squashtf directory. The easiest way is to put them in a volume and mount it on /etc/squashtf . Here is an example mounting a single trusted_key.pub public key: docker run ... -v /path/to/trusted/key/trusted_key.pub:/etc/squashtf/squashtf.pub ... The orchestrator will validate tokens according to those public keys (it will try each key, if more than one is supplied, until it finds one that validate the token). It uses the sub and exp claims in the payload and rejects tokens that are past their expiration time if one is specified in the token. The following command will generate a private key and its corresponding public key: openssl genrsa -out trusted_key.pem 4096 openssl rsa -pubout -in trusted_key.pem -out trusted_key.pub To create JWT tokens from this key, you can use the following Python script (or any other JWT token creator you are familiar with): import jwt # pip install PyJWT[crypto] with open ( 'trusted_key.pem' , 'r' ) as f : pem = f . read () with open ( 'trusted_key.pub' , 'r' ) as f : pub = f . read () # create a signed token token = jwt . encode ({ 'iss' : 'you' , 'sub' : 'me' }, pem , algorithm = 'RS512' ) print ( token ) # verify it payload = jwt . decode ( token , pub , algorithms = [ 'RS512' ]) print ( payload ) Plugins \u00b6 Most plugins do not need specific configuration, but those that must access external resources do. In the core plugins, the SSH channel plugin (and possibly the S3 publisher plugin if you use it) must be configured. Here are their respective detailed configuration documentations: SSH Channel Configuration S3 Publisher Configuration SSH Channel Configuration \u00b6 If you only have one execution environment, you can simply specify it through environment variables. It must be accessible via SSH. SSH_CHANNEL_HOST : required, either a hostname or an IP address. SSH_CHANNEL_PORT : optional, the port number (22 by default). SSH_CHANNEL_USER : required, the user to use to log to the execution environment. SSH_CHANNEL_PASSWORD : required, the corresponding password. SSH_CHANNEL_TAGS : required, a comma-separated list of tags this environment can handle. If you have more than one execution environment, you will have to provide a pools definition. SSH_CHANNEL_POOLS : optional, a path to the pools definitions. If SSH_CHANNEL_POOLS is set, it must point to a YAML file which will look like: pools : demo : - host : demo.example.com username : demo password : 1234 tags : [ ssh , windows ] demo2 : - host : host.example.com port : 22 username : alice ssh_host_keys : /data/ssh/known_hosts key_filename : /data/ssh/example.pem missing_host_key_policy : reject tags : [ ssh , linux ] - hosts : [ foo.example.com , bar.example.com ] port : 22 username : bob ssh_host_keys : /data/ssh/known_hosts key_filename : /data/ssh/secret.pem passphrase : secret missing_host_key_policy : auto-add tags : [ ssh , linux ] Please refer to \u201c SSH Channel Configuration \u201d for more information on pools. If you specify both an execution environment and a pools definition, they will be merged, the specified execution environment possibly overriding an existing item in the pool (same host/port). S3 Publisher Configuration \u00b6 If you intend to use the S3 Publisher plugin (so that your tests results are uploaded to a S3 bucket), you must provide a s3credentials.json configuration file and mount it as /app/s3publisher/s3credentials.json . It can work with any S3-compatible provider (AWS of course, but also Scaleway and others). You must provide the four following entries: region_name , endpoint_url , aws_access_key_id , and aws_secret_access_key . You can also specify bucket (if you do not, results will be published to a \"squash_orchestrator\" bucket). The s3credentials.json file you provide will looks like this: { \"region_name\" : \"fr-par\" , \"endpoint_url\" : \"https://s3.fr-par.scw.cloud\" , \"aws_access_key_id\" : \"access_key\" , \"aws_secret_access_key\" : \"secret_access_key\" , \"bucket\" : \"my_bucket\" } Here is an example mounting a my_s3_credentials.json configuration file: docker run ... -v /path/to/my_s3credentials.json:/app/s3publisher/s3credentials.json ... Testing your deployment \u00b6 Assuming you have deployed and configured a Robotframework execution environment, you can run the following workflow to ensure everything is ok. Put this in a robotdemo.yaml file: apiVersion : opentestfactory.org/v1alpha1 kind : Workflow metadata : name : RobotFramework Example variables : SERVER : production jobs : keyword-driven : runs-on : [ ssh , robotframework ] steps : - run : echo $SERVER - uses : actions/checkout@v2 with : repository : https://github.com/robotframework/RobotDemo.git - run : 'ls -al' working-directory : RobotDemo - uses : robotframework/robot@v1 with : datasource : RobotDemo/keyword_driven.robot data-driven : runs-on : [ ssh , robotframework ] name : Data driven tests steps : - uses : actions/checkout@v2 with : repository : https://github.com/robotframework/RobotDemo.git - uses : robotframework/robot@v1 with : datasource : RobotDemo/data_driven.robot Then run your workflow: curl -X POST --data-binary @robotdemo.yaml -H \"Authorization: Bearer <yourtoken>\" -H \"Content-type: application/x-yaml\" https://<ip>/workflows","title":"Installation"},{"location":"installation/#orchestrator-installation-documentation","text":"","title":"Orchestrator Installation Documentation"},{"location":"installation/#getting-started","text":"The OpenTestFactory orchestrator is a set of services running together. They may or may not run on the same machine, and they may or may not be started at the same time. The only prerequisite is that the EventBus , the service they use to communicate together, is available when they launch. To ease the installation of the orchestrator, an \u2018all-in-one\u2019 docker image is provided. It contains all core services. The most current stable image is opentestfactory/operator:latest . If you want a specific version, you can use a specific tag such as 0.10.3-master . To get the latest image, use the following command: docker pull opentestfactory/operator:latest","title":"Getting started"},{"location":"installation/#configuring-the-all-in-one-image","text":"Three things must be configured if you want to use the \u2018all-in-one\u2019 image: Startup Trusted Keys Plugins Once this configuration is done, you can run the image and the orchestrator will be ready to use. Here is a simple command that will start the orchestrator so that it can use one existing execution environment , with self-generated trusted keys (which is not recommended in a production setup): docker run -d \\ --name orchestrator \\ -p 7774 :7774 \\ -p 7775 :7775 \\ -p 7776 :7776 \\ -p 38368 :38368 \\ -e SSH_CHANNEL_HOST = the_environment_ip_or_hostname \\ -e SSH_CHANNEL_USER = user \\ -e SSH_CHANNEL_PASSWORD = secret \\ -e SSH_CHANNEL_TAGS = ssh,linux,robotframework \\ opentestfactory/orchestrator:latest It exposes the following services on the corresponding ports: receptionist (port 7774) observer (port 7775) killswitch (port 7776) eventbus (port 38368) The orchestrator runs until one service fails or ends.","title":"Configuring the 'all-in-one' image"},{"location":"installation/#startup","text":"By default, the \u2018all-in-one\u2019 image will start the core services and all plugins it can find in the core and in the /app/plugins directory (and sub-directories). Some demonstration plugins that are part of the core are disabled by default: dummyee HelloWorld localpublisher You can override those defaults by providing an alternative /app/squashtf.yaml configuration file. Here is the default one: # squashtf.yaml eventbus : python3 -m squashtf.core.eventbus services : - ${{ CORE }}/core plugins : - ${{ CORE }}/plugins - /app/plugins disabled : - dummyee - HelloWorld - localpublisher If you have plugins installed in another location, add this location in the plugins section. If you want to enable or disable some of those plugins, remove or add their name in the disabled section.","title":"Startup"},{"location":"installation/#trusted-keys","text":"To send requests to the exposed services, you need a signed JWT token. You can either provide your trusted key(s) or let the orchestrator create a JWT token for you. (It is not recommended to use this created JWT token in a production environment.)","title":"Trusted Keys"},{"location":"installation/#getting-the-created-jwt-token","text":"A unique JWT token is created only if no trusted keys are provided. The orchestrator will generate a temporary private key, create and sign a JWT token using it, and then use the corresponding public key as its only trusted key. The temporary private key is not kept anywhere. The created JWT token is displayed in the logs of the orchestrator. Look for \"Creating temporary JWT token\" : docker logs orchestrator | grep --after-context = 10 \"Creating temporary JWT token\" A different JWT token will be created each time the orchestrator is started. It is not recommended to use this created JWT token in a production environment.","title":"Getting the created JWT token"},{"location":"installation/#providing-your-own-trusted-keys","text":"You can either use existing private key(s) or generate new private key(s) to create and sign your JWT tokens. You provide your public key(s) in the /etc/squashtf directory. The easiest way is to put them in a volume and mount it on /etc/squashtf . Here is an example mounting a single trusted_key.pub public key: docker run ... -v /path/to/trusted/key/trusted_key.pub:/etc/squashtf/squashtf.pub ... The orchestrator will validate tokens according to those public keys (it will try each key, if more than one is supplied, until it finds one that validate the token). It uses the sub and exp claims in the payload and rejects tokens that are past their expiration time if one is specified in the token. The following command will generate a private key and its corresponding public key: openssl genrsa -out trusted_key.pem 4096 openssl rsa -pubout -in trusted_key.pem -out trusted_key.pub To create JWT tokens from this key, you can use the following Python script (or any other JWT token creator you are familiar with): import jwt # pip install PyJWT[crypto] with open ( 'trusted_key.pem' , 'r' ) as f : pem = f . read () with open ( 'trusted_key.pub' , 'r' ) as f : pub = f . read () # create a signed token token = jwt . encode ({ 'iss' : 'you' , 'sub' : 'me' }, pem , algorithm = 'RS512' ) print ( token ) # verify it payload = jwt . decode ( token , pub , algorithms = [ 'RS512' ]) print ( payload )","title":"Providing your own trusted keys"},{"location":"installation/#plugins","text":"Most plugins do not need specific configuration, but those that must access external resources do. In the core plugins, the SSH channel plugin (and possibly the S3 publisher plugin if you use it) must be configured. Here are their respective detailed configuration documentations: SSH Channel Configuration S3 Publisher Configuration","title":"Plugins"},{"location":"installation/#ssh-channel-configuration","text":"If you only have one execution environment, you can simply specify it through environment variables. It must be accessible via SSH. SSH_CHANNEL_HOST : required, either a hostname or an IP address. SSH_CHANNEL_PORT : optional, the port number (22 by default). SSH_CHANNEL_USER : required, the user to use to log to the execution environment. SSH_CHANNEL_PASSWORD : required, the corresponding password. SSH_CHANNEL_TAGS : required, a comma-separated list of tags this environment can handle. If you have more than one execution environment, you will have to provide a pools definition. SSH_CHANNEL_POOLS : optional, a path to the pools definitions. If SSH_CHANNEL_POOLS is set, it must point to a YAML file which will look like: pools : demo : - host : demo.example.com username : demo password : 1234 tags : [ ssh , windows ] demo2 : - host : host.example.com port : 22 username : alice ssh_host_keys : /data/ssh/known_hosts key_filename : /data/ssh/example.pem missing_host_key_policy : reject tags : [ ssh , linux ] - hosts : [ foo.example.com , bar.example.com ] port : 22 username : bob ssh_host_keys : /data/ssh/known_hosts key_filename : /data/ssh/secret.pem passphrase : secret missing_host_key_policy : auto-add tags : [ ssh , linux ] Please refer to \u201c SSH Channel Configuration \u201d for more information on pools. If you specify both an execution environment and a pools definition, they will be merged, the specified execution environment possibly overriding an existing item in the pool (same host/port).","title":"SSH Channel Configuration"},{"location":"installation/#s3-publisher-configuration","text":"If you intend to use the S3 Publisher plugin (so that your tests results are uploaded to a S3 bucket), you must provide a s3credentials.json configuration file and mount it as /app/s3publisher/s3credentials.json . It can work with any S3-compatible provider (AWS of course, but also Scaleway and others). You must provide the four following entries: region_name , endpoint_url , aws_access_key_id , and aws_secret_access_key . You can also specify bucket (if you do not, results will be published to a \"squash_orchestrator\" bucket). The s3credentials.json file you provide will looks like this: { \"region_name\" : \"fr-par\" , \"endpoint_url\" : \"https://s3.fr-par.scw.cloud\" , \"aws_access_key_id\" : \"access_key\" , \"aws_secret_access_key\" : \"secret_access_key\" , \"bucket\" : \"my_bucket\" } Here is an example mounting a my_s3_credentials.json configuration file: docker run ... -v /path/to/my_s3credentials.json:/app/s3publisher/s3credentials.json ...","title":"S3 Publisher Configuration"},{"location":"installation/#testing-your-deployment","text":"Assuming you have deployed and configured a Robotframework execution environment, you can run the following workflow to ensure everything is ok. Put this in a robotdemo.yaml file: apiVersion : opentestfactory.org/v1alpha1 kind : Workflow metadata : name : RobotFramework Example variables : SERVER : production jobs : keyword-driven : runs-on : [ ssh , robotframework ] steps : - run : echo $SERVER - uses : actions/checkout@v2 with : repository : https://github.com/robotframework/RobotDemo.git - run : 'ls -al' working-directory : RobotDemo - uses : robotframework/robot@v1 with : datasource : RobotDemo/keyword_driven.robot data-driven : runs-on : [ ssh , robotframework ] name : Data driven tests steps : - uses : actions/checkout@v2 with : repository : https://github.com/robotframework/RobotDemo.git - uses : robotframework/robot@v1 with : datasource : RobotDemo/data_driven.robot Then run your workflow: curl -X POST --data-binary @robotdemo.yaml -H \"Authorization: Bearer <yourtoken>\" -H \"Content-type: application/x-yaml\" https://<ip>/workflows","title":"Testing your deployment"},{"location":"quickstart/","text":"Quickstart for OpenTestFactory Orchestrator \u00b6 Add an OpenTestFactory Orchestrator workflow to an existing repository in 5 minutes or less. Introduction \u00b6 You only need an existing OpenTestFactory Orchestrator service to create and run a workflow. In this guide, you will add a workflow that run tests using the robotframework/robot action. The workflow uses Robotframework to run a set of tests in an existing execution environment . Installation \u00b6 Prepare a Robotframework execution environment docker run -d \\ --name robotenv \\ --env PASSWORD_ACCESS = true \\ --env USER_NAME = otf \\ --env USER_PASSWORD = secret \\ squashtest/agent-robotframework:latest Prepare an OpenTestFactory Orchestrator that is linked to this execution environment docker run -d \\ --name orchestrator \\ --link = robotenv \\ -p 7774 :7774 \\ -e SSH_CHANNEL_HOST = robotenv \\ -e SSH_CHANNEL_USER = otf \\ -e SSH_CHANNEL_PASSWORD = secret \\ -e SSH_CHANNEL_TAGS = ssh,linux,robotframework \\ -e SSH_CHANNEL_PORT = 2222 \\ squashtest/squash-orchestrator:latest Get the generated token docker logs orchestrator | grep --after-context = 10 \"Creating temporary JWT token\" Your token will be displayed, surrounded by quotes: In this example, the token starts with eyJ and ends with RPU . It will be different for you. Create your first workflow \u00b6 Clone the robotframework/RobotDemo repository from GitHub. git clone https://github.com/robotframework/RobotDemo.git In this repository, create a new file at the top named Squashfile . For more information, refer to your git documentation. cd RobotDemo touch Squashfile Copy the following YAML content in the Squashfile file. metadata : name : RobotFramework Example variables : SERVER : production jobs : # Set the job key. The key is displayed as the job name when # a job name is not provided keyword-driven : runs-on : robotframework steps : - run : echo $SERVER - uses : actions/checkout@v2 with : repository : https://github.com/robotframework/RobotDemo.git - uses : robotframework/robot@v1 with : datasource : RobotDemo/keyword_driven.robot To run your workflow, run the following command, replacing <yourtoken> by the token you got at installation time. curl -X POST \\ --data-binary @Squashfile \\ -H \"Authorization: Bearer <yourtoken>\" \\ -H \"Content-type: application/x-yaml\" \\ http://localhost:7774/workflows The orchestrator will then execute your workflow. View your workflow results \u00b6 Check the logs of the orchestrator. This is a place where you observe your workflow executions. docker logs orchestrator In a more realistic case, you would upload your tests execution reports to either a test management system or a shared directory. More workflow templates \u00b6 The OpenTestFactory provides preconfigured workflow templates that you can customize to create your own testing workflows. You can browse the full list of workflow templates in the opentestfactory/templates repository. Next steps \u00b6 The robotframework workflow you just added can be integrated in your CI/CD toolchain to run any time code is pushed to your repository to help you spot errors and inconsistencies in your code. But this is only the beginning of what you can do with the OpenTestFactory orchestrator. Your repository can contain multiple workflows that trigger different jobs based on different events. The orchestrator can help you automate nearly every aspect of your applicating test processes. Ready to get started? Here are some helpful resources for taking your next steps with the OpenTestFactory orchestrator: \u201c Learn OpenTestFactory Orchestrator \u201d for an in-depth tutorial \u201cGuides\u201d for specific uses cases and examples robotframework/robot for more details about configuring the Robotframework action","title":"Quick Start"},{"location":"quickstart/#quickstart-for-opentestfactory-orchestrator","text":"Add an OpenTestFactory Orchestrator workflow to an existing repository in 5 minutes or less.","title":"Quickstart for OpenTestFactory Orchestrator"},{"location":"quickstart/#introduction","text":"You only need an existing OpenTestFactory Orchestrator service to create and run a workflow. In this guide, you will add a workflow that run tests using the robotframework/robot action. The workflow uses Robotframework to run a set of tests in an existing execution environment .","title":"Introduction"},{"location":"quickstart/#installation","text":"Prepare a Robotframework execution environment docker run -d \\ --name robotenv \\ --env PASSWORD_ACCESS = true \\ --env USER_NAME = otf \\ --env USER_PASSWORD = secret \\ squashtest/agent-robotframework:latest Prepare an OpenTestFactory Orchestrator that is linked to this execution environment docker run -d \\ --name orchestrator \\ --link = robotenv \\ -p 7774 :7774 \\ -e SSH_CHANNEL_HOST = robotenv \\ -e SSH_CHANNEL_USER = otf \\ -e SSH_CHANNEL_PASSWORD = secret \\ -e SSH_CHANNEL_TAGS = ssh,linux,robotframework \\ -e SSH_CHANNEL_PORT = 2222 \\ squashtest/squash-orchestrator:latest Get the generated token docker logs orchestrator | grep --after-context = 10 \"Creating temporary JWT token\" Your token will be displayed, surrounded by quotes: In this example, the token starts with eyJ and ends with RPU . It will be different for you.","title":"Installation"},{"location":"quickstart/#create-your-first-workflow","text":"Clone the robotframework/RobotDemo repository from GitHub. git clone https://github.com/robotframework/RobotDemo.git In this repository, create a new file at the top named Squashfile . For more information, refer to your git documentation. cd RobotDemo touch Squashfile Copy the following YAML content in the Squashfile file. metadata : name : RobotFramework Example variables : SERVER : production jobs : # Set the job key. The key is displayed as the job name when # a job name is not provided keyword-driven : runs-on : robotframework steps : - run : echo $SERVER - uses : actions/checkout@v2 with : repository : https://github.com/robotframework/RobotDemo.git - uses : robotframework/robot@v1 with : datasource : RobotDemo/keyword_driven.robot To run your workflow, run the following command, replacing <yourtoken> by the token you got at installation time. curl -X POST \\ --data-binary @Squashfile \\ -H \"Authorization: Bearer <yourtoken>\" \\ -H \"Content-type: application/x-yaml\" \\ http://localhost:7774/workflows The orchestrator will then execute your workflow.","title":"Create your first workflow"},{"location":"quickstart/#view-your-workflow-results","text":"Check the logs of the orchestrator. This is a place where you observe your workflow executions. docker logs orchestrator In a more realistic case, you would upload your tests execution reports to either a test management system or a shared directory.","title":"View your workflow results"},{"location":"quickstart/#more-workflow-templates","text":"The OpenTestFactory provides preconfigured workflow templates that you can customize to create your own testing workflows. You can browse the full list of workflow templates in the opentestfactory/templates repository.","title":"More workflow templates"},{"location":"quickstart/#next-steps","text":"The robotframework workflow you just added can be integrated in your CI/CD toolchain to run any time code is pushed to your repository to help you spot errors and inconsistencies in your code. But this is only the beginning of what you can do with the OpenTestFactory orchestrator. Your repository can contain multiple workflows that trigger different jobs based on different events. The orchestrator can help you automate nearly every aspect of your applicating test processes. Ready to get started? Here are some helpful resources for taking your next steps with the OpenTestFactory orchestrator: \u201c Learn OpenTestFactory Orchestrator \u201d for an in-depth tutorial \u201cGuides\u201d for specific uses cases and examples robotframework/robot for more details about configuring the Robotframework action","title":"Next steps"},{"location":"guides/agent/","text":"Agents \u00b6 Workflow jobs steps run on execution environments. There are currently two ways the orchestrator can communicate with execution environments: SSH and agents. SSH is the common choice if the execution environment is based on Linux or MacOS. It can also be used on Windows, but there are some limitations if the actions to perform require a graphic session. Agents are an alternative that do not suffer from the above limitation, and so are more commonly seen in Windows contexts. Agents may also be used in places where the execution environments change often, or are not allowed to include a SSH server. Finally, agents initiate the conversation with the orchestrator, which may be prefered in some contexts: there is no need to have an open port so that the orchestrator can run commands on the execution environment. Overview \u00b6 An agent is a process that runs on an execution environment. That process will contact the orchestrator at regular intervals, looking for some orders to execute. If there is a pending order, the agent will do as asked and send the result back to the orchestrator. There can be any number of agents talking with a given orchestrator. But, typically, on a given execution environment, there is at most one agent running. Not all execution environments have agents running on them. If the execution environment can be accessed via SSH, this may be used instead. Installation \u00b6 The OpenTestFactory agent is a Python application that is installed in the execution environment. It requires Python 3.8 or higher. It works on Linux, MacOS, and Windows. As it is a simple script it may also work on other operating systems. It only has one external dependency, the well-known requests library (it will be installed if not already present on the execution environment). To install it, use the following command: pip3 install --upgrade opentf-agent You can test your installation by running the following command: opentf-agent --help Usage \u00b6 Summary \u00b6 $ opentf-agent --help usage: opentf-agent [-h] --tags TAGS --host HOST [--port PORT] [--path_prefix PATH_PREFIX] [--token TOKEN] [--encoding ENCODING] [--script_path SCRIPT_PATH] [--workspace_dir WORKSPACE_DIR] [--name NAME] [--polling_delay POLLING_DELAY] [--liveness_probe LIVENESS_PROBE] [--retry RETRY] [--debug] OpenTestFactory Agent optional arguments: -h, --help show this help message and exit --tags TAGS a comma-separated list of tags (e.g. windows,robotframework) --host HOST target host with protocol (e.g., https://example.local) --port PORT target port (default to 24368) --path_prefix PATH_PREFIX target context path (default to no context path) --token TOKEN token --encoding ENCODING encoding on the console side (defaults to utf-8) --script_path SCRIPT_PATH where to put temporary files (defaults to current directory) --workspace_dir WORKSPACE_DIR where to put workspaces (defaults to current directory) --name NAME agent name (defaults to \"test agent\") --polling_delay POLLING_DELAY polling delay in seconds (default to 5) --liveness_probe LIVENESS_PROBE liveness probe in seconds (default to 300 seconds) --retry RETRY how many times to try joining host (default to 5, 0 = try forever) --debug whether to log debug information. Required parameters \u00b6 There are two required parameters, --tags and --host . --tags is a comma-separated list of tags. One of those tags MUST be one and only one of linux , macos , or windows . There may be any number of other tags. --host is the orchestrator this agent will register to. It is a fully qualified domain name (or an IP address), and must start with a protocol, typically https:// . Optional parameters \u00b6 If the port the orchestrator listen to for agent registration differs from the standard one (24368), you can use the --port option to adjust it. If the orchestrator does not have its own domain name but used a shared fully qualified name, you can specify its path prefix using the --path_prefix option. Stopping the agent \u00b6 You can stop a running agent by killing it or interrupting it ( ^C ). It will try to deregister from the orchestrator. If the deregistration is successful, there will be a Agent successfully deregistered message in the console. If the deregistration fails, the orchestrator may still consider the agent to be available for a limited amount of time (as specified by --liveness_probe ), which may cause jobs targeting a compatible execution environment to time-out. Examples \u00b6 Example 1 \u00b6 Assuming there is an OpenTestFactory orchestrator running on orchestrator.example.com , with a known token stored in the TOKEN environment variable, the following command will register the Windows-based execution environment and will possibly receive commands targeting windows and/or robotframework: chcp 65001 opentf-agent --tags windows,robotframework --host https://orchestrator.example.com/ --token %TOKEN% The agent will poll the orchestrator every 5 seconds and will execute the received commands. The chcp command sets the console to Unicode. It is Windows-specific. It is not mandatory but may be needed depending on the test framework available in the execution environment. Temporary files will be generated in the current directory, as well as workspace directories. Example 2 \u00b6 Assuming again the existence of an OpenTestFactory orchestrator running on orchestrator.example.com , listening for agent registrations on port 12345, with a known token stored in the TOKEN environment variable. The following command will register the Linux-based execution environment and will poll the orchestrator every seconds. Il will accept jobs targeting linux , and/or junit , and/or robotframework . Scripts will be temporarily stored on /tmp , and workspaces will be allocated on /var/opentf/workspace . opentf-agent \\ --name \"all in one execution environment\" \\ --tags linux,junit,cypress \\ --host https://orchestrator.example.com \\ --port 12345 \\ --token $TOKEN \\ --script_path /tmp \\ --workspace_dir /var/opentf/workspace \\ --polling_delay 1 Troubleshouting \u00b6 You can list the registered agents on a given orchestrator by running the following command: curl \\ -H \"Authorization: bearer $TOKEN \" \\ https://orchestrator.example.com:24368/agents You can deregister a rogue agent on a given orchestrator by running the followig command: curl \\ -X DELETE \\ -H \"Authorization: bearer $TOKEN \" \\ https://orchestrator.example.com:24368/agents/<agent_id> where <agent_id> is the UUID of the agent as shown by the previous command.","title":"Agents"},{"location":"guides/agent/#agents","text":"Workflow jobs steps run on execution environments. There are currently two ways the orchestrator can communicate with execution environments: SSH and agents. SSH is the common choice if the execution environment is based on Linux or MacOS. It can also be used on Windows, but there are some limitations if the actions to perform require a graphic session. Agents are an alternative that do not suffer from the above limitation, and so are more commonly seen in Windows contexts. Agents may also be used in places where the execution environments change often, or are not allowed to include a SSH server. Finally, agents initiate the conversation with the orchestrator, which may be prefered in some contexts: there is no need to have an open port so that the orchestrator can run commands on the execution environment.","title":"Agents"},{"location":"guides/agent/#overview","text":"An agent is a process that runs on an execution environment. That process will contact the orchestrator at regular intervals, looking for some orders to execute. If there is a pending order, the agent will do as asked and send the result back to the orchestrator. There can be any number of agents talking with a given orchestrator. But, typically, on a given execution environment, there is at most one agent running. Not all execution environments have agents running on them. If the execution environment can be accessed via SSH, this may be used instead.","title":"Overview"},{"location":"guides/agent/#installation","text":"The OpenTestFactory agent is a Python application that is installed in the execution environment. It requires Python 3.8 or higher. It works on Linux, MacOS, and Windows. As it is a simple script it may also work on other operating systems. It only has one external dependency, the well-known requests library (it will be installed if not already present on the execution environment). To install it, use the following command: pip3 install --upgrade opentf-agent You can test your installation by running the following command: opentf-agent --help","title":"Installation"},{"location":"guides/agent/#usage","text":"","title":"Usage"},{"location":"guides/agent/#summary","text":"$ opentf-agent --help usage: opentf-agent [-h] --tags TAGS --host HOST [--port PORT] [--path_prefix PATH_PREFIX] [--token TOKEN] [--encoding ENCODING] [--script_path SCRIPT_PATH] [--workspace_dir WORKSPACE_DIR] [--name NAME] [--polling_delay POLLING_DELAY] [--liveness_probe LIVENESS_PROBE] [--retry RETRY] [--debug] OpenTestFactory Agent optional arguments: -h, --help show this help message and exit --tags TAGS a comma-separated list of tags (e.g. windows,robotframework) --host HOST target host with protocol (e.g., https://example.local) --port PORT target port (default to 24368) --path_prefix PATH_PREFIX target context path (default to no context path) --token TOKEN token --encoding ENCODING encoding on the console side (defaults to utf-8) --script_path SCRIPT_PATH where to put temporary files (defaults to current directory) --workspace_dir WORKSPACE_DIR where to put workspaces (defaults to current directory) --name NAME agent name (defaults to \"test agent\") --polling_delay POLLING_DELAY polling delay in seconds (default to 5) --liveness_probe LIVENESS_PROBE liveness probe in seconds (default to 300 seconds) --retry RETRY how many times to try joining host (default to 5, 0 = try forever) --debug whether to log debug information.","title":"Summary"},{"location":"guides/agent/#required-parameters","text":"There are two required parameters, --tags and --host . --tags is a comma-separated list of tags. One of those tags MUST be one and only one of linux , macos , or windows . There may be any number of other tags. --host is the orchestrator this agent will register to. It is a fully qualified domain name (or an IP address), and must start with a protocol, typically https:// .","title":"Required parameters"},{"location":"guides/agent/#optional-parameters","text":"If the port the orchestrator listen to for agent registration differs from the standard one (24368), you can use the --port option to adjust it. If the orchestrator does not have its own domain name but used a shared fully qualified name, you can specify its path prefix using the --path_prefix option.","title":"Optional parameters"},{"location":"guides/agent/#stopping-the-agent","text":"You can stop a running agent by killing it or interrupting it ( ^C ). It will try to deregister from the orchestrator. If the deregistration is successful, there will be a Agent successfully deregistered message in the console. If the deregistration fails, the orchestrator may still consider the agent to be available for a limited amount of time (as specified by --liveness_probe ), which may cause jobs targeting a compatible execution environment to time-out.","title":"Stopping the agent"},{"location":"guides/agent/#examples","text":"","title":"Examples"},{"location":"guides/agent/#example-1","text":"Assuming there is an OpenTestFactory orchestrator running on orchestrator.example.com , with a known token stored in the TOKEN environment variable, the following command will register the Windows-based execution environment and will possibly receive commands targeting windows and/or robotframework: chcp 65001 opentf-agent --tags windows,robotframework --host https://orchestrator.example.com/ --token %TOKEN% The agent will poll the orchestrator every 5 seconds and will execute the received commands. The chcp command sets the console to Unicode. It is Windows-specific. It is not mandatory but may be needed depending on the test framework available in the execution environment. Temporary files will be generated in the current directory, as well as workspace directories.","title":"Example 1"},{"location":"guides/agent/#example-2","text":"Assuming again the existence of an OpenTestFactory orchestrator running on orchestrator.example.com , listening for agent registrations on port 12345, with a known token stored in the TOKEN environment variable. The following command will register the Linux-based execution environment and will poll the orchestrator every seconds. Il will accept jobs targeting linux , and/or junit , and/or robotframework . Scripts will be temporarily stored on /tmp , and workspaces will be allocated on /var/opentf/workspace . opentf-agent \\ --name \"all in one execution environment\" \\ --tags linux,junit,cypress \\ --host https://orchestrator.example.com \\ --port 12345 \\ --token $TOKEN \\ --script_path /tmp \\ --workspace_dir /var/opentf/workspace \\ --polling_delay 1","title":"Example 2"},{"location":"guides/agent/#troubleshouting","text":"You can list the registered agents on a given orchestrator by running the following command: curl \\ -H \"Authorization: bearer $TOKEN \" \\ https://orchestrator.example.com:24368/agents You can deregister a rogue agent on a given orchestrator by running the followig command: curl \\ -X DELETE \\ -H \"Authorization: bearer $TOKEN \" \\ https://orchestrator.example.com:24368/agents/<agent_id> where <agent_id> is the UUID of the agent as shown by the previous command.","title":"Troubleshouting"},{"location":"guides/inception/","text":"Inception \u00b6 Sometime, you do not want to run tests, but you want to test your plugins or, well, tests, or some parts of them. The orchestrator comes with a special execution environment, inception , that you can use for this purpose. Quickstart \u00b6 Here is a simple workflow: metadata : name : Test Inception resources : files : - report1 - report2 - report3 jobs : prepare : runs-on : inception steps : - uses : actions/prepare-inception@v1 with : report.html : ${{ resources.files.report1 }} output.xml : ${{ resources.files.report2 }} log.html : ${{ resources.files.report3 }} robot : runs-on : inception needs : [ prepare ] generator : tm.squashtest.org/tm.generator@v1 with : squashTMUrl : https://squashtm.example.com/squash squashTMAutomatedServerLogin : ${{ variables.SQUASH_USER }} squashTMAutomatedServerPassword : ${{ variables.SQUASH_PASSWORD }} testPlanUuid : ... testPlanType : ... A few new things. First, a new files section in the resources part, that lists the expected external inputs. Second, a preparation job, that uses a new action provider, actions/prepare-inception@v1 . And finally an almost unchanged generator job, that has only two prerequisites: it targets the inception execution environment, and it needs the preparation job. And that is about it. You can then run your workflow the usual way: curl -X POST \\ -H \"Authorization: Bearer ${ TOKEN } \" \\ -F workflow = @Squashfile \\ -F report1 = @report1.html \\ -F report2 = @report2.xml \\ -F report3 = @report3.xml \\ -F variables = \"SQUASH_USER= ${ USER } \\nSQUASH_PASSWORD= ${ PASSWD } \" \\ https://orchestrator.example.com/workflows That part is new, too. You can now send more complex requests to the receptionist service. What does it do? \u00b6 The inception execution environment does the following things: It accepts all ExecutionCommand requests, as long as they carry the inception tag. This differs from the usual channel plugins: they only accept ExecutionCommand requests that they can fully satisfy. It does not execute the ExecutionCommand statements. It always returns a success status. In the But it catches the ::attach:: workflow commands and attached the prepared files to the ExecutionResult it publishes. It outputs the commands it receives. [If you were to have another execution environment that provides that inception tag, which would be a very bad idea, and if the arranger decides to send the ExecutionCommand to that environment, you would get the usual behavior, not the one described here.] Limitations \u00b6 There can be any number of workflows that may use it at any given time, but, for a given workflow, it cannot be used in parallel. If the ::attach:: workflow commands are the result of environment-side execution, they will not be correctly caught if they are complex or guarded.","title":"Inception"},{"location":"guides/inception/#inception","text":"Sometime, you do not want to run tests, but you want to test your plugins or, well, tests, or some parts of them. The orchestrator comes with a special execution environment, inception , that you can use for this purpose.","title":"Inception"},{"location":"guides/inception/#quickstart","text":"Here is a simple workflow: metadata : name : Test Inception resources : files : - report1 - report2 - report3 jobs : prepare : runs-on : inception steps : - uses : actions/prepare-inception@v1 with : report.html : ${{ resources.files.report1 }} output.xml : ${{ resources.files.report2 }} log.html : ${{ resources.files.report3 }} robot : runs-on : inception needs : [ prepare ] generator : tm.squashtest.org/tm.generator@v1 with : squashTMUrl : https://squashtm.example.com/squash squashTMAutomatedServerLogin : ${{ variables.SQUASH_USER }} squashTMAutomatedServerPassword : ${{ variables.SQUASH_PASSWORD }} testPlanUuid : ... testPlanType : ... A few new things. First, a new files section in the resources part, that lists the expected external inputs. Second, a preparation job, that uses a new action provider, actions/prepare-inception@v1 . And finally an almost unchanged generator job, that has only two prerequisites: it targets the inception execution environment, and it needs the preparation job. And that is about it. You can then run your workflow the usual way: curl -X POST \\ -H \"Authorization: Bearer ${ TOKEN } \" \\ -F workflow = @Squashfile \\ -F report1 = @report1.html \\ -F report2 = @report2.xml \\ -F report3 = @report3.xml \\ -F variables = \"SQUASH_USER= ${ USER } \\nSQUASH_PASSWORD= ${ PASSWD } \" \\ https://orchestrator.example.com/workflows That part is new, too. You can now send more complex requests to the receptionist service.","title":"Quickstart"},{"location":"guides/inception/#what-does-it-do","text":"The inception execution environment does the following things: It accepts all ExecutionCommand requests, as long as they carry the inception tag. This differs from the usual channel plugins: they only accept ExecutionCommand requests that they can fully satisfy. It does not execute the ExecutionCommand statements. It always returns a success status. In the But it catches the ::attach:: workflow commands and attached the prepared files to the ExecutionResult it publishes. It outputs the commands it receives. [If you were to have another execution environment that provides that inception tag, which would be a very bad idea, and if the arranger decides to send the ExecutionCommand to that environment, you would get the usual behavior, not the one described here.]","title":"What does it do?"},{"location":"guides/inception/#limitations","text":"There can be any number of workflows that may use it at any given time, but, for a given workflow, it cannot be used in parallel. If the ::attach:: workflow commands are the result of environment-side execution, they will not be correctly caught if they are complex or guarded.","title":"Limitations"},{"location":"guides/outputs/","text":"Outputs \u00b6 In a workflow, each job run in its own separate execution environment. But, sometime, you would like to get some information from a job you depend on. A basic ordering is offered by the needs section. Once a job depends on another, it can access the outputs produced by the job(s) it depends on, through contexts . Hello World \u00b6 Steps can produce outputs through workflow commands. Outputs are name/value pairs and hold strings. Workflow commands are printed in the standard output stream of an execution environment. For more information on defining workflow commands, see \u201c Workflow commands for OpenTestFactory Orchestrator .\u201d The set-output workflow command is as follows: ::set-output name=xxx::yyyy The following step will create an output named test of value hello : steps: - id: step1 run: echo \"::set-output name=test::hello\" A step can produce any number of outputs: steps: - id: step1 run: | echo \"::set-output name=foo::foo foo\" echo \"::set-output name=bar::bar baz\" A job can collect and expose the outputs produced by the steps it contains to its dependent jobs. Here, the job job1 will expose an output1 output that will have a value of hello , and an output2 output that will have a value of something else : job1 : outputs : output1 : ${{ steps.step1.outputs.test }} output1 : something else steps : - id : step1 run : echo \"::set-output name=test::hello\" Other jobs that depend in this job1 job can then access those outputs: metadata : name : Using outputs from a previous job jobs : job1 : runs-on : linux # Map a step output to a job output outputs : output1 : ${{ steps.step1.outputs.test }} output2 : ${{ steps.step2.outputs.test }} steps : - id : step1 run : echo \"::set-output name=test::hello\" - id : step2 run : echo \"::set-output name=test::world\" job2 : runs-on : linux needs : job1 steps : - run : echo ${{needs.job1.outputs.output1}} ${{needs.job1.outputs.output2}} This should echo \u2018hello world\u2019 in the execution environment in which job2 runs.","title":"Outputs"},{"location":"guides/outputs/#outputs","text":"In a workflow, each job run in its own separate execution environment. But, sometime, you would like to get some information from a job you depend on. A basic ordering is offered by the needs section. Once a job depends on another, it can access the outputs produced by the job(s) it depends on, through contexts .","title":"Outputs"},{"location":"guides/outputs/#hello-world","text":"Steps can produce outputs through workflow commands. Outputs are name/value pairs and hold strings. Workflow commands are printed in the standard output stream of an execution environment. For more information on defining workflow commands, see \u201c Workflow commands for OpenTestFactory Orchestrator .\u201d The set-output workflow command is as follows: ::set-output name=xxx::yyyy The following step will create an output named test of value hello : steps: - id: step1 run: echo \"::set-output name=test::hello\" A step can produce any number of outputs: steps: - id: step1 run: | echo \"::set-output name=foo::foo foo\" echo \"::set-output name=bar::bar baz\" A job can collect and expose the outputs produced by the steps it contains to its dependent jobs. Here, the job job1 will expose an output1 output that will have a value of hello , and an output2 output that will have a value of something else : job1 : outputs : output1 : ${{ steps.step1.outputs.test }} output1 : something else steps : - id : step1 run : echo \"::set-output name=test::hello\" Other jobs that depend in this job1 job can then access those outputs: metadata : name : Using outputs from a previous job jobs : job1 : runs-on : linux # Map a step output to a job output outputs : output1 : ${{ steps.step1.outputs.test }} output2 : ${{ steps.step2.outputs.test }} steps : - id : step1 run : echo \"::set-output name=test::hello\" - id : step2 run : echo \"::set-output name=test::world\" job2 : runs-on : linux needs : job1 steps : - run : echo ${{needs.job1.outputs.output1}} ${{needs.job1.outputs.output2}} This should echo \u2018hello world\u2019 in the execution environment in which job2 runs.","title":"Hello World"},{"location":"guides/repositories/","text":"Repositories \u00b6 In a typical workflow, you will checkout code at the beginning of each job. If you have many job in your workflow, duplicating the repository parameters can become tedious. Workflows allow you to define repository resources, that you can share between your jobs. Additionally, those resources can carry their own specific credentials, so that job generators can omit them. Repositories resources \u00b6 Repository resources are defined in the .resources.repositories part of a workflow. There can be any number of repositories resources defined, and they may refer to the same repository, but they all must have different names. resources : repositories : - name : foo type : github repository : a/b.git endpoint : https://github.com - name : bar type : gitlab repository : c/d.git endpoint : httpd://gitlab.com Once defined, those repository resources can be referenced either explicitly or implicitly in your jobs: jobs : job1 : steps : - uses : actions/checkout@v2 with : repository : ${{ resources.repositories.foo }} - uses : actions/checkout@v2 with : repository : https://gitlab.com/c/d.git The explicit reference uses the expression syntax. The implicit reference is used if the endpoint and repository matches. Credentials \u00b6 If you define a repository in the resources section, all checkouts of this repository will use the credentials you specified in the resources section, not the ones you may have specified in the checkout action. Examples \u00b6 In the following example, the checkout will use the credentials specified in variables.user and variables.pwd : metadata : name : my workflow resources : repositories : - name : awesome repository : awesome/stuff.git type : github endpoint : https://${{ variables.user }}:${{ variables.pwd }}@github.com/ jobs : job1 : runs-on : linux steps : - uses : actions/checkout@v2 with : repository : https://github.com/awesome/stuff.git In that almost identical example, the checkout will also use the credentials specified in variables.user and variables.pwd , not me and secret : metadata : name : my workflow resources : repositories : - name : awesome repository : awesome/stuff.git type : gitlab endpoint : https://${{ variables.user }}:${{ variables.pwd }}@gitlab.com/ jobs : job1 : runs-on : linux steps : - uses : actions/checkout@v2 with : repository : https://me:secret@gitlab.com/awesome/stuff.git Next Steps \u00b6 Here are some helpful resources for taking your next steps with the OpenTestFactory orchestrator: \u201c actions/checkout@v2 \u201d for more details about configuring the checkout action \u201cGuides\u201d for specific uses cases and examples \u201c Context and expression syntax for orchestrator Workflows \u201d for more information on expressions","title":"Repositories"},{"location":"guides/repositories/#repositories","text":"In a typical workflow, you will checkout code at the beginning of each job. If you have many job in your workflow, duplicating the repository parameters can become tedious. Workflows allow you to define repository resources, that you can share between your jobs. Additionally, those resources can carry their own specific credentials, so that job generators can omit them.","title":"Repositories"},{"location":"guides/repositories/#repositories-resources","text":"Repository resources are defined in the .resources.repositories part of a workflow. There can be any number of repositories resources defined, and they may refer to the same repository, but they all must have different names. resources : repositories : - name : foo type : github repository : a/b.git endpoint : https://github.com - name : bar type : gitlab repository : c/d.git endpoint : httpd://gitlab.com Once defined, those repository resources can be referenced either explicitly or implicitly in your jobs: jobs : job1 : steps : - uses : actions/checkout@v2 with : repository : ${{ resources.repositories.foo }} - uses : actions/checkout@v2 with : repository : https://gitlab.com/c/d.git The explicit reference uses the expression syntax. The implicit reference is used if the endpoint and repository matches.","title":"Repositories resources"},{"location":"guides/repositories/#credentials","text":"If you define a repository in the resources section, all checkouts of this repository will use the credentials you specified in the resources section, not the ones you may have specified in the checkout action.","title":"Credentials"},{"location":"guides/repositories/#examples","text":"In the following example, the checkout will use the credentials specified in variables.user and variables.pwd : metadata : name : my workflow resources : repositories : - name : awesome repository : awesome/stuff.git type : github endpoint : https://${{ variables.user }}:${{ variables.pwd }}@github.com/ jobs : job1 : runs-on : linux steps : - uses : actions/checkout@v2 with : repository : https://github.com/awesome/stuff.git In that almost identical example, the checkout will also use the credentials specified in variables.user and variables.pwd , not me and secret : metadata : name : my workflow resources : repositories : - name : awesome repository : awesome/stuff.git type : gitlab endpoint : https://${{ variables.user }}:${{ variables.pwd }}@gitlab.com/ jobs : job1 : runs-on : linux steps : - uses : actions/checkout@v2 with : repository : https://me:secret@gitlab.com/awesome/stuff.git","title":"Examples"},{"location":"guides/repositories/#next-steps","text":"Here are some helpful resources for taking your next steps with the OpenTestFactory orchestrator: \u201c actions/checkout@v2 \u201d for more details about configuring the checkout action \u201cGuides\u201d for specific uses cases and examples \u201c Context and expression syntax for orchestrator Workflows \u201d for more information on expressions","title":"Next Steps"},{"location":"learn-squash-orchestrator/essential-features-of-squash-orchestrator/","text":"Essential features of OpenTestFactory orchestrator \u00b6 The orchestrator is designed to help you build robust and dynamic automations. This guide will show you how to craft workflows that include environment variables, customized scripts, and more. Overview \u00b6 The OpenTestFactory orchestrator allow you to customize your workflows to meet the unique needs of your application and team. In this guide, we\u2019ll discuss some of the essential customization techniques such as using variables, running scripts, and sharing data and artifacts between jobs. Using variables in your workflows \u00b6 The orchestrator include default environment variables for each workflow run. If you need to use custom environment variables, you can set these in your YAML workflow file. This example demonstrates how to create custom variables named POSTGRES_HOST and POSTGRES_PORT . These variables are then available to the node client.js script. jobs : example-job : steps : - name : Connect to PostgreSQL run : node client.js variables : POSTGRES_HOST : postgres POSTGRES_PORT : 5432 For more information, see \u201c Using environment variables. \u201c Adding scripts to your workflow \u00b6 You can use steps to run scripts and shell commands, which are then executed on the assigned execution environment. This example demonstrates how a step can use the run keyword to execute npm install -g bats on the execution environment. jobs : example-job : steps : - run : npm install -g bats For example, to run a script as an action, you can store the script in your repository and supply the path and shell type. jobs : example-job : steps : - name : Run build script run : ./scripts/build.sh shell : bash For more information, see \u201c Workflow syntax for OpenTestFactory orchestrator. \u201c","title":"Essential features of Squash Orchestrator"},{"location":"learn-squash-orchestrator/essential-features-of-squash-orchestrator/#essential-features-of-opentestfactory-orchestrator","text":"The orchestrator is designed to help you build robust and dynamic automations. This guide will show you how to craft workflows that include environment variables, customized scripts, and more.","title":"Essential features of OpenTestFactory orchestrator"},{"location":"learn-squash-orchestrator/essential-features-of-squash-orchestrator/#overview","text":"The OpenTestFactory orchestrator allow you to customize your workflows to meet the unique needs of your application and team. In this guide, we\u2019ll discuss some of the essential customization techniques such as using variables, running scripts, and sharing data and artifacts between jobs.","title":"Overview"},{"location":"learn-squash-orchestrator/essential-features-of-squash-orchestrator/#using-variables-in-your-workflows","text":"The orchestrator include default environment variables for each workflow run. If you need to use custom environment variables, you can set these in your YAML workflow file. This example demonstrates how to create custom variables named POSTGRES_HOST and POSTGRES_PORT . These variables are then available to the node client.js script. jobs : example-job : steps : - name : Connect to PostgreSQL run : node client.js variables : POSTGRES_HOST : postgres POSTGRES_PORT : 5432 For more information, see \u201c Using environment variables. \u201c","title":"Using variables in your workflows"},{"location":"learn-squash-orchestrator/essential-features-of-squash-orchestrator/#adding-scripts-to-your-workflow","text":"You can use steps to run scripts and shell commands, which are then executed on the assigned execution environment. This example demonstrates how a step can use the run keyword to execute npm install -g bats on the execution environment. jobs : example-job : steps : - run : npm install -g bats For example, to run a script as an action, you can store the script in your repository and supply the path and shell type. jobs : example-job : steps : - name : Run build script run : ./scripts/build.sh shell : bash For more information, see \u201c Workflow syntax for OpenTestFactory orchestrator. \u201c","title":"Adding scripts to your workflow"},{"location":"learn-squash-orchestrator/finding-and-customizing-plugins/","text":"Finding and customizing plugins \u00b6 Plugins are the building blocks that power your workflow. A workflow can contain generators and providers plugins created by the community, or you can create your own plugins. This guide will show you how to discover, use and customize plugins. Overview \u00b6 TBD Browsing plugins \u00b6 You can search and browse plugins directly in this documentation, in the Generators and Providers sections. Adding a plugin to your workflow \u00b6 A plugin\u2019s listing page includes the plugin\u2019s version and the workflow syntax required to use the plugin. To keep your workflow stable even when updates are made to a plugin, you can reference the version of the plugin by specifying the tag number in your workflow file. Navigate to the plugin you want to use in your workflow Under \u201cExample\u201d, click to copy the workflow syntax. Paste the syntax as a new step or job in your workflow. For more information, see \u201c Workflow syntax for OpenTestFactory Orchestrator. \u201c If the plugin requires you to provide inputs, set them in your workflow. For information on inputs a plugin might require, see \u201c Using inputs and outputs with a plugin. \u201c Using inputs and outputs with a plugin \u00b6 A plugin often accepts or requires inputs and generates outputs that you can use. For example, a plugin might require you to specify a path to a file, the name of a label, or other data it will use as part of the plugin processing. To see the inputs and outputs of a plugin, or the plugin.yaml in the root of the plugin\u2019s repository. In this example plugin.yaml , the inputs keyword defines a required input called file-path , and includes a default value that will be used if none is specified. The outputs keyword defines an output called results-file , which tells you where to locate the results. name : 'Example' description : 'Receives file and generates output' inputs : file-path : # id of input description : \"Path to test script\" required : true default : 'test-file.js' outputs : results-file : # id of output description : \"Path to results file\" Next steps \u00b6 To continue learning about the OpenTestFactory Orchestrator, see \u201c Essential features of OpenTestFactory Orchestrator. \u201c","title":"Finding and customizing plugins"},{"location":"learn-squash-orchestrator/finding-and-customizing-plugins/#finding-and-customizing-plugins","text":"Plugins are the building blocks that power your workflow. A workflow can contain generators and providers plugins created by the community, or you can create your own plugins. This guide will show you how to discover, use and customize plugins.","title":"Finding and customizing plugins"},{"location":"learn-squash-orchestrator/finding-and-customizing-plugins/#overview","text":"TBD","title":"Overview"},{"location":"learn-squash-orchestrator/finding-and-customizing-plugins/#browsing-plugins","text":"You can search and browse plugins directly in this documentation, in the Generators and Providers sections.","title":"Browsing plugins"},{"location":"learn-squash-orchestrator/finding-and-customizing-plugins/#adding-a-plugin-to-your-workflow","text":"A plugin\u2019s listing page includes the plugin\u2019s version and the workflow syntax required to use the plugin. To keep your workflow stable even when updates are made to a plugin, you can reference the version of the plugin by specifying the tag number in your workflow file. Navigate to the plugin you want to use in your workflow Under \u201cExample\u201d, click to copy the workflow syntax. Paste the syntax as a new step or job in your workflow. For more information, see \u201c Workflow syntax for OpenTestFactory Orchestrator. \u201c If the plugin requires you to provide inputs, set them in your workflow. For information on inputs a plugin might require, see \u201c Using inputs and outputs with a plugin. \u201c","title":"Adding a plugin to your workflow"},{"location":"learn-squash-orchestrator/finding-and-customizing-plugins/#using-inputs-and-outputs-with-a-plugin","text":"A plugin often accepts or requires inputs and generates outputs that you can use. For example, a plugin might require you to specify a path to a file, the name of a label, or other data it will use as part of the plugin processing. To see the inputs and outputs of a plugin, or the plugin.yaml in the root of the plugin\u2019s repository. In this example plugin.yaml , the inputs keyword defines a required input called file-path , and includes a default value that will be used if none is specified. The outputs keyword defines an output called results-file , which tells you where to locate the results. name : 'Example' description : 'Receives file and generates output' inputs : file-path : # id of input description : \"Path to test script\" required : true default : 'test-file.js' outputs : results-file : # id of output description : \"Path to results file\"","title":"Using inputs and outputs with a plugin"},{"location":"learn-squash-orchestrator/finding-and-customizing-plugins/#next-steps","text":"To continue learning about the OpenTestFactory Orchestrator, see \u201c Essential features of OpenTestFactory Orchestrator. \u201c","title":"Next steps"},{"location":"learn-squash-orchestrator/introduction-to-squash-orchestrator/","text":"Introduction to OpenTestFactory Orchestrator \u00b6 Learn about the core concepts and various components of the OpenTestFactory orchestrator, and see an example that shows you how to add testing automation to your repository. Overview \u00b6 The OpenTestFactory orchestrator helps you automate testing tasks within your software development life cycle. Workflows are trigger-driven, meaning that you can run a series of commands after a specified trigger has occurred. For example, every time someone creates a pull request for a repository, you can automatically run a command that executes a software testing script. The components of the OpenTestFactory Orchestrator \u00b6 Below is a list of the multiple orchestrator components that work together to run jobs. You can see how these components interact with each others. Workflows \u00b6 The workflow is an automated procedure that you add to your repository. Workflows are made up of one or more jobs and can be scheduled or triggered. The workflow can be used to test a project. Triggers \u00b6 A trigger is a specific activity that launch a workflow. For example, activity can originate from your CI toolchain when someone pushes a commit to a repository or when a pull request is created. You can also launch a workflow when an external event occurs. Jobs \u00b6 A job can either be a generator or a set of steps that execute on the same execution environment. By default, a workflow with multiple jobs will run those jobs in parallel. You can also configure a workflow to run jobs sequentially. For example, a workflow can have two sequential jobs that configure and test code, where the test job is dependent on the status of the configure job. If the configure job fails, the test job will not run. Generators \u00b6 Generators are standalone commands that produce jobs. You can create your own generators, or use and customize generators created by the OpenTestFactory orchestrator community. To use a generator in a workflow, you must include it as a job. Steps \u00b6 A step is an individual task that can run commands in a job. A step can either be a provider or a shell command. Each step in a job executes on the execution environment, allowing the steps in that job to share data with each other. Providers \u00b6 Providers are standalone commands that are combined into steps to create a job . Providers are the smallest portable building block of a workflow. You can create your own providers, or use and customize providers created by the orchestrator community. To use a provider plugin in a workflow you must include it as a step. Execution environments \u00b6 An execution environment is a server that the orchestrator can talk to. You can use an execution environment that is specific to the system you want to test, or you can use a collection of generic execution environments that each host a specific testing framework you have code for. An execution environment listens for available jobs, runs one job at a time, and reports the progress, logs, and results back to the OpenTestFactory Orchestrator. Create an example workflow \u00b6 The orchestrator uses YAML syntax to define the jobs and steps. These YAML files are stored in your code repository. You can create an example workflow in your repository that automatically triggers a series of commands whenever code is pushed. In this workflow, the orchestrator checks out the pushed code, installs the software dependencies and runs bats -v . In your repository, create a new file called Squashfile and add the following code. metadata : name : learn-opentf-orchestrator jobs : my-first-job : runs-on : [ linux , robotframework ] steps : - uses : actions/checkout@v2 with : repository : https://github.com/robotframework/RobotDemo.git - uses : robotframework/robot@v1 with : datasource : RobotDemo/keyboard_driven.robot - run : ls -l Commit these changes and push them to your repository. Your new OpenTestFactory orchestrator workflow file is now installed in your repository and will run each time someone triggers it. Understanding the workflow file \u00b6 To help you understand how YAML syntax is used to create a workflow file, this section explains each line of the introduction\u2019s example: Line Explanation metadata: Groups together the metadata elements. name: learn-opentf-orchestrator The name of the workflow. jobs: Groups together all the jobs that run in the learn-opentf-orchestrator workflow file. my-first-job: Defines the name of the my-first-hob job stored within the jobs section. runs-on: [linux, robotframework] Configures the job to run on a RobotFramework execution environment. This means the job will execute on a Linux system with the RobotFramework framework installed. steps: Groups together all the steps that run in the my-first-job job. Each item nested under this section is a separate provider plugin or shell command. uses: actions/checkout@v2 The uses keyword tells the job to retrieve v2 of the action named actions/checkout@v2 . This is an action that checks out your repository and downloads it to the execution environment, allowing you to run actions against your code (such as testing tools). You must use the checkout action any time your workflow will run against the repository\u2019s code. with: The with keyword provides parameters to the action. repository: https://github.com/robotframework/RobotDemo.git The actions/checkout@v2 action has a repository parameter that specify the repository to check out. uses: robotframework/robot@v1 This action runs the specified RobotFramework datasource. with: The with keyword provides parameters to the action. datasource: RobotDemo/keyboard_driven.robot The roboframework/robot@v1 action has a datasource parameter that specify the datasource to use. run: ls -l The run keyword tels the job to execute a command on the execution environment. In this case, you are using ls to show the content of the current directory. Visualizing the workflow file \u00b6 In this diagram, you can see the workflow file you just created and how the OpenTestFactory orchestrator components are organized in a hierarchy. Each step execute a single provider or shell command. Steps 1 and 2 use prebuilt community providers. Steps 3 and 4 run shell commands directly on the execution environment. To find more prebuilt providers for your workflows, see \u201c Finding and customizing plugins. \u201c DIAGRAM Next steps \u00b6 To continue learning about the OpenTestFactory orchestrator, see \u201c Finding and customizing plugins. \u201c","title":"Introduction to Squash Orchestrator"},{"location":"learn-squash-orchestrator/introduction-to-squash-orchestrator/#introduction-to-opentestfactory-orchestrator","text":"Learn about the core concepts and various components of the OpenTestFactory orchestrator, and see an example that shows you how to add testing automation to your repository.","title":"Introduction to OpenTestFactory Orchestrator"},{"location":"learn-squash-orchestrator/introduction-to-squash-orchestrator/#overview","text":"The OpenTestFactory orchestrator helps you automate testing tasks within your software development life cycle. Workflows are trigger-driven, meaning that you can run a series of commands after a specified trigger has occurred. For example, every time someone creates a pull request for a repository, you can automatically run a command that executes a software testing script.","title":"Overview"},{"location":"learn-squash-orchestrator/introduction-to-squash-orchestrator/#the-components-of-the-opentestfactory-orchestrator","text":"Below is a list of the multiple orchestrator components that work together to run jobs. You can see how these components interact with each others.","title":"The components of the OpenTestFactory Orchestrator"},{"location":"learn-squash-orchestrator/introduction-to-squash-orchestrator/#workflows","text":"The workflow is an automated procedure that you add to your repository. Workflows are made up of one or more jobs and can be scheduled or triggered. The workflow can be used to test a project.","title":"Workflows"},{"location":"learn-squash-orchestrator/introduction-to-squash-orchestrator/#triggers","text":"A trigger is a specific activity that launch a workflow. For example, activity can originate from your CI toolchain when someone pushes a commit to a repository or when a pull request is created. You can also launch a workflow when an external event occurs.","title":"Triggers"},{"location":"learn-squash-orchestrator/introduction-to-squash-orchestrator/#jobs","text":"A job can either be a generator or a set of steps that execute on the same execution environment. By default, a workflow with multiple jobs will run those jobs in parallel. You can also configure a workflow to run jobs sequentially. For example, a workflow can have two sequential jobs that configure and test code, where the test job is dependent on the status of the configure job. If the configure job fails, the test job will not run.","title":"Jobs"},{"location":"learn-squash-orchestrator/introduction-to-squash-orchestrator/#generators","text":"Generators are standalone commands that produce jobs. You can create your own generators, or use and customize generators created by the OpenTestFactory orchestrator community. To use a generator in a workflow, you must include it as a job.","title":"Generators"},{"location":"learn-squash-orchestrator/introduction-to-squash-orchestrator/#steps","text":"A step is an individual task that can run commands in a job. A step can either be a provider or a shell command. Each step in a job executes on the execution environment, allowing the steps in that job to share data with each other.","title":"Steps"},{"location":"learn-squash-orchestrator/introduction-to-squash-orchestrator/#providers","text":"Providers are standalone commands that are combined into steps to create a job . Providers are the smallest portable building block of a workflow. You can create your own providers, or use and customize providers created by the orchestrator community. To use a provider plugin in a workflow you must include it as a step.","title":"Providers"},{"location":"learn-squash-orchestrator/introduction-to-squash-orchestrator/#execution-environments","text":"An execution environment is a server that the orchestrator can talk to. You can use an execution environment that is specific to the system you want to test, or you can use a collection of generic execution environments that each host a specific testing framework you have code for. An execution environment listens for available jobs, runs one job at a time, and reports the progress, logs, and results back to the OpenTestFactory Orchestrator.","title":"Execution environments"},{"location":"learn-squash-orchestrator/introduction-to-squash-orchestrator/#create-an-example-workflow","text":"The orchestrator uses YAML syntax to define the jobs and steps. These YAML files are stored in your code repository. You can create an example workflow in your repository that automatically triggers a series of commands whenever code is pushed. In this workflow, the orchestrator checks out the pushed code, installs the software dependencies and runs bats -v . In your repository, create a new file called Squashfile and add the following code. metadata : name : learn-opentf-orchestrator jobs : my-first-job : runs-on : [ linux , robotframework ] steps : - uses : actions/checkout@v2 with : repository : https://github.com/robotframework/RobotDemo.git - uses : robotframework/robot@v1 with : datasource : RobotDemo/keyboard_driven.robot - run : ls -l Commit these changes and push them to your repository. Your new OpenTestFactory orchestrator workflow file is now installed in your repository and will run each time someone triggers it.","title":"Create an example workflow"},{"location":"learn-squash-orchestrator/introduction-to-squash-orchestrator/#understanding-the-workflow-file","text":"To help you understand how YAML syntax is used to create a workflow file, this section explains each line of the introduction\u2019s example: Line Explanation metadata: Groups together the metadata elements. name: learn-opentf-orchestrator The name of the workflow. jobs: Groups together all the jobs that run in the learn-opentf-orchestrator workflow file. my-first-job: Defines the name of the my-first-hob job stored within the jobs section. runs-on: [linux, robotframework] Configures the job to run on a RobotFramework execution environment. This means the job will execute on a Linux system with the RobotFramework framework installed. steps: Groups together all the steps that run in the my-first-job job. Each item nested under this section is a separate provider plugin or shell command. uses: actions/checkout@v2 The uses keyword tells the job to retrieve v2 of the action named actions/checkout@v2 . This is an action that checks out your repository and downloads it to the execution environment, allowing you to run actions against your code (such as testing tools). You must use the checkout action any time your workflow will run against the repository\u2019s code. with: The with keyword provides parameters to the action. repository: https://github.com/robotframework/RobotDemo.git The actions/checkout@v2 action has a repository parameter that specify the repository to check out. uses: robotframework/robot@v1 This action runs the specified RobotFramework datasource. with: The with keyword provides parameters to the action. datasource: RobotDemo/keyboard_driven.robot The roboframework/robot@v1 action has a datasource parameter that specify the datasource to use. run: ls -l The run keyword tels the job to execute a command on the execution environment. In this case, you are using ls to show the content of the current directory.","title":"Understanding the workflow file"},{"location":"learn-squash-orchestrator/introduction-to-squash-orchestrator/#visualizing-the-workflow-file","text":"In this diagram, you can see the workflow file you just created and how the OpenTestFactory orchestrator components are organized in a hierarchy. Each step execute a single provider or shell command. Steps 1 and 2 use prebuilt community providers. Steps 3 and 4 run shell commands directly on the execution environment. To find more prebuilt providers for your workflows, see \u201c Finding and customizing plugins. \u201c DIAGRAM","title":"Visualizing the workflow file"},{"location":"learn-squash-orchestrator/introduction-to-squash-orchestrator/#next-steps","text":"To continue learning about the OpenTestFactory orchestrator, see \u201c Finding and customizing plugins. \u201c","title":"Next steps"},{"location":"learn-squash-orchestrator/learn/","text":"Learn OpenTestFactory Orchestrator \u00b6 Whether you are new to the OpenTestFactory orchestrator or interested in learning all it has to offer, this guide will help you use the orchestrator to accelerate your testing workflows. Introduction to OpenTestFactory Orchestrator \u00b6 Learn about the core concepts and various components of the OpenTestFactory Orchestrator, and see an example that shows you how to add testing automation to your repository. Finding and customizing plugins \u00b6 Plugins are the building blocks that power your workflow. A workflow can contain generators and providers plugins created by the community, or you can create your own plugins. This guide will show you how to discover, use, and customize plugins. Essential features of OpenTestFactory Orchestrator \u00b6 The OpenTestFactory orchestrator is designed to help you build robust and dynamic automations. This guide will show you how to craft workflows that include environment variables, customized scripts, and more. Managing complex workflows \u00b6 This guide shows you how to use the advanced features of the OpenTestFactory orchestrator, with dependent jobs, caching, build matrices, environments, and labels. Sharing workflows with your organization \u00b6 Security hardening for OpenTestFactory Orchestrator \u00b6","title":"Overview"},{"location":"learn-squash-orchestrator/learn/#learn-opentestfactory-orchestrator","text":"Whether you are new to the OpenTestFactory orchestrator or interested in learning all it has to offer, this guide will help you use the orchestrator to accelerate your testing workflows.","title":"Learn OpenTestFactory Orchestrator"},{"location":"learn-squash-orchestrator/learn/#introduction-to-opentestfactory-orchestrator","text":"Learn about the core concepts and various components of the OpenTestFactory Orchestrator, and see an example that shows you how to add testing automation to your repository.","title":"Introduction to OpenTestFactory Orchestrator"},{"location":"learn-squash-orchestrator/learn/#finding-and-customizing-plugins","text":"Plugins are the building blocks that power your workflow. A workflow can contain generators and providers plugins created by the community, or you can create your own plugins. This guide will show you how to discover, use, and customize plugins.","title":"Finding and customizing plugins"},{"location":"learn-squash-orchestrator/learn/#essential-features-of-opentestfactory-orchestrator","text":"The OpenTestFactory orchestrator is designed to help you build robust and dynamic automations. This guide will show you how to craft workflows that include environment variables, customized scripts, and more.","title":"Essential features of OpenTestFactory Orchestrator"},{"location":"learn-squash-orchestrator/learn/#managing-complex-workflows","text":"This guide shows you how to use the advanced features of the OpenTestFactory orchestrator, with dependent jobs, caching, build matrices, environments, and labels.","title":"Managing complex workflows"},{"location":"learn-squash-orchestrator/learn/#sharing-workflows-with-your-organization","text":"","title":"Sharing workflows with your organization"},{"location":"learn-squash-orchestrator/learn/#security-hardening-for-opentestfactory-orchestrator","text":"","title":"Security hardening for OpenTestFactory Orchestrator"},{"location":"plugins/about-plugins/","text":"About plugins \u00b6 Plugins are individual tasks that you can combine to create jobs and customize your workflow. You can create your own plugins or use and customize plugins shared by the OpenTestFactory community. About plugins \u00b6 You can create plugins by writing custom code that interacts with your workflow in any way you would like, including integrating with any available third-party API. For example, an action can send SMS alerts when urgent issues are created. You can write your own plugins to use in your workflow or share the plugins you build with the OpenTestFactory community. Plugins can run directly on a machine or in a Docker container. You can define a plugin\u2019s inputs, outputs, and environment variables. Type of plugins \u00b6 You can build plugins using any technology you like. TODO","title":"About plugins"},{"location":"plugins/about-plugins/#about-plugins","text":"Plugins are individual tasks that you can combine to create jobs and customize your workflow. You can create your own plugins or use and customize plugins shared by the OpenTestFactory community.","title":"About plugins"},{"location":"plugins/about-plugins/#about-plugins_1","text":"You can create plugins by writing custom code that interacts with your workflow in any way you would like, including integrating with any available third-party API. For example, an action can send SMS alerts when urgent issues are created. You can write your own plugins to use in your workflow or share the plugins you build with the OpenTestFactory community. Plugins can run directly on a machine or in a Docker container. You can define a plugin\u2019s inputs, outputs, and environment variables.","title":"About plugins"},{"location":"plugins/about-plugins/#type-of-plugins","text":"You can build plugins using any technology you like. TODO","title":"Type of plugins"},{"location":"plugins/creating-a-generator-plugin/","text":"Creating a generator plugin \u00b6 In this guide you will learn how to build a generator plugin. Introduction \u00b6 In this guide, you\u2019ll learn about the basic components needed to create and use a packaged generator plugin. To focus this guide on the components needed to package the plugin, the functionality of the plugin\u2019s code is minimal. The plugin prints \u201cHello World\u201d in the logs or \u201cHello {who-to-greet}\u201d if you provide a custom name. This guide uses the Squash Orchestrator Toolkit module to speed up development. For more information, see the squashtf/toolkit repository. Once you complete this project, you should understand how to build your own generator plugin and test it in a workflow. To ensure your plugins are compatible with all Squash Orchestrator deployments (linux, windows, \u2026), the packaged code you write should be pure and not rely on other non-portable binaries. Prerequisites \u00b6 You may find it helpful to have a basic understanding of Squash Orchestrator environment variables: Using environment variables Before you begin, you will need to create a repository. Create a new repository on GitHub/Gitlab/BitBucket/.... You can choose any repository name or use \u201chello-world-generator-plugin\u201d like this example. Clone your repository to your computer. From your terminal, change directories into your new repository. cd hello-world-generator-plugin Create a web service \u00b6 Generator plugins are simple web services. They subscribe to specific events on startup, and publish events in response. The squashtf/toolkit module streamlines the process if you want to write your plugin in Python. For more information on doing things in a less assisted way, see \u201cWriting plugins the hard way.\u201d In your new hello-world-generator-plugin directory, create a new main.py file. main.py \u00b6 # main.py from squashtf.toolkit import make_plugin , run_plugin from .implementation import generate plugin = make_plugin ( name = 'helloworld' , description = 'A helloworld generator.' , generator = generate ) if __name__ == '__main__' : run_plugin ( plugin ) Create a plugin metadata file \u00b6 Create a new plugin.yaml file in the hello-world-generator-plugin directory you created above. For more information, see \u201c Metadata syntax for Squash Orchestrator plugins .\u201d plugin.yaml \u00b6 # plugin.yaml apiVersion : 'opentestfactory.org/v1alpha1' kind : 'GeneratorPlugin' metadata : name : 'HelloWorld' description : 'Greet someone' events : - category : helloworld categoryVersion : v1 cmd : 'python3 -m main' inputs : who-to-greet : description : 'Who to greet' required : false default : 'World' outputs : random-number : description : 'Random number' value : ${{ steps.random-number-generator.outputs.random-id }} Writing the plugin code \u00b6 Generator plugins must return a possibly empty collection of jobs. Each job has a name and a definition. For more information about the jobs syntax, see \u201c Workflow syntax for Squash Orchestrator .\u201d Job names and step ids are local to the returned collection of jobs. They do not conflict with names and ids used in the referring workflow. The following Python script example uses the who-to-greet input variable to print \u201cHello {who-to-greet}\u201d in the log file and maps the random generated number to the random-number output variable. implementation.py \u00b6 # implementation.py def generate ( inputs ): jobs = { 'jobs' : { 'job1' : { 'runs-on' : 'linux' , 'steps' : [ { 'run' : 'echo Hello ' + inputs [ 'who-to-greet' ] + '.' , 'shell' : 'bash' }, { 'id' : 'random-number-generator' , 'run' : 'echo \"::set-output name=random-id::$(echo $RANDOM)\"' , 'shell' : 'bash' } ] } } } return jobs Create a README \u00b6 To let people know how to use your plugin, you can create a README file. A README is most helpful when you plan to share your plugin publicly, but is also a great way to remind you or your team how to use the plugin. In your hello-world-generator-plugin directory, create a README.md file that specifies the following information: A detailed description of what the plugin does. Required input and outputs arguments. Optional input and outputs arguments. Environment variables the plugin uses. An example of how to use your plugin in a workflow. README.md \u00b6 # Hello world generator plugin This action prints \"Hello World\" or \"Hello\" + the name of a person to greet to the log. ## Inputs ### `who-to-greets` **Required** The name of the person to greet. Default `\"World\"` . ## Outputs ### `random-id` A random number. ## Example usage generator: helloworld@v1 with: who-to-greets: 'Mona the Octocat' Commit, tag, push, and deploy your plugin \u00b6 From your terminal, commit your plugin.yaml , implementation.py , helloworld.py , and README.md files. It is best practice to also add a version tag for releases of your plugin. For more information on versioning your plugin, see \u201cAbout plugins.\u201d git add plugin.yaml implementation.py helloworld.py README.md git commit -m \"My first plugin is ready\" git tag -a -m \"My first plugin release\" v1 git push --follow-tags TODO (Deploy) Testing out your plugin in a workflow \u00b6 Now you are ready to test your plugin out in a workflow. Example \u00b6 jobs : hello_world_job : runs-on : linux name : A job to say hello generator : helloworld@v1 with : who-to-greets : \"Mona the Octocat\"","title":"Creating a generator plugin"},{"location":"plugins/creating-a-generator-plugin/#creating-a-generator-plugin","text":"In this guide you will learn how to build a generator plugin.","title":"Creating a generator plugin"},{"location":"plugins/creating-a-generator-plugin/#introduction","text":"In this guide, you\u2019ll learn about the basic components needed to create and use a packaged generator plugin. To focus this guide on the components needed to package the plugin, the functionality of the plugin\u2019s code is minimal. The plugin prints \u201cHello World\u201d in the logs or \u201cHello {who-to-greet}\u201d if you provide a custom name. This guide uses the Squash Orchestrator Toolkit module to speed up development. For more information, see the squashtf/toolkit repository. Once you complete this project, you should understand how to build your own generator plugin and test it in a workflow. To ensure your plugins are compatible with all Squash Orchestrator deployments (linux, windows, \u2026), the packaged code you write should be pure and not rely on other non-portable binaries.","title":"Introduction"},{"location":"plugins/creating-a-generator-plugin/#prerequisites","text":"You may find it helpful to have a basic understanding of Squash Orchestrator environment variables: Using environment variables Before you begin, you will need to create a repository. Create a new repository on GitHub/Gitlab/BitBucket/.... You can choose any repository name or use \u201chello-world-generator-plugin\u201d like this example. Clone your repository to your computer. From your terminal, change directories into your new repository. cd hello-world-generator-plugin","title":"Prerequisites"},{"location":"plugins/creating-a-generator-plugin/#create-a-web-service","text":"Generator plugins are simple web services. They subscribe to specific events on startup, and publish events in response. The squashtf/toolkit module streamlines the process if you want to write your plugin in Python. For more information on doing things in a less assisted way, see \u201cWriting plugins the hard way.\u201d In your new hello-world-generator-plugin directory, create a new main.py file.","title":"Create a web service"},{"location":"plugins/creating-a-generator-plugin/#mainpy","text":"# main.py from squashtf.toolkit import make_plugin , run_plugin from .implementation import generate plugin = make_plugin ( name = 'helloworld' , description = 'A helloworld generator.' , generator = generate ) if __name__ == '__main__' : run_plugin ( plugin )","title":"main.py"},{"location":"plugins/creating-a-generator-plugin/#create-a-plugin-metadata-file","text":"Create a new plugin.yaml file in the hello-world-generator-plugin directory you created above. For more information, see \u201c Metadata syntax for Squash Orchestrator plugins .\u201d","title":"Create a plugin metadata file"},{"location":"plugins/creating-a-generator-plugin/#pluginyaml","text":"# plugin.yaml apiVersion : 'opentestfactory.org/v1alpha1' kind : 'GeneratorPlugin' metadata : name : 'HelloWorld' description : 'Greet someone' events : - category : helloworld categoryVersion : v1 cmd : 'python3 -m main' inputs : who-to-greet : description : 'Who to greet' required : false default : 'World' outputs : random-number : description : 'Random number' value : ${{ steps.random-number-generator.outputs.random-id }}","title":"plugin.yaml"},{"location":"plugins/creating-a-generator-plugin/#writing-the-plugin-code","text":"Generator plugins must return a possibly empty collection of jobs. Each job has a name and a definition. For more information about the jobs syntax, see \u201c Workflow syntax for Squash Orchestrator .\u201d Job names and step ids are local to the returned collection of jobs. They do not conflict with names and ids used in the referring workflow. The following Python script example uses the who-to-greet input variable to print \u201cHello {who-to-greet}\u201d in the log file and maps the random generated number to the random-number output variable.","title":"Writing the plugin code"},{"location":"plugins/creating-a-generator-plugin/#implementationpy","text":"# implementation.py def generate ( inputs ): jobs = { 'jobs' : { 'job1' : { 'runs-on' : 'linux' , 'steps' : [ { 'run' : 'echo Hello ' + inputs [ 'who-to-greet' ] + '.' , 'shell' : 'bash' }, { 'id' : 'random-number-generator' , 'run' : 'echo \"::set-output name=random-id::$(echo $RANDOM)\"' , 'shell' : 'bash' } ] } } } return jobs","title":"implementation.py"},{"location":"plugins/creating-a-generator-plugin/#create-a-readme","text":"To let people know how to use your plugin, you can create a README file. A README is most helpful when you plan to share your plugin publicly, but is also a great way to remind you or your team how to use the plugin. In your hello-world-generator-plugin directory, create a README.md file that specifies the following information: A detailed description of what the plugin does. Required input and outputs arguments. Optional input and outputs arguments. Environment variables the plugin uses. An example of how to use your plugin in a workflow.","title":"Create a README"},{"location":"plugins/creating-a-generator-plugin/#readmemd","text":"# Hello world generator plugin This action prints \"Hello World\" or \"Hello\" + the name of a person to greet to the log. ## Inputs ### `who-to-greets` **Required** The name of the person to greet. Default `\"World\"` . ## Outputs ### `random-id` A random number. ## Example usage generator: helloworld@v1 with: who-to-greets: 'Mona the Octocat'","title":"README.md"},{"location":"plugins/creating-a-generator-plugin/#commit-tag-push-and-deploy-your-plugin","text":"From your terminal, commit your plugin.yaml , implementation.py , helloworld.py , and README.md files. It is best practice to also add a version tag for releases of your plugin. For more information on versioning your plugin, see \u201cAbout plugins.\u201d git add plugin.yaml implementation.py helloworld.py README.md git commit -m \"My first plugin is ready\" git tag -a -m \"My first plugin release\" v1 git push --follow-tags TODO (Deploy)","title":"Commit, tag, push, and deploy your plugin"},{"location":"plugins/creating-a-generator-plugin/#testing-out-your-plugin-in-a-workflow","text":"Now you are ready to test your plugin out in a workflow.","title":"Testing out your plugin in a workflow"},{"location":"plugins/creating-a-generator-plugin/#example","text":"jobs : hello_world_job : runs-on : linux name : A job to say hello generator : helloworld@v1 with : who-to-greets : \"Mona the Octocat\"","title":"Example"},{"location":"plugins/creating-a-provider-plugin/","text":"Creating a provider plugin \u00b6 In this guide you\u2019ll learn how to build a provider plugin. Introduction \u00b6 In this guide, you will learn about the basic components needed to create and use a packaged provider plugin. To focus this guide on the components needed to package the plugin, the functionality of the plugin\u2019s code is minimal. The plugin prints \u201cHello World\u201d in the logs or \u201cHello {who-to-greet}\u201d if you provide a custom name. This guide uses the Squash Orchestrator Toolkit module to speed up development. For more information, see the squashtf/toolkit repository. Once you complete this project, you should understand how to build your own provider plugin and test it in a workflow. To ensure your plugins are compatible with all Squash Orchestrator deployments (linux, windows, \u2026), the packaged code you write should be pure and not rely on other non-portable binaries. Prerequisites \u00b6 You may find it helpful to have a basic understanding of Squash Orchestrator environment variables: Using environment variables Before you begin, you\u2019ll need to create a repository. Create a new repository on GitHub/Gitlab/BitBucket/.... You can choose any repository name or use \u201chello-world-provider-plugin\u201d like this example. Clone your repository to your computer. From your terminal, change directories into your new repository. cd hello-world-provider-plugin Create a web service \u00b6 Provider plugins are simple web services. They subscribe to specific events on startup, and publish events in response. The squashtf/toolkit module streamlines the process if you want to write your plugin in Python. For more information on doing things in a less assisted way, see \u201cWriting plugins the hard way.\u201d In your new hello-world-provider-plugin directory, create a new main.py file. main.py \u00b6 # main.py from squashtf.toolkit import make_plugin , run_plugin from .implementation import handler plugin = make_plugin ( name = 'helloworld' , description = 'A helloworld provider.' , provider = handler ) if __name__ == '__main__' : run_plugin ( plugin ) Create a plugin metadata file \u00b6 Create a new plugin.yaml file in the hello-world-provider-plugin directory you created above. For more information, see \u201c Metadata syntax for Squash Orchestrator plugins .\u201d plugin.yaml \u00b6 # plugin.yaml apiVersion : 'opentestfactory.org/v1alpha1' kind : 'ProviderPlugin' metadata : name : 'HelloWorld' description : 'Greet someone' cmd : 'python3 -m main' events : - categoryPrefix : helloworld category : greet categoryVersion : v1 inputs : who-to-greet : description : 'Who to greet' required : false default : 'World' outputs : random-number : description : 'Random number' value : ${{ steps.random-number-generator.outputs.random-id }} Writing the plugin code \u00b6 Provider plugins must return a possibly empty collection of steps. Each step has a name and a definition. For more information about the steps syntax, see \u201c Workflow syntax for Squash Orchestrator .\u201d Steps names and ids are local to the returned collection of steps. They do not conflict with names and ids used in the referring workflow. The following Python script example uses the who-to-greet input variable to print \u201cHello {who-to-greet}\u201d in the log file and maps the random generated number to the random-number output variable. implementation.py \u00b6 # implementation.py def handler ( inputs ): steps = [ { 'run' : 'echo Hello ' + inputs [ 'who-to-greet' ] + '.' , 'shell' : 'bash' }, { 'id' : 'random-number-generator' , 'run' : 'echo \"::set-output name=random-id::$(echo $RANDOM)\"' , 'shell' : 'bash' } ] return jobs Create a README \u00b6 To let people know how to use your plugin, you can create a README file. A README is most helpful when you plan to share your plugin publicly, but is also a great way to remind you or your team how to use the plugin. In your hello-world-provider-plugin directory, create a README.md file that specifies the following information: A detailed description of what the plugin does. Required input and outputs arguments. Optional input and outputs arguments. Environment variables the plugin uses. An example of how to use your plugin in a workflow. README.md \u00b6 # Hello world provider plugin This action prints \"Hello World\" or \"Hello\" + the name of a person to greet to the log. ## Inputs ### `who-to-greets` **Required** The name of the person to greet. Default `\"World\"` . ## Outputs ### `random-id` A random number. ## Example usage uses: helloworld/greet@v1 with: who-to-greets: 'Mona the Octocat' Commit, tag, push, and deploy your plugin \u00b6 From your terminal, commit your plugin.yaml , implementation.py , helloworld.py , and README.md files. It is best practice to also add a version tag for releases of your plugin. For more information on versioning your plugin, see \u201cAbout plugins.\u201d git add plugin.yaml implementation.py helloworld.py README.md git commit -m \"My first plugin is ready\" git tag -a -m \"My first plugin release\" v1 git push --follow-tags TODO (Deploy) Testing out your plugin in a workflow \u00b6 Now you are ready to test your plugin out in a workflow. Example \u00b6 jobs : hello_world_job : runs-on : linux name : A job to say hello steps : - uses : helloworld/greet@v1 with : who-to-greets : \"Mona the Octocat\"","title":"Creating a provider plugin"},{"location":"plugins/creating-a-provider-plugin/#creating-a-provider-plugin","text":"In this guide you\u2019ll learn how to build a provider plugin.","title":"Creating a provider plugin"},{"location":"plugins/creating-a-provider-plugin/#introduction","text":"In this guide, you will learn about the basic components needed to create and use a packaged provider plugin. To focus this guide on the components needed to package the plugin, the functionality of the plugin\u2019s code is minimal. The plugin prints \u201cHello World\u201d in the logs or \u201cHello {who-to-greet}\u201d if you provide a custom name. This guide uses the Squash Orchestrator Toolkit module to speed up development. For more information, see the squashtf/toolkit repository. Once you complete this project, you should understand how to build your own provider plugin and test it in a workflow. To ensure your plugins are compatible with all Squash Orchestrator deployments (linux, windows, \u2026), the packaged code you write should be pure and not rely on other non-portable binaries.","title":"Introduction"},{"location":"plugins/creating-a-provider-plugin/#prerequisites","text":"You may find it helpful to have a basic understanding of Squash Orchestrator environment variables: Using environment variables Before you begin, you\u2019ll need to create a repository. Create a new repository on GitHub/Gitlab/BitBucket/.... You can choose any repository name or use \u201chello-world-provider-plugin\u201d like this example. Clone your repository to your computer. From your terminal, change directories into your new repository. cd hello-world-provider-plugin","title":"Prerequisites"},{"location":"plugins/creating-a-provider-plugin/#create-a-web-service","text":"Provider plugins are simple web services. They subscribe to specific events on startup, and publish events in response. The squashtf/toolkit module streamlines the process if you want to write your plugin in Python. For more information on doing things in a less assisted way, see \u201cWriting plugins the hard way.\u201d In your new hello-world-provider-plugin directory, create a new main.py file.","title":"Create a web service"},{"location":"plugins/creating-a-provider-plugin/#mainpy","text":"# main.py from squashtf.toolkit import make_plugin , run_plugin from .implementation import handler plugin = make_plugin ( name = 'helloworld' , description = 'A helloworld provider.' , provider = handler ) if __name__ == '__main__' : run_plugin ( plugin )","title":"main.py"},{"location":"plugins/creating-a-provider-plugin/#create-a-plugin-metadata-file","text":"Create a new plugin.yaml file in the hello-world-provider-plugin directory you created above. For more information, see \u201c Metadata syntax for Squash Orchestrator plugins .\u201d","title":"Create a plugin metadata file"},{"location":"plugins/creating-a-provider-plugin/#pluginyaml","text":"# plugin.yaml apiVersion : 'opentestfactory.org/v1alpha1' kind : 'ProviderPlugin' metadata : name : 'HelloWorld' description : 'Greet someone' cmd : 'python3 -m main' events : - categoryPrefix : helloworld category : greet categoryVersion : v1 inputs : who-to-greet : description : 'Who to greet' required : false default : 'World' outputs : random-number : description : 'Random number' value : ${{ steps.random-number-generator.outputs.random-id }}","title":"plugin.yaml"},{"location":"plugins/creating-a-provider-plugin/#writing-the-plugin-code","text":"Provider plugins must return a possibly empty collection of steps. Each step has a name and a definition. For more information about the steps syntax, see \u201c Workflow syntax for Squash Orchestrator .\u201d Steps names and ids are local to the returned collection of steps. They do not conflict with names and ids used in the referring workflow. The following Python script example uses the who-to-greet input variable to print \u201cHello {who-to-greet}\u201d in the log file and maps the random generated number to the random-number output variable.","title":"Writing the plugin code"},{"location":"plugins/creating-a-provider-plugin/#implementationpy","text":"# implementation.py def handler ( inputs ): steps = [ { 'run' : 'echo Hello ' + inputs [ 'who-to-greet' ] + '.' , 'shell' : 'bash' }, { 'id' : 'random-number-generator' , 'run' : 'echo \"::set-output name=random-id::$(echo $RANDOM)\"' , 'shell' : 'bash' } ] return jobs","title":"implementation.py"},{"location":"plugins/creating-a-provider-plugin/#create-a-readme","text":"To let people know how to use your plugin, you can create a README file. A README is most helpful when you plan to share your plugin publicly, but is also a great way to remind you or your team how to use the plugin. In your hello-world-provider-plugin directory, create a README.md file that specifies the following information: A detailed description of what the plugin does. Required input and outputs arguments. Optional input and outputs arguments. Environment variables the plugin uses. An example of how to use your plugin in a workflow.","title":"Create a README"},{"location":"plugins/creating-a-provider-plugin/#readmemd","text":"# Hello world provider plugin This action prints \"Hello World\" or \"Hello\" + the name of a person to greet to the log. ## Inputs ### `who-to-greets` **Required** The name of the person to greet. Default `\"World\"` . ## Outputs ### `random-id` A random number. ## Example usage uses: helloworld/greet@v1 with: who-to-greets: 'Mona the Octocat'","title":"README.md"},{"location":"plugins/creating-a-provider-plugin/#commit-tag-push-and-deploy-your-plugin","text":"From your terminal, commit your plugin.yaml , implementation.py , helloworld.py , and README.md files. It is best practice to also add a version tag for releases of your plugin. For more information on versioning your plugin, see \u201cAbout plugins.\u201d git add plugin.yaml implementation.py helloworld.py README.md git commit -m \"My first plugin is ready\" git tag -a -m \"My first plugin release\" v1 git push --follow-tags TODO (Deploy)","title":"Commit, tag, push, and deploy your plugin"},{"location":"plugins/creating-a-provider-plugin/#testing-out-your-plugin-in-a-workflow","text":"Now you are ready to test your plugin out in a workflow.","title":"Testing out your plugin in a workflow"},{"location":"plugins/creating-a-provider-plugin/#example","text":"jobs : hello_world_job : runs-on : linux name : A job to say hello steps : - uses : helloworld/greet@v1 with : who-to-greets : \"Mona the Octocat\"","title":"Example"},{"location":"plugins/creating-plugins/","text":"Creating plugins \u00b6 You can create your own plugins, use and customize plugins shared by the Squash TF community, or write and share the plugins you build. About plugins Creating a generator plugin Creating a provider plugin Creating a publication plugin Creating a channel plugin","title":"Creating plugins"},{"location":"plugins/creating-plugins/#creating-plugins","text":"You can create your own plugins, use and customize plugins shared by the Squash TF community, or write and share the plugins you build. About plugins Creating a generator plugin Creating a provider plugin Creating a publication plugin Creating a channel plugin","title":"Creating plugins"},{"location":"plugins/metadata-syntax-for-squashtf-plugins/","text":"Metadata syntax for OpenTestFactory orchestrator plugins \u00b6 You can create plugins to perform tasks in your workflows. Plugins require a metadata file that uses YAML syntax. About YAML syntax for OpenTestFactory orchestrator plugins \u00b6 Plugins require a metadata file. The metadata filename must be either plugin.yml or plugin.yaml . The data in the metadata file defines the inputs, outputs and main entrypoint for your plugin. Plugin metadata files use YAML syntax. If you\u2019re new to YAML, you can read \u201c Learn YAML in five minutes .\u201d You can put more than one YAML document in your metadata file. They may all refer to the same plugin. kind \u00b6 Required The plugin type ( ChannelPlugin , ProviderPlugin , PublicationPlugin , or GeneratorPlugin ). metadata.name \u00b6 Required The name of your plugin. metadata.author \u00b6 Optional The name of the plugin\u2019s author. metadata.description \u00b6 Required A short description of the plugin. cmd \u00b6 Required The shell command used to start the plugin. events \u00b6 Required for ProviderPlugin and GeneratorPlugin plugins, not present for other plugin kinds. The event(s) to subscribe to. Possible entries are category , categoryPrefix , and categoryVersion . If categoryPrefix is not specified, category is required. categoryVersion can only be specified if at least the category or categoryPrefix is specified. Example \u00b6 This example subscribes to helloworld@v1 , helloworld@v2 , and helloworld events, but not to prefix/helloworld@v1 . events : - category : helloworld This example subscribes to prefix/foo@v1 and prefix/bar@v2 events, but not to prefix@v1 . events : - categoryPrefix : prefix This example subscribes to prefix/foo@v1 and prefix/bar@v1 events, but not to prefix/foo@v2 . events : - categoryPrefix : prefix categoryVersion : v1 This last example only subscribes to prefix/helloworld@v1 and prefix/helloworld@v2 events. events : - categoryPrefix : prefix category : helloworld categoryVersion : v1 - categoryPrefix : prefix category : helloworld categoryVersion : v2 inputs \u00b6 Optional Input parameters allow you to specify data that the plugin expects to use during runtime. The OpenTestFactory orchestrator stores input parameters as environment variables. Input ids with uppercase letters are converted to lowercase during runtime. We recommended using lowercase input ids. Example \u00b6 This example configures two inputs: numOctocats and octocatEyeColor. The numOctocats input is not required and will default to a value of \u20181\u2019. The octocatEyeColor input is required and has no default value. Workflow files that use this plugin must use the with keyword to set an input value for octocatEyeColor. For more information about the with syntax, see \u201c Workflow syntax for OpenTestFactory orchestrator .\u201d inputs : numOctocats : description : 'Number of Octocats' required : false default : '1' octocatEyeColor : description : 'Eye color of the Octocats' required : true When you specify an input to a plugin in a workflow file or use a default input value, the orchestrator creates an environment variable for the input with the name INPUT_<VARIABLE_NAME> . The environment variable created converts input names to uppercase letters and replaces spaces with _ characters. For example, if a workflow defined the numOctocats and octocatEyeColor inputs, the action code could read the values of the inputs using the INPUT_NUMOCTOCATS and INPUT_OCTOCATEYECOLOR environment variables. inputs.<input_id> \u00b6 Required A string identifier to associate with the input. The value of <input_id> is a map of the input\u2019s metadata. The <input_id> must be a unique identifier within the inputs object. The <input_id> must start with a letter or _ and contain only alphanumeric characters, - , or _ . inputs.<input_id>.description \u00b6 Required A string description of the input parameter. inputs.<input_id>.required \u00b6 Required A boolean to indicate whether the plugin requires the input parameter. Set to true when the parameter is required. inputs.<input_id>.default \u00b6 Optional A string representing the default value. The default value is used when an input parameter isn\u2019t specified in a workflow file. outputs \u00b6 Optional Output parameters allow you to declare data that a plugin sets. Plugins that run later in a workflow can use the output data set in previously run plugins. For example, if you had a plugin that performed the addition of two inputs (x + y = z), the plugin could output the sum (z) for other plugins to use as an input. If you don\u2019t declare an output in your plugin metadata file, you can still set outputs and use them in a workflow. For more information on setting outputs in a plugin, see \u201c Workflow commands for OpenTestFactory orchestrator .\u201d Example \u00b6 outputs : sum : # id of the output description : 'The sum of the inputs' outputs.<output_id> \u00b6 Required A string identifier to associate with the output. The value of <output_id> is a map of the output\u2019s metadata. The <output_id> must be a unique identifier within the outputs object. The <output_id> must start with a letter or _ and contain only alphanumeric characters, - , or _ . outputs.<output_id>.description \u00b6 Required A string description of the output parameter. outputs for composite run steps plugins \u00b6 Optional outputs use the same parameters as outputs.<output_id> and outputs.<output_id>.description (see \u201coutputs for OpenTestFactory orchestrator plugins\u201d), but also includes the value token. Example \u00b6 outputs : random-number : description : \"Random number\" value : ${{ steps.random-number-generator.outputs.random-id }} runs : using : \"composite\" steps : - id : random-number-generator run : echo \"::set-output name=random-id::$(echo $RANDOM)\" shell : bash outputs.<output_id.value> \u00b6 Required The value that the output parameter will be mapped to. You can set this to a string or an expression with context. For example, you can use the steps context to set the value of an output to the output value of a step. For more information on how to use context and expression syntax, see \u201c Context and expression syntax for OpenTestFactory workflows .\u201d branding \u00b6 You can use a color and Feather icon to create a badge to personalize and distinguish your plugin. Badges are shown next to your plugin name in the OpenTestFactory Marketplace. Example \u00b6 branding : icon : 'award' color : 'green' branding.color \u00b6 The background color of the badge. Can be one of: white , yellow , blue , green , orange , red , purple , or gray-dark . branding.icon \u00b6 The name of the Feather icon to use. activity airplay alert-circle alert-octagon alert-triangle align-center align-justify align-left align-right anchor aperture archive arrow-down-circle arrow-down-left arrow-down-right arrow-down arrow-left-circle arrow-left arrow-right-circle arrow-right arrow-up-circle arrow-up-left arrow-up-right arrow-up at-sign award bar-chart-2 bar-chart battery-charging battery bell-off bell bluetooth bold book-open book bookmark box briefcase calendar camera-off camera cast check-circle check-square check chevron-down chevron-left chevron-right chevron-up chevrons-down chevrons-left chevrons-right chevrons-up circle clipboard clock cloud-drizzle cloud-lightning cloud-off cloud-rain cloud-snow cloud code command compass copy corner-down-left corner-down-right corner-left-down corner-left-up corner-right-down corner-right-up corner-up-left corner-up-right cpu credit-card crop crosshair database delete disc dollar-sign download-cloud download droplet edit-2 edit-3 edit external-link eye-off eye facebook fast-forward feather file-minus file-plus file-text file film filter flag folder-minus folder-plus folder gift git-branch git-commit git-merge git-pull-request globe grid hard-drive hash headphones heart help-circle home image inbox info italic layers layout life-buoy link-2 link list loader lock log-in log-out mail map-pin map maximize-2 maximize menu message-circle message-square mic-off mic minimize-2 minimize minus-circle minus-square minus monitor moon more-horizontal more-vertical move music navigation-2 navigation octagon package paperclip pause-circle pause percent phone-call phone-forwarded phone-incoming phone-missed phone-off phone-outgoing phone pie-chart play-circle play plus-circle plus-square plus pocket power printer radio refresh-ccw refresh-cw repeat rewind rotate-ccw rotate-cw rss save scissors search send server settings share-2 share shield-off shield shopping-bag shopping-cart shuffle sidebar skip-back skip-forward slash sliders smartphone speaker square star stop-circle sun sunrise sunset tablet tag target terminal thermometer thumbs-down thumbs-up toggle-left toggle-right trash-2 trash trending-down trending-up triangle truck tv type umbrella underline unlock upload-cloud upload user-check user-minus user-plus user-x user users video-off video voicemail volume-1 volume-2 volume-x volume watch wifi-off wifi wind x-circle x-square x zap-off zap zoom-in zoom-out","title":"Metatadata syntax"},{"location":"plugins/metadata-syntax-for-squashtf-plugins/#metadata-syntax-for-opentestfactory-orchestrator-plugins","text":"You can create plugins to perform tasks in your workflows. Plugins require a metadata file that uses YAML syntax.","title":"Metadata syntax for OpenTestFactory orchestrator plugins"},{"location":"plugins/metadata-syntax-for-squashtf-plugins/#about-yaml-syntax-for-opentestfactory-orchestrator-plugins","text":"Plugins require a metadata file. The metadata filename must be either plugin.yml or plugin.yaml . The data in the metadata file defines the inputs, outputs and main entrypoint for your plugin. Plugin metadata files use YAML syntax. If you\u2019re new to YAML, you can read \u201c Learn YAML in five minutes .\u201d You can put more than one YAML document in your metadata file. They may all refer to the same plugin.","title":"About YAML syntax for OpenTestFactory orchestrator plugins"},{"location":"plugins/metadata-syntax-for-squashtf-plugins/#kind","text":"Required The plugin type ( ChannelPlugin , ProviderPlugin , PublicationPlugin , or GeneratorPlugin ).","title":"kind"},{"location":"plugins/metadata-syntax-for-squashtf-plugins/#metadataname","text":"Required The name of your plugin.","title":"metadata.name"},{"location":"plugins/metadata-syntax-for-squashtf-plugins/#metadataauthor","text":"Optional The name of the plugin\u2019s author.","title":"metadata.author"},{"location":"plugins/metadata-syntax-for-squashtf-plugins/#metadatadescription","text":"Required A short description of the plugin.","title":"metadata.description"},{"location":"plugins/metadata-syntax-for-squashtf-plugins/#cmd","text":"Required The shell command used to start the plugin.","title":"cmd"},{"location":"plugins/metadata-syntax-for-squashtf-plugins/#events","text":"Required for ProviderPlugin and GeneratorPlugin plugins, not present for other plugin kinds. The event(s) to subscribe to. Possible entries are category , categoryPrefix , and categoryVersion . If categoryPrefix is not specified, category is required. categoryVersion can only be specified if at least the category or categoryPrefix is specified.","title":"events"},{"location":"plugins/metadata-syntax-for-squashtf-plugins/#example","text":"This example subscribes to helloworld@v1 , helloworld@v2 , and helloworld events, but not to prefix/helloworld@v1 . events : - category : helloworld This example subscribes to prefix/foo@v1 and prefix/bar@v2 events, but not to prefix@v1 . events : - categoryPrefix : prefix This example subscribes to prefix/foo@v1 and prefix/bar@v1 events, but not to prefix/foo@v2 . events : - categoryPrefix : prefix categoryVersion : v1 This last example only subscribes to prefix/helloworld@v1 and prefix/helloworld@v2 events. events : - categoryPrefix : prefix category : helloworld categoryVersion : v1 - categoryPrefix : prefix category : helloworld categoryVersion : v2","title":"Example"},{"location":"plugins/metadata-syntax-for-squashtf-plugins/#inputs","text":"Optional Input parameters allow you to specify data that the plugin expects to use during runtime. The OpenTestFactory orchestrator stores input parameters as environment variables. Input ids with uppercase letters are converted to lowercase during runtime. We recommended using lowercase input ids.","title":"inputs"},{"location":"plugins/metadata-syntax-for-squashtf-plugins/#example_1","text":"This example configures two inputs: numOctocats and octocatEyeColor. The numOctocats input is not required and will default to a value of \u20181\u2019. The octocatEyeColor input is required and has no default value. Workflow files that use this plugin must use the with keyword to set an input value for octocatEyeColor. For more information about the with syntax, see \u201c Workflow syntax for OpenTestFactory orchestrator .\u201d inputs : numOctocats : description : 'Number of Octocats' required : false default : '1' octocatEyeColor : description : 'Eye color of the Octocats' required : true When you specify an input to a plugin in a workflow file or use a default input value, the orchestrator creates an environment variable for the input with the name INPUT_<VARIABLE_NAME> . The environment variable created converts input names to uppercase letters and replaces spaces with _ characters. For example, if a workflow defined the numOctocats and octocatEyeColor inputs, the action code could read the values of the inputs using the INPUT_NUMOCTOCATS and INPUT_OCTOCATEYECOLOR environment variables.","title":"Example"},{"location":"plugins/metadata-syntax-for-squashtf-plugins/#inputsinput_id","text":"Required A string identifier to associate with the input. The value of <input_id> is a map of the input\u2019s metadata. The <input_id> must be a unique identifier within the inputs object. The <input_id> must start with a letter or _ and contain only alphanumeric characters, - , or _ .","title":"inputs.&lt;input_id&gt;"},{"location":"plugins/metadata-syntax-for-squashtf-plugins/#inputsinput_iddescription","text":"Required A string description of the input parameter.","title":"inputs.&lt;input_id&gt;.description"},{"location":"plugins/metadata-syntax-for-squashtf-plugins/#inputsinput_idrequired","text":"Required A boolean to indicate whether the plugin requires the input parameter. Set to true when the parameter is required.","title":"inputs.&lt;input_id&gt;.required"},{"location":"plugins/metadata-syntax-for-squashtf-plugins/#inputsinput_iddefault","text":"Optional A string representing the default value. The default value is used when an input parameter isn\u2019t specified in a workflow file.","title":"inputs.&lt;input_id&gt;.default"},{"location":"plugins/metadata-syntax-for-squashtf-plugins/#outputs","text":"Optional Output parameters allow you to declare data that a plugin sets. Plugins that run later in a workflow can use the output data set in previously run plugins. For example, if you had a plugin that performed the addition of two inputs (x + y = z), the plugin could output the sum (z) for other plugins to use as an input. If you don\u2019t declare an output in your plugin metadata file, you can still set outputs and use them in a workflow. For more information on setting outputs in a plugin, see \u201c Workflow commands for OpenTestFactory orchestrator .\u201d","title":"outputs"},{"location":"plugins/metadata-syntax-for-squashtf-plugins/#example_2","text":"outputs : sum : # id of the output description : 'The sum of the inputs'","title":"Example"},{"location":"plugins/metadata-syntax-for-squashtf-plugins/#outputsoutput_id","text":"Required A string identifier to associate with the output. The value of <output_id> is a map of the output\u2019s metadata. The <output_id> must be a unique identifier within the outputs object. The <output_id> must start with a letter or _ and contain only alphanumeric characters, - , or _ .","title":"outputs.&lt;output_id&gt;"},{"location":"plugins/metadata-syntax-for-squashtf-plugins/#outputsoutput_iddescription","text":"Required A string description of the output parameter.","title":"outputs.&lt;output_id&gt;.description"},{"location":"plugins/metadata-syntax-for-squashtf-plugins/#outputs-for-composite-run-steps-plugins","text":"Optional outputs use the same parameters as outputs.<output_id> and outputs.<output_id>.description (see \u201coutputs for OpenTestFactory orchestrator plugins\u201d), but also includes the value token.","title":"outputs for composite run steps plugins"},{"location":"plugins/metadata-syntax-for-squashtf-plugins/#example_3","text":"outputs : random-number : description : \"Random number\" value : ${{ steps.random-number-generator.outputs.random-id }} runs : using : \"composite\" steps : - id : random-number-generator run : echo \"::set-output name=random-id::$(echo $RANDOM)\" shell : bash","title":"Example"},{"location":"plugins/metadata-syntax-for-squashtf-plugins/#outputsoutput_idvalue","text":"Required The value that the output parameter will be mapped to. You can set this to a string or an expression with context. For example, you can use the steps context to set the value of an output to the output value of a step. For more information on how to use context and expression syntax, see \u201c Context and expression syntax for OpenTestFactory workflows .\u201d","title":"outputs.&lt;output_id.value&gt;"},{"location":"plugins/metadata-syntax-for-squashtf-plugins/#branding","text":"You can use a color and Feather icon to create a badge to personalize and distinguish your plugin. Badges are shown next to your plugin name in the OpenTestFactory Marketplace.","title":"branding"},{"location":"plugins/metadata-syntax-for-squashtf-plugins/#example_4","text":"branding : icon : 'award' color : 'green'","title":"Example"},{"location":"plugins/metadata-syntax-for-squashtf-plugins/#brandingcolor","text":"The background color of the badge. Can be one of: white , yellow , blue , green , orange , red , purple , or gray-dark .","title":"branding.color"},{"location":"plugins/metadata-syntax-for-squashtf-plugins/#brandingicon","text":"The name of the Feather icon to use. activity airplay alert-circle alert-octagon alert-triangle align-center align-justify align-left align-right anchor aperture archive arrow-down-circle arrow-down-left arrow-down-right arrow-down arrow-left-circle arrow-left arrow-right-circle arrow-right arrow-up-circle arrow-up-left arrow-up-right arrow-up at-sign award bar-chart-2 bar-chart battery-charging battery bell-off bell bluetooth bold book-open book bookmark box briefcase calendar camera-off camera cast check-circle check-square check chevron-down chevron-left chevron-right chevron-up chevrons-down chevrons-left chevrons-right chevrons-up circle clipboard clock cloud-drizzle cloud-lightning cloud-off cloud-rain cloud-snow cloud code command compass copy corner-down-left corner-down-right corner-left-down corner-left-up corner-right-down corner-right-up corner-up-left corner-up-right cpu credit-card crop crosshair database delete disc dollar-sign download-cloud download droplet edit-2 edit-3 edit external-link eye-off eye facebook fast-forward feather file-minus file-plus file-text file film filter flag folder-minus folder-plus folder gift git-branch git-commit git-merge git-pull-request globe grid hard-drive hash headphones heart help-circle home image inbox info italic layers layout life-buoy link-2 link list loader lock log-in log-out mail map-pin map maximize-2 maximize menu message-circle message-square mic-off mic minimize-2 minimize minus-circle minus-square minus monitor moon more-horizontal more-vertical move music navigation-2 navigation octagon package paperclip pause-circle pause percent phone-call phone-forwarded phone-incoming phone-missed phone-off phone-outgoing phone pie-chart play-circle play plus-circle plus-square plus pocket power printer radio refresh-ccw refresh-cw repeat rewind rotate-ccw rotate-cw rss save scissors search send server settings share-2 share shield-off shield shopping-bag shopping-cart shuffle sidebar skip-back skip-forward slash sliders smartphone speaker square star stop-circle sun sunrise sunset tablet tag target terminal thermometer thumbs-down thumbs-up toggle-left toggle-right trash-2 trash trending-down trending-up triangle truck tv type umbrella underline unlock upload-cloud upload user-check user-minus user-plus user-x user users video-off video voicemail volume-1 volume-2 volume-x volume watch wifi-off wifi wind x-circle x-square x zap-off zap zoom-in zoom-out","title":"branding.icon"},{"location":"providers/actionprovider/","text":"actions \u00b6 actions/checkout@v2 \u00b6 Checkout a repository at a particular version. checkout actions have a mandatory repository input: - uses : actions/checkout@v1 with : repository : https://github.com/robotframework/RobotDemo.git They also allow for two optional inputs: ref , which is the git reference to checkout: a branch name, a tag name or a commit sha (defaults to the default branch if unspecified) path , which is where to clone/checkout the repository, relative to the current workspace - uses : actions/checkout@v2 with : repository : https://github.com/robotframework/RobotDemo.git ref : dev path : foo/bar Inputs \u00b6 repository (required) the repository to clone ref (optional) the branch, tag, or sha to checkout path (optional) where to clone the repository actions/create-file@v1 \u00b6 Create a file on the execution environment. create-file actions have mandatory data , format and path inputs. The following values for format are handled: ini json yaml Example \u00b6 - uses : actions/create-file@v1 with : data : foo : key1 : value1 key2 : value2 bar : key1 : value1 key2 : value2 format : ini path : foobar.ini This will create a foobar.ini file with the following content: [foo] key1 = value1 key2 = value2 [bar] key1 = value1 key2 = value2 Inputs \u00b6 data (required) the file content format (required) the file format, one of ini, json, yaml path (required) the file location, relative to the current workspace actions/delete-file@v1 \u00b6 Delete a file on the execution environment. delete-file actions have a mandatory path \u00eenput: - uses : actions/delete-file@v1 with : path : foobar.ini Inputs \u00b6 path (required) the file location, relative to the current workspace actions/get-files@v1 \u00b6 Attach a set of files from the execution environment. get-files actions attach the matching files so that publisher plugins can process them. They have mandatory pattern input. Example \u00b6 - uses : actions/get-files@v1 with : pattern : '*.xml' If you need to get files that are not in the current directory, use the working-directory statement: - uses : actions/get-files@v1 with : pattern : '*.json' working-directory : /data/foo Inputs \u00b6 pattern (required) the pattern that identify the files to attach actions/prepare-inception@v1 \u00b6 Preload the inception environment with data. prepare-inception actions have a mandatory input per file it prepares. Example \u00b6 - uses : actions/prepare-inception@v1 with export.xml : ${{ resources.files.export }} report.html : ${{ resources.files.report }} Inputs \u00b6 pattern (optional) the pattern that identify the files to attach feather.replace()","title":"actions"},{"location":"providers/actionprovider/#actions","text":"","title":"actions"},{"location":"providers/actionprovider/#actionscheckoutv2","text":"Checkout a repository at a particular version. checkout actions have a mandatory repository input: - uses : actions/checkout@v1 with : repository : https://github.com/robotframework/RobotDemo.git They also allow for two optional inputs: ref , which is the git reference to checkout: a branch name, a tag name or a commit sha (defaults to the default branch if unspecified) path , which is where to clone/checkout the repository, relative to the current workspace - uses : actions/checkout@v2 with : repository : https://github.com/robotframework/RobotDemo.git ref : dev path : foo/bar","title":" actions/checkout@v2"},{"location":"providers/actionprovider/#inputs","text":"repository (required) the repository to clone ref (optional) the branch, tag, or sha to checkout path (optional) where to clone the repository","title":"Inputs"},{"location":"providers/actionprovider/#actionscreate-filev1","text":"Create a file on the execution environment. create-file actions have mandatory data , format and path inputs. The following values for format are handled: ini json yaml","title":" actions/create-file@v1"},{"location":"providers/actionprovider/#example","text":"- uses : actions/create-file@v1 with : data : foo : key1 : value1 key2 : value2 bar : key1 : value1 key2 : value2 format : ini path : foobar.ini This will create a foobar.ini file with the following content: [foo] key1 = value1 key2 = value2 [bar] key1 = value1 key2 = value2","title":"Example"},{"location":"providers/actionprovider/#inputs_1","text":"data (required) the file content format (required) the file format, one of ini, json, yaml path (required) the file location, relative to the current workspace","title":"Inputs"},{"location":"providers/actionprovider/#actionsdelete-filev1","text":"Delete a file on the execution environment. delete-file actions have a mandatory path \u00eenput: - uses : actions/delete-file@v1 with : path : foobar.ini","title":" actions/delete-file@v1"},{"location":"providers/actionprovider/#inputs_2","text":"path (required) the file location, relative to the current workspace","title":"Inputs"},{"location":"providers/actionprovider/#actionsget-filesv1","text":"Attach a set of files from the execution environment. get-files actions attach the matching files so that publisher plugins can process them. They have mandatory pattern input.","title":" actions/get-files@v1"},{"location":"providers/actionprovider/#example_1","text":"- uses : actions/get-files@v1 with : pattern : '*.xml' If you need to get files that are not in the current directory, use the working-directory statement: - uses : actions/get-files@v1 with : pattern : '*.json' working-directory : /data/foo","title":"Example"},{"location":"providers/actionprovider/#inputs_3","text":"pattern (required) the pattern that identify the files to attach","title":"Inputs"},{"location":"providers/actionprovider/#actionsprepare-inceptionv1","text":"Preload the inception environment with data. prepare-inception actions have a mandatory input per file it prepares.","title":" actions/prepare-inception@v1"},{"location":"providers/actionprovider/#example_2","text":"- uses : actions/prepare-inception@v1 with export.xml : ${{ resources.files.export }} report.html : ${{ resources.files.report }}","title":"Example"},{"location":"providers/actionprovider/#inputs_4","text":"pattern (optional) the pattern that identify the files to attach feather.replace()","title":"Inputs"},{"location":"providers/cucumber/","text":"cucumber \u00b6 cucumber/cucumber@v1 \u00b6 Process \u2018cucumber\u2019 action. - uses : cucumber/cucumber@v1 with : test : cucumberProject/src/test/java/features/sample.feature tag : tag1 reporters : - junit - pretty - html tag is optional, allowing to target a specific tagged scenario or dataset in the .feature file. Any, all, or none of the 3 reporters can be added to the list of desired generated reports. Will attach the junit report as report.xml , the pretty report as report.json , and the html report as html-report.tar . Inputs \u00b6 test (required) the datasource to use tag (optional) target a specific tagged scenario or dataset reporters (optional) the desired reports cucumber/execute@v1 \u00b6 Process \u2018execute\u2019 action. execute actions have a mandatory test input: - uses : cucumber/execute@v1 with : test : cucumberProject/src/test/java/features/sample.feature Will attach the junit report as report.xml , the pretty report as report.json , and the html report as html-report.tar . Inputs \u00b6 test (required) the datasource to use cucumber/params@v1 \u00b6 Process \u2018params\u2019 actions. params actions have mandatory data and format inputs: - uses : cucumber/params@v1 with : data : global : test : DSNAME : tag_value format : format format must so far be SQUASHTM_FORMAT ( tm.squashtest.org/params@v1 ). Inputs \u00b6 data (required) the data to use for the automated test format (required) the format to use for the automated test data","title":"cucumber"},{"location":"providers/cucumber/#cucumber","text":"","title":"cucumber"},{"location":"providers/cucumber/#cucumbercucumberv1","text":"Process \u2018cucumber\u2019 action. - uses : cucumber/cucumber@v1 with : test : cucumberProject/src/test/java/features/sample.feature tag : tag1 reporters : - junit - pretty - html tag is optional, allowing to target a specific tagged scenario or dataset in the .feature file. Any, all, or none of the 3 reporters can be added to the list of desired generated reports. Will attach the junit report as report.xml , the pretty report as report.json , and the html report as html-report.tar .","title":"cucumber/cucumber@v1"},{"location":"providers/cucumber/#inputs","text":"test (required) the datasource to use tag (optional) target a specific tagged scenario or dataset reporters (optional) the desired reports","title":"Inputs"},{"location":"providers/cucumber/#cucumberexecutev1","text":"Process \u2018execute\u2019 action. execute actions have a mandatory test input: - uses : cucumber/execute@v1 with : test : cucumberProject/src/test/java/features/sample.feature Will attach the junit report as report.xml , the pretty report as report.json , and the html report as html-report.tar .","title":"cucumber/execute@v1"},{"location":"providers/cucumber/#inputs_1","text":"test (required) the datasource to use","title":"Inputs"},{"location":"providers/cucumber/#cucumberparamsv1","text":"Process \u2018params\u2019 actions. params actions have mandatory data and format inputs: - uses : cucumber/params@v1 with : data : global : test : DSNAME : tag_value format : format format must so far be SQUASHTM_FORMAT ( tm.squashtest.org/params@v1 ).","title":"cucumber/params@v1"},{"location":"providers/cucumber/#inputs_2","text":"data (required) the data to use for the automated test format (required) the format to use for the automated test data","title":"Inputs"},{"location":"providers/cypress/","text":"cypress \u00b6 cypress/cypress@v1 \u00b6 Process \u2018cypress\u2019 action. If the action is used more than once in a job, it is up to the caller to ensure no previous test execution results remains before executing a new test. It is also up to the caller to attach the relevant reports so that publishers can do their job too, by using the actions/get-files@v1 action or some other means. Example \u00b6 - uses : cypress/cypress@v1 with : browser : chrome reporter : junit reporter-options : \"mochaFile=mocha_results/test-output-[hash].xml,toConsole=true\" headless : true env : profile=postgres config-file : cypress/config/... Inputs \u00b6 browser (optional) the browser to use reporter (optional) the report format to use reporter-options (optional) additionnal flags for the report generation headless (optional) a boolean, false by default env (optional) additionnal properties config-file (optional) configuration file path cypress/execute@v1 \u00b6 Process \u2018execute\u2019 action. execute actions have a mandatory test input. Example \u00b6 - uses : cypress/execute@v1 with : test : cypressProject/cypress/test.spec.js Inputs \u00b6 test (required) the datasource to use cypress/params@v1 \u00b6 Process \u2018params\u2019 actions. params actions have mandatory data and format inputs. format must so far be SQUASHTM_FORMAT ( tm.squashtest.org/params@v1 ). Example \u00b6 - uses : cypress/params@v1 with : data : global : key1 : value1 key2 : value2 test : key1 : value1 key3 : value3 format : format Inputs \u00b6 data (required) the data to use for the automated test format (required) the format to use for the automated test data","title":"cypress"},{"location":"providers/cypress/#cypress","text":"","title":"cypress"},{"location":"providers/cypress/#cypresscypressv1","text":"Process \u2018cypress\u2019 action. If the action is used more than once in a job, it is up to the caller to ensure no previous test execution results remains before executing a new test. It is also up to the caller to attach the relevant reports so that publishers can do their job too, by using the actions/get-files@v1 action or some other means.","title":"cypress/cypress@v1"},{"location":"providers/cypress/#example","text":"- uses : cypress/cypress@v1 with : browser : chrome reporter : junit reporter-options : \"mochaFile=mocha_results/test-output-[hash].xml,toConsole=true\" headless : true env : profile=postgres config-file : cypress/config/...","title":"Example"},{"location":"providers/cypress/#inputs","text":"browser (optional) the browser to use reporter (optional) the report format to use reporter-options (optional) additionnal flags for the report generation headless (optional) a boolean, false by default env (optional) additionnal properties config-file (optional) configuration file path","title":"Inputs"},{"location":"providers/cypress/#cypressexecutev1","text":"Process \u2018execute\u2019 action. execute actions have a mandatory test input.","title":"cypress/execute@v1"},{"location":"providers/cypress/#example_1","text":"- uses : cypress/execute@v1 with : test : cypressProject/cypress/test.spec.js","title":"Example"},{"location":"providers/cypress/#inputs_1","text":"test (required) the datasource to use","title":"Inputs"},{"location":"providers/cypress/#cypressparamsv1","text":"Process \u2018params\u2019 actions. params actions have mandatory data and format inputs. format must so far be SQUASHTM_FORMAT ( tm.squashtest.org/params@v1 ).","title":"cypress/params@v1"},{"location":"providers/cypress/#example_2","text":"- uses : cypress/params@v1 with : data : global : key1 : value1 key2 : value2 test : key1 : value1 key3 : value3 format : format","title":"Example"},{"location":"providers/cypress/#inputs_2","text":"data (required) the data to use for the automated test format (required) the format to use for the automated test data","title":"Inputs"},{"location":"providers/junit/","text":"junit \u00b6 junit/junit@v1 \u00b6 Process \u2018junit\u2019 action. junit actions have a mandatory datasource input: - uses : junit/junit@v1 with : datasource : foobar Inputs \u00b6 datasource (required) the datasource to use junit/mvntest@v1 \u00b6 Process \u2018mvntest\u2019 action. mvntest actions have a mandatory test input. It may contain a properties input too. This action will attach all *.txt and *.xml files found in the target/surefire-reports directory. Example \u00b6 - uses : junit/mvntest@v1 with : test : class#method properties : foo : value1 bar : value2 Inputs \u00b6 test (required) the datasource to use properties (optional) the additional properties to use junit/execute@v1 \u00b6 An \u2018execute\u2019 action for use by generators. execute actions have a mandatory test input: - uses : junit/execute@v1 with : test : path/to/test/root/qualified.testsuite.ClassName#testName The path to test root is a relative path from the step\u2019s working directory. If not specified, this is the job\u2019s working directory, in which case it must include the repository name. Inputs \u00b6 test (required) the datasource to use junit/params@v1 \u00b6 A \u2018params\u2019 actions for use by generators. params actions have mandatory data and format inputs. format must so far be SQUASHTM_FORMAT ( tm.squashtest.org/params@v1 ). Example \u00b6 - uses : junit/params@v1 with : data : key1 : value1 key2 : value2 format : format Inputs \u00b6 data (required) the data to use for the automated test format (required) the format to use for the automated test data","title":"junit"},{"location":"providers/junit/#junit","text":"","title":"junit"},{"location":"providers/junit/#junitjunitv1","text":"Process \u2018junit\u2019 action. junit actions have a mandatory datasource input: - uses : junit/junit@v1 with : datasource : foobar","title":"junit/junit@v1"},{"location":"providers/junit/#inputs","text":"datasource (required) the datasource to use","title":"Inputs"},{"location":"providers/junit/#junitmvntestv1","text":"Process \u2018mvntest\u2019 action. mvntest actions have a mandatory test input. It may contain a properties input too. This action will attach all *.txt and *.xml files found in the target/surefire-reports directory.","title":"junit/mvntest@v1"},{"location":"providers/junit/#example","text":"- uses : junit/mvntest@v1 with : test : class#method properties : foo : value1 bar : value2","title":"Example"},{"location":"providers/junit/#inputs_1","text":"test (required) the datasource to use properties (optional) the additional properties to use","title":"Inputs"},{"location":"providers/junit/#junitexecutev1","text":"An \u2018execute\u2019 action for use by generators. execute actions have a mandatory test input: - uses : junit/execute@v1 with : test : path/to/test/root/qualified.testsuite.ClassName#testName The path to test root is a relative path from the step\u2019s working directory. If not specified, this is the job\u2019s working directory, in which case it must include the repository name.","title":"junit/execute@v1"},{"location":"providers/junit/#inputs_2","text":"test (required) the datasource to use","title":"Inputs"},{"location":"providers/junit/#junitparamsv1","text":"A \u2018params\u2019 actions for use by generators. params actions have mandatory data and format inputs. format must so far be SQUASHTM_FORMAT ( tm.squashtest.org/params@v1 ).","title":"junit/params@v1"},{"location":"providers/junit/#example_1","text":"- uses : junit/params@v1 with : data : key1 : value1 key2 : value2 format : format","title":"Example"},{"location":"providers/junit/#inputs_3","text":"data (required) the data to use for the automated test format (required) the format to use for the automated test data","title":"Inputs"},{"location":"providers/robotframework/","text":"robotframework \u00b6 robotframework/robot@v1 \u00b6 Run a Robot Framework test suite. If you want to generate Allure reports, the allure-robotframework Python library must be installed in the execution environment. Examples \u00b6 This first example runs all tests in the foobar test suite: - uses : robotframework/robot@v1 with : datasource : foobar This second example runs the foo test in the foobar test suite, and an Allure report will be generated: - uses : robotframework/robot@v1 with : datasource : foobar test : foo report_for_allure : true Inputs \u00b6 datasource (required) The datasource to use. test (optional) Specify a test case present in the datasource. By default, all test cases in the datasource are executed. report_for_allure (optional) A boolean. Set to true to enable the generation of Allure reports. By default, Allure reports are not generated. robotframework/execute@v1 \u00b6 An execute action for use by generators. Runs a test suite or a test case in a test suite. Example \u00b6 - uses : robotframework/execute@v1 with : test : foobar Inputs \u00b6 test (required) The test suite (and optional test case) to execute. It is of the form: {datasource}[#{testcase}] robotframework/params@v1 \u00b6 A params action for use by generators. Example \u00b6 - uses : robotframework/params@v1 with : data : key1 : value1 key2 : value2 format : format Inputs \u00b6 data (required) The data to use for the automated test. format (required) The format to use for the automated test data. format must so far be SQUASHTM_FORMAT ( tm.squashtest.org/params@v1 ). feather.replace()","title":"robotframework"},{"location":"providers/robotframework/#robotframework","text":"","title":"robotframework"},{"location":"providers/robotframework/#robotframeworkrobotv1","text":"Run a Robot Framework test suite. If you want to generate Allure reports, the allure-robotframework Python library must be installed in the execution environment.","title":" robotframework/robot@v1"},{"location":"providers/robotframework/#examples","text":"This first example runs all tests in the foobar test suite: - uses : robotframework/robot@v1 with : datasource : foobar This second example runs the foo test in the foobar test suite, and an Allure report will be generated: - uses : robotframework/robot@v1 with : datasource : foobar test : foo report_for_allure : true","title":"Examples"},{"location":"providers/robotframework/#inputs","text":"datasource (required) The datasource to use. test (optional) Specify a test case present in the datasource. By default, all test cases in the datasource are executed. report_for_allure (optional) A boolean. Set to true to enable the generation of Allure reports. By default, Allure reports are not generated.","title":"Inputs"},{"location":"providers/robotframework/#robotframeworkexecutev1","text":"An execute action for use by generators. Runs a test suite or a test case in a test suite.","title":" robotframework/execute@v1"},{"location":"providers/robotframework/#example","text":"- uses : robotframework/execute@v1 with : test : foobar","title":"Example"},{"location":"providers/robotframework/#inputs_1","text":"test (required) The test suite (and optional test case) to execute. It is of the form: {datasource}[#{testcase}]","title":"Inputs"},{"location":"providers/robotframework/#robotframeworkparamsv1","text":"A params action for use by generators.","title":" robotframework/params@v1"},{"location":"providers/robotframework/#example_1","text":"- uses : robotframework/params@v1 with : data : key1 : value1 key2 : value2 format : format","title":"Example"},{"location":"providers/robotframework/#inputs_2","text":"data (required) The data to use for the automated test. format (required) The format to use for the automated test data. format must so far be SQUASHTM_FORMAT ( tm.squashtest.org/params@v1 ). feather.replace()","title":"Inputs"},{"location":"providers/soapui/","text":"soapui \u00b6 soapui/soapui@v1 \u00b6 Process \u2018soapui\u2019 action. soapui actions have a mandatory project input. If the action is used more than once in a job, it is up to the caller to ensure no previous test execution results remains before executing a new test. It is also up to the caller to attach the relevant reports so that publishers can do their job too, by using the actions/get-files@v1 action or some other means. Example \u00b6 - uses : soapui/soapui@v1 with : project : foo - uses : soapui/soapui@v1 with : project : foo testcase : bar testsuite : baz user : user host : host:port endpoint : https://host:port/foo properties : PROPERTY1 : value1 PROPERTY2 : value2 format : PDF type : JUnit-Style HTML Report target : foo/bar Inputs \u00b6 project (required) the project to run testcase (optional) the test case in the project to run (-c option) testsuite (optional) the test suite in the project to run (-s option) user (optional) the user to use in test requests authorization (-u option) host (optional) the host and port to use in test requests (-h option) endpoint (optional) the endpoint to use in test requests (-e option) properties (optional) define the value(s) of system properties (-D options) format (optional) the report format(s), a comma-separated string (-F option) type (optional) the type of the report format (-R option) target (optional) the root directory where the runner save reports (-f option) soapui/execute@v1 \u00b6 An \u2018execute\u2019 action for use by generators. execute actions have a mandatory test input. Example \u00b6 - uses : soapui/execute@v1 with : test : path/to/test.xml#testsuiteName#testCaseName Inputs \u00b6 test (required) the datasource to use soapui/execute@v1 \u00b6 A \u2018params\u2019 action for use by generators. params actions have mandatory data and format inputs. format must so far be SQUASHTM_FORMAT ( tm.squashtest.org/params@v1 ). Example \u00b6 - uses : junit/params@v1 with : data : key1 : value1 key2 : value2 format : format Inputs \u00b6 data (required) the data to use for the automated test format (required) the format to use for the automated test data","title":"soapui"},{"location":"providers/soapui/#soapui","text":"","title":"soapui"},{"location":"providers/soapui/#soapuisoapuiv1","text":"Process \u2018soapui\u2019 action. soapui actions have a mandatory project input. If the action is used more than once in a job, it is up to the caller to ensure no previous test execution results remains before executing a new test. It is also up to the caller to attach the relevant reports so that publishers can do their job too, by using the actions/get-files@v1 action or some other means.","title":"soapui/soapui@v1"},{"location":"providers/soapui/#example","text":"- uses : soapui/soapui@v1 with : project : foo - uses : soapui/soapui@v1 with : project : foo testcase : bar testsuite : baz user : user host : host:port endpoint : https://host:port/foo properties : PROPERTY1 : value1 PROPERTY2 : value2 format : PDF type : JUnit-Style HTML Report target : foo/bar","title":"Example"},{"location":"providers/soapui/#inputs","text":"project (required) the project to run testcase (optional) the test case in the project to run (-c option) testsuite (optional) the test suite in the project to run (-s option) user (optional) the user to use in test requests authorization (-u option) host (optional) the host and port to use in test requests (-h option) endpoint (optional) the endpoint to use in test requests (-e option) properties (optional) define the value(s) of system properties (-D options) format (optional) the report format(s), a comma-separated string (-F option) type (optional) the type of the report format (-R option) target (optional) the root directory where the runner save reports (-f option)","title":"Inputs"},{"location":"providers/soapui/#soapuiexecutev1","text":"An \u2018execute\u2019 action for use by generators. execute actions have a mandatory test input.","title":"soapui/execute@v1"},{"location":"providers/soapui/#example_1","text":"- uses : soapui/execute@v1 with : test : path/to/test.xml#testsuiteName#testCaseName","title":"Example"},{"location":"providers/soapui/#inputs_1","text":"test (required) the datasource to use","title":"Inputs"},{"location":"providers/soapui/#soapuiexecutev1_1","text":"A \u2018params\u2019 action for use by generators. params actions have mandatory data and format inputs. format must so far be SQUASHTM_FORMAT ( tm.squashtest.org/params@v1 ).","title":"soapui/execute@v1"},{"location":"providers/soapui/#example_2","text":"- uses : junit/params@v1 with : data : key1 : value1 key2 : value2 format : format","title":"Example"},{"location":"providers/soapui/#inputs_2","text":"data (required) the data to use for the automated test format (required) the format to use for the automated test data","title":"Inputs"},{"location":"reference/context-and-expression-syntax/","text":"Context and expression syntax for OpenTestFactory workflows \u00b6 You can access context information and evaluate expressions in workflows. About contexts and expressions \u00b6 You can use expressions to programmatically set variables in workflow files and access contexts. An expression can be any combination of literal values, references to a context, or functions. You can combine literals, context references, and functions using operators. Expressions are commonly used with the conditional if keyword in a workflow file to determine whether a step should run. When an if conditional is true , the step will run. You need to use specific syntax to tell the OpenTestFactory orchestrator to evaluate an expression rather than treat it as a string. ${{ <expression> }} When you use expressions in an if conditional, you may omit the expression syntax ( ${{ }} ) because the orchestrator automatically evaluates the if conditional as an expression. For more information about if conditionals, see \u201c Workflow syntax for OpenTestFactory orchestrator .\u201d Example expression in an if conditional \u00b6 steps : - uses : robotframework/robot@v1 if : ${{ <expression> }} Example setting an environment variable \u00b6 variables : my_env_var : ${{ <expression> }} Contexts \u00b6 Contexts are a way to access information about workflow runs, execution environments, jobs, and steps. Contexts use the expression syntax. ${{ <context> }} Context name Type Description squashtf object Information about the workflow run. For more information, see squashtf context . variables object Contains environment variables set in a workflow, job, or step. For more information, see variables context . resources object Information about the resources set in a workflow. For more information, see resources context . job object Information about the currently executing job. For more information, see job context . steps object Information about the steps that have been run in this job. For more information, see steps context . runner object Information about the execution environment that is running the current job. For more information, see runner context . needs object Enables access to the outputs of all jobs that are defined as a dependency of the current job. For more information, see needs context . As part of an expression, you may access context information using one of two syntaxes. Index syntax: squashtf['job'] Property dereference syntax: squashtf.job In order to use property dereference syntax, the property name must: start with a-Z or _ . be followed by a-Z 0-9 - or _ . squashtf context \u00b6 The squashtf context contains information about the workflow run and the event that triggered the run. You can read most of the squashtf context data in environment variables. For more information about environment variables, see \u201c Using environment variables .\u201d Warning : When using the whole squashtf context, be mindful that it includes sensitive information such as squashtf.token . The orchestrator masks secrets when they are printed to the console, but you should be cautious when exporting or printing the context . Property name Type Description squashtf object The top-level context available during any job or step in a workflow. squashtf.workflow string The name of the workflow. If the workflow file doesn\u2019t specify a name, the value of this property is the full path of the workflow file in the repository. squashtf.job string The job_id of the current job. squashtf.actor string The login of the user that initiated the workflow run. squashtf.token string A token to authenticate on behalf of the orchestrator plugin. squashtf.step string The name of the step currently running. The orchestrator removes special characters or uses the name run when the current step runs a script. If you use the same action more than once in the same job, the name will include a suffix with the sequence number. For example, the first script you run will have the name run1 , and the second script will be named run2 . Similarly, the second invocation of actions/checkout will be actionscheckout2 . variables context \u00b6 The variables context contains environment variables that have been set in a workflow, job, or step. For more information about setting environment variables in your workflow, see \u201c Workflow syntax for OpenTestFactory orchestrator .\u201d The variables context syntax allows you to use the value of an environment variable in your workflow file. If you want to use the value of an environment variable inside an execution environment, use the execution environment operating system\u2019s normal method for reading environment variables. You can only use the variables context in the value of the with and name keys, or in a step\u2019s if conditional. For more information on the step syntax, see \u201c Workflow syntax for OpenTestFactory orchestrator .\u201d Property name Type Description variables object This context changes for each step in a job. You can access this context from any step in a job. variables.<var name> string The value of a specific environment variable. resources context \u00b6 The resources context contains resources that have been set in a workflow. For more information about setting resources in your workflow, see \u201c Workflow syntax for OpenTestFactory orchestrator .\u201d The resources context syntax allow you to use the resources in your workflow file. You can only use the resources context in the value of the with and name keys. For more information on the step syntax, see \u201c Workflow syntax for OpenTestFactory orchestrator .\u201d Property name Type Description resources.<resource type> object The resource type objects. Possible values are testmanagers, repositories, or files. resources.<resource type>.<name> object The set of properties for the resource. There is at least a name property, the other are resource type dependent. job context \u00b6 The job context contains information about the currently running job. Property name Type Description job object This context changes for each job in a workflow run. You can access this context from any step in a job. job.status string The current status of the job. Possible values are success , failure , or cancelled . steps context \u00b6 The steps context contains information about the steps in the current job that have already run. Property name Type Description steps object This context changes for each step in a job. You can access this context from any step in a job. steps.<step id>.outputs object The set of outputs defined for the step. For more information, see \u201cMetadata syntax for GitHub Actions.\u201d steps.<step id>.outputs.<output name> string The value of a specific output. steps.<step id>.outcome string The result of a completed step before continue-on-error is applied. Possible values are success , failure , cancelled , or skipped . When a continue-on-error step fails, the outcome is failure , but the final conclusion is success . steps.<step id>.conclusion string The result of a completed step after continue-on-error is applied. Possible values are success , failure , cancelled , or skipped . When a continue-on-error step fails, the outcome is failure , but the final conclusion is success . runner context \u00b6 The runner context contains information about the execution environment that is executing the current job. Property name Type Description runner.os string The operating system of the runner executing the job. Possible values are Linux, Windows, or macOS. runner.temp string The path of the temporary directory for the execution environment. This directory is guaranteed to be empty at the start of each job, even on self-hosted execution environments. needs context \u00b6 The needs context contains outputs from all jobs that are defined as a dependency of the current job. For more information on defining job dependencies, see \u201c Workflow syntax for OpenTestFactory orchestrator .\u201d Property name Type Description needs.<job id> object A single job that the current job depends on. needs.<job id>.result string The result of a job that the current job depends on. Possible values are success , failure , or cancelled . needs.<job id>.outputs object The set of outputs of a job that the current job depends on. needs.<job id>.outputs.<output name> string The value of a specific output for a job that the current job depends on. Literals \u00b6 As part of an expression, you can use boolean , null , number , or string data types. Boolean literals are not case sensitive, so you can use true or True. Data type Literal value boolean true or false null null number Any number format supported by JSON. string You must use single quotes. Escape literal single-quotes with a single quote. Example \u00b6 variables : myNull : ${{ null }} myBoolean : ${{ false }} myIntegerNumber : ${{ 711 }} myFloatNumber : ${{ -9.2 }} myHexNumber : ${{ 0xff }} myExponentialNumber : ${{ -2.99-e2 }} myString : ${{ 'Mona the Octocat' }} myEscapedString : ${{ 'It''s open source!' }} Operators \u00b6 Operator Description [ ] Index . Property dereference ! Not < Less than <= Less than or equal > Greater than >= Greater than or equal == Equal != Not equal The orchestrator performs loose equality comparisons. If the types do not match, the orchestrator coerces the type to a number. The orchestrator casts data types to a number using these conversions: Type Result Null 0 Boolean true returns 1 false returns 0 String Parsed from any legal JSON number format, otherwise NaN . Note: empty string returns 0 . Array NaN Object NaN A comparison of one NaN to another NaN does not result in true . For more information, see the \u201c NaN Mozilla docs .\u201d The orchestrator ignores case when comparing strings. Objects and arrays are only considered equal when they are the same instance. Functions \u00b6 The OpenTestFactory orchestrator offers a set of built-in functions that you can use in expressions. Some functions cast values to a string to perform comparisons. The orchestrator casts data types to a string using these conversions: Type Result Null '' Boolean 'true' or 'false' Number Decimal format, exponential for large numbers Array Arrays are not converted to a string Object Objects are not converted to a string contains \u00b6 contains ( search , item ) Returns true if search contains item . If search is an array, this function returns true if the item is an element in the array. If search is a string, this function returns true if the item is a substring of search . This function is not case sensitive. Casts values to a string. Example using an array \u00b6 contains ( github . event . issue . labels . * . name , 'bug' ) Example using a string \u00b6 contains ( 'Hello world' , 'llo' ) returns true startsWith \u00b6 startsWith ( searchString , searchValue ) Returns true when searchString starts with searchValue . This function is not case sensitive. Casts values to a string. Example \u00b6 startsWith ( 'Hello world' , 'He' ) returns true endsWith \u00b6 endsWith ( searchString , searchValue ) Returns true if searchString ends with searchValue . This function is not case sensitive. Casts values to a string. Example \u00b6 endsWith ( 'Hello world' , 'ld' ) returns true format \u00b6 format ( string , replaceValue0 , replaceValue1 , ..., replaceValueN ) Replaces values in the string, with the variable replaceValueN . Variables in the string are specified using the {N} syntax, where N is an integer. You must specify at least one replaceValue and string . There is no maximum for the number of variables ( replaceValueN ) you can use. Escape curly braces using double braces. Example \u00b6 Returns \u2018Hello Mona the Octocat\u2019 format ( 'Hello {0} {1} {2}' , 'Mona' , 'the' , 'Octocat' ) Example escaping braces \u00b6 Returns \u2018{Hello Mona the Octocat!}\u2019 format ( '{{Hello {0} {1} {2}!}}' , 'Mona' , 'the' , 'Octocat' ) join \u00b6 join ( array , optionalSeparator ) The value for array can be an array or a string. All values in array are concatenated into a string. If you provide optionalSeparator , it is inserted between the concatenated values. Otherwise, the default separator , is used. Casts values to a string. Example \u00b6 join ( github . event . issue . labels . * . name , ', ' ) may return 'bug, help wanted' toJson \u00b6 toJSON ( value ) Returns a pretty-print JSON representation of value . You can use this function to debug the information provided in contexts. Example \u00b6 toJSON ( job ) might return { \"status\" : \"Success\" } fromJson \u00b6 fromJSON ( value ) Returns a JSON object for value . You can use this function to provide a JSON object as an evaluated expression. Example \u00b6 This workflow sets a JSON matrix in one job, and passes it to the next job using an output and fromJSON . metadata : name : build jobs : job1 : runs-on : ubuntu-latest outputs : matrix : ${{ steps.set-matrix.outputs.matrix }} steps : - id : set-matrix run : echo \"::set-output name=matrix::{\\\"include\\\":[{\\\"project\\\":\\\"foo\\\",\\\"config\\\":\\\"Debug\\\"},{\\\"project\\\":\\\"bar\\\",\\\"config\\\":\\\"Release\\\"}]}\" job2 : needs : job1 runs-on : ubuntu-latest strategy : matrix : ${{fromJson(needs.job1.outputs.matrix)}} steps : - run : build Job status check functions \u00b6 You can use the following status check functions as expressions in if conditionals. If your if expression does not contain any of the status functions it will automatically result with success() . For more information about if conditionals, see \u201c Workflow syntax for OpenTestFactory orchestrator .\u201d success \u00b6 Returns true when none of the previous steps have failed or been canceled. Example \u00b6 steps : ... - name : The job has succeeded if : ${{ success() }} always \u00b6 Always returns true , even when canceled. A job or step will not run when a critical failure prevents the task from running. For example, if getting sources failed. Example \u00b6 if : ${{ always() }} cancelled \u00b6 Returns true if the workflow was canceled. Example \u00b6 if : ${{ cancelled() }} failure \u00b6 Returns true when any previous step of a job fails. Example \u00b6 steps : ... - name : The job has failed if : ${{ failure() }}","title":"Context and expression syntax"},{"location":"reference/context-and-expression-syntax/#context-and-expression-syntax-for-opentestfactory-workflows","text":"You can access context information and evaluate expressions in workflows.","title":"Context and expression syntax for OpenTestFactory workflows"},{"location":"reference/context-and-expression-syntax/#about-contexts-and-expressions","text":"You can use expressions to programmatically set variables in workflow files and access contexts. An expression can be any combination of literal values, references to a context, or functions. You can combine literals, context references, and functions using operators. Expressions are commonly used with the conditional if keyword in a workflow file to determine whether a step should run. When an if conditional is true , the step will run. You need to use specific syntax to tell the OpenTestFactory orchestrator to evaluate an expression rather than treat it as a string. ${{ <expression> }} When you use expressions in an if conditional, you may omit the expression syntax ( ${{ }} ) because the orchestrator automatically evaluates the if conditional as an expression. For more information about if conditionals, see \u201c Workflow syntax for OpenTestFactory orchestrator .\u201d","title":"About contexts and expressions"},{"location":"reference/context-and-expression-syntax/#example-expression-in-an-if-conditional","text":"steps : - uses : robotframework/robot@v1 if : ${{ <expression> }}","title":"Example expression in an if conditional"},{"location":"reference/context-and-expression-syntax/#example-setting-an-environment-variable","text":"variables : my_env_var : ${{ <expression> }}","title":"Example setting an environment variable"},{"location":"reference/context-and-expression-syntax/#contexts","text":"Contexts are a way to access information about workflow runs, execution environments, jobs, and steps. Contexts use the expression syntax. ${{ <context> }} Context name Type Description squashtf object Information about the workflow run. For more information, see squashtf context . variables object Contains environment variables set in a workflow, job, or step. For more information, see variables context . resources object Information about the resources set in a workflow. For more information, see resources context . job object Information about the currently executing job. For more information, see job context . steps object Information about the steps that have been run in this job. For more information, see steps context . runner object Information about the execution environment that is running the current job. For more information, see runner context . needs object Enables access to the outputs of all jobs that are defined as a dependency of the current job. For more information, see needs context . As part of an expression, you may access context information using one of two syntaxes. Index syntax: squashtf['job'] Property dereference syntax: squashtf.job In order to use property dereference syntax, the property name must: start with a-Z or _ . be followed by a-Z 0-9 - or _ .","title":"Contexts"},{"location":"reference/context-and-expression-syntax/#squashtf-context","text":"The squashtf context contains information about the workflow run and the event that triggered the run. You can read most of the squashtf context data in environment variables. For more information about environment variables, see \u201c Using environment variables .\u201d Warning : When using the whole squashtf context, be mindful that it includes sensitive information such as squashtf.token . The orchestrator masks secrets when they are printed to the console, but you should be cautious when exporting or printing the context . Property name Type Description squashtf object The top-level context available during any job or step in a workflow. squashtf.workflow string The name of the workflow. If the workflow file doesn\u2019t specify a name, the value of this property is the full path of the workflow file in the repository. squashtf.job string The job_id of the current job. squashtf.actor string The login of the user that initiated the workflow run. squashtf.token string A token to authenticate on behalf of the orchestrator plugin. squashtf.step string The name of the step currently running. The orchestrator removes special characters or uses the name run when the current step runs a script. If you use the same action more than once in the same job, the name will include a suffix with the sequence number. For example, the first script you run will have the name run1 , and the second script will be named run2 . Similarly, the second invocation of actions/checkout will be actionscheckout2 .","title":"squashtf context"},{"location":"reference/context-and-expression-syntax/#variables-context","text":"The variables context contains environment variables that have been set in a workflow, job, or step. For more information about setting environment variables in your workflow, see \u201c Workflow syntax for OpenTestFactory orchestrator .\u201d The variables context syntax allows you to use the value of an environment variable in your workflow file. If you want to use the value of an environment variable inside an execution environment, use the execution environment operating system\u2019s normal method for reading environment variables. You can only use the variables context in the value of the with and name keys, or in a step\u2019s if conditional. For more information on the step syntax, see \u201c Workflow syntax for OpenTestFactory orchestrator .\u201d Property name Type Description variables object This context changes for each step in a job. You can access this context from any step in a job. variables.<var name> string The value of a specific environment variable.","title":"variables context"},{"location":"reference/context-and-expression-syntax/#resources-context","text":"The resources context contains resources that have been set in a workflow. For more information about setting resources in your workflow, see \u201c Workflow syntax for OpenTestFactory orchestrator .\u201d The resources context syntax allow you to use the resources in your workflow file. You can only use the resources context in the value of the with and name keys. For more information on the step syntax, see \u201c Workflow syntax for OpenTestFactory orchestrator .\u201d Property name Type Description resources.<resource type> object The resource type objects. Possible values are testmanagers, repositories, or files. resources.<resource type>.<name> object The set of properties for the resource. There is at least a name property, the other are resource type dependent.","title":"resources context"},{"location":"reference/context-and-expression-syntax/#job-context","text":"The job context contains information about the currently running job. Property name Type Description job object This context changes for each job in a workflow run. You can access this context from any step in a job. job.status string The current status of the job. Possible values are success , failure , or cancelled .","title":"job context"},{"location":"reference/context-and-expression-syntax/#steps-context","text":"The steps context contains information about the steps in the current job that have already run. Property name Type Description steps object This context changes for each step in a job. You can access this context from any step in a job. steps.<step id>.outputs object The set of outputs defined for the step. For more information, see \u201cMetadata syntax for GitHub Actions.\u201d steps.<step id>.outputs.<output name> string The value of a specific output. steps.<step id>.outcome string The result of a completed step before continue-on-error is applied. Possible values are success , failure , cancelled , or skipped . When a continue-on-error step fails, the outcome is failure , but the final conclusion is success . steps.<step id>.conclusion string The result of a completed step after continue-on-error is applied. Possible values are success , failure , cancelled , or skipped . When a continue-on-error step fails, the outcome is failure , but the final conclusion is success .","title":"steps context"},{"location":"reference/context-and-expression-syntax/#runner-context","text":"The runner context contains information about the execution environment that is executing the current job. Property name Type Description runner.os string The operating system of the runner executing the job. Possible values are Linux, Windows, or macOS. runner.temp string The path of the temporary directory for the execution environment. This directory is guaranteed to be empty at the start of each job, even on self-hosted execution environments.","title":"runner context"},{"location":"reference/context-and-expression-syntax/#needs-context","text":"The needs context contains outputs from all jobs that are defined as a dependency of the current job. For more information on defining job dependencies, see \u201c Workflow syntax for OpenTestFactory orchestrator .\u201d Property name Type Description needs.<job id> object A single job that the current job depends on. needs.<job id>.result string The result of a job that the current job depends on. Possible values are success , failure , or cancelled . needs.<job id>.outputs object The set of outputs of a job that the current job depends on. needs.<job id>.outputs.<output name> string The value of a specific output for a job that the current job depends on.","title":"needs context"},{"location":"reference/context-and-expression-syntax/#literals","text":"As part of an expression, you can use boolean , null , number , or string data types. Boolean literals are not case sensitive, so you can use true or True. Data type Literal value boolean true or false null null number Any number format supported by JSON. string You must use single quotes. Escape literal single-quotes with a single quote.","title":"Literals"},{"location":"reference/context-and-expression-syntax/#example","text":"variables : myNull : ${{ null }} myBoolean : ${{ false }} myIntegerNumber : ${{ 711 }} myFloatNumber : ${{ -9.2 }} myHexNumber : ${{ 0xff }} myExponentialNumber : ${{ -2.99-e2 }} myString : ${{ 'Mona the Octocat' }} myEscapedString : ${{ 'It''s open source!' }}","title":"Example"},{"location":"reference/context-and-expression-syntax/#operators","text":"Operator Description [ ] Index . Property dereference ! Not < Less than <= Less than or equal > Greater than >= Greater than or equal == Equal != Not equal The orchestrator performs loose equality comparisons. If the types do not match, the orchestrator coerces the type to a number. The orchestrator casts data types to a number using these conversions: Type Result Null 0 Boolean true returns 1 false returns 0 String Parsed from any legal JSON number format, otherwise NaN . Note: empty string returns 0 . Array NaN Object NaN A comparison of one NaN to another NaN does not result in true . For more information, see the \u201c NaN Mozilla docs .\u201d The orchestrator ignores case when comparing strings. Objects and arrays are only considered equal when they are the same instance.","title":"Operators"},{"location":"reference/context-and-expression-syntax/#functions","text":"The OpenTestFactory orchestrator offers a set of built-in functions that you can use in expressions. Some functions cast values to a string to perform comparisons. The orchestrator casts data types to a string using these conversions: Type Result Null '' Boolean 'true' or 'false' Number Decimal format, exponential for large numbers Array Arrays are not converted to a string Object Objects are not converted to a string","title":"Functions"},{"location":"reference/context-and-expression-syntax/#contains","text":"contains ( search , item ) Returns true if search contains item . If search is an array, this function returns true if the item is an element in the array. If search is a string, this function returns true if the item is a substring of search . This function is not case sensitive. Casts values to a string.","title":"contains"},{"location":"reference/context-and-expression-syntax/#example-using-an-array","text":"contains ( github . event . issue . labels . * . name , 'bug' )","title":"Example using an array"},{"location":"reference/context-and-expression-syntax/#example-using-a-string","text":"contains ( 'Hello world' , 'llo' ) returns true","title":"Example using a string"},{"location":"reference/context-and-expression-syntax/#startswith","text":"startsWith ( searchString , searchValue ) Returns true when searchString starts with searchValue . This function is not case sensitive. Casts values to a string.","title":"startsWith"},{"location":"reference/context-and-expression-syntax/#example_1","text":"startsWith ( 'Hello world' , 'He' ) returns true","title":"Example"},{"location":"reference/context-and-expression-syntax/#endswith","text":"endsWith ( searchString , searchValue ) Returns true if searchString ends with searchValue . This function is not case sensitive. Casts values to a string.","title":"endsWith"},{"location":"reference/context-and-expression-syntax/#example_2","text":"endsWith ( 'Hello world' , 'ld' ) returns true","title":"Example"},{"location":"reference/context-and-expression-syntax/#format","text":"format ( string , replaceValue0 , replaceValue1 , ..., replaceValueN ) Replaces values in the string, with the variable replaceValueN . Variables in the string are specified using the {N} syntax, where N is an integer. You must specify at least one replaceValue and string . There is no maximum for the number of variables ( replaceValueN ) you can use. Escape curly braces using double braces.","title":"format"},{"location":"reference/context-and-expression-syntax/#example_3","text":"Returns \u2018Hello Mona the Octocat\u2019 format ( 'Hello {0} {1} {2}' , 'Mona' , 'the' , 'Octocat' )","title":"Example"},{"location":"reference/context-and-expression-syntax/#example-escaping-braces","text":"Returns \u2018{Hello Mona the Octocat!}\u2019 format ( '{{Hello {0} {1} {2}!}}' , 'Mona' , 'the' , 'Octocat' )","title":"Example escaping braces"},{"location":"reference/context-and-expression-syntax/#join","text":"join ( array , optionalSeparator ) The value for array can be an array or a string. All values in array are concatenated into a string. If you provide optionalSeparator , it is inserted between the concatenated values. Otherwise, the default separator , is used. Casts values to a string.","title":"join"},{"location":"reference/context-and-expression-syntax/#example_4","text":"join ( github . event . issue . labels . * . name , ', ' ) may return 'bug, help wanted'","title":"Example"},{"location":"reference/context-and-expression-syntax/#tojson","text":"toJSON ( value ) Returns a pretty-print JSON representation of value . You can use this function to debug the information provided in contexts.","title":"toJson"},{"location":"reference/context-and-expression-syntax/#example_5","text":"toJSON ( job ) might return { \"status\" : \"Success\" }","title":"Example"},{"location":"reference/context-and-expression-syntax/#fromjson","text":"fromJSON ( value ) Returns a JSON object for value . You can use this function to provide a JSON object as an evaluated expression.","title":"fromJson"},{"location":"reference/context-and-expression-syntax/#example_6","text":"This workflow sets a JSON matrix in one job, and passes it to the next job using an output and fromJSON . metadata : name : build jobs : job1 : runs-on : ubuntu-latest outputs : matrix : ${{ steps.set-matrix.outputs.matrix }} steps : - id : set-matrix run : echo \"::set-output name=matrix::{\\\"include\\\":[{\\\"project\\\":\\\"foo\\\",\\\"config\\\":\\\"Debug\\\"},{\\\"project\\\":\\\"bar\\\",\\\"config\\\":\\\"Release\\\"}]}\" job2 : needs : job1 runs-on : ubuntu-latest strategy : matrix : ${{fromJson(needs.job1.outputs.matrix)}} steps : - run : build","title":"Example"},{"location":"reference/context-and-expression-syntax/#job-status-check-functions","text":"You can use the following status check functions as expressions in if conditionals. If your if expression does not contain any of the status functions it will automatically result with success() . For more information about if conditionals, see \u201c Workflow syntax for OpenTestFactory orchestrator .\u201d","title":"Job status check functions"},{"location":"reference/context-and-expression-syntax/#success","text":"Returns true when none of the previous steps have failed or been canceled.","title":"success"},{"location":"reference/context-and-expression-syntax/#example_7","text":"steps : ... - name : The job has succeeded if : ${{ success() }}","title":"Example"},{"location":"reference/context-and-expression-syntax/#always","text":"Always returns true , even when canceled. A job or step will not run when a critical failure prevents the task from running. For example, if getting sources failed.","title":"always"},{"location":"reference/context-and-expression-syntax/#example_8","text":"if : ${{ always() }}","title":"Example"},{"location":"reference/context-and-expression-syntax/#cancelled","text":"Returns true if the workflow was canceled.","title":"cancelled"},{"location":"reference/context-and-expression-syntax/#example_9","text":"if : ${{ cancelled() }}","title":"Example"},{"location":"reference/context-and-expression-syntax/#failure","text":"Returns true when any previous step of a job fails.","title":"failure"},{"location":"reference/context-and-expression-syntax/#example_10","text":"steps : ... - name : The job has failed if : ${{ failure() }}","title":"Example"},{"location":"reference/overview/","text":"Overview \u00b6","title":"Overview"},{"location":"reference/overview/#overview","text":"","title":"Overview"},{"location":"reference/using-environment-variables/","text":"Using environment variables \u00b6 Squash Orchestrator sets default environment variables for each Squash Orchestrator workflow run. You can also set custom environment variables in your workflow file. About environment variables \u00b6 Squash Orchestrator sets default environment variables that are available to every step in a workflow run. Environment variables are case-sensitive. Commands run in actions or steps can create, read, and modify environment variables. To set custom environment variables, you need to specify the variables in the workflow file. You can define environment variables for a step, job, or entire workflow using the jobs.<job_id>.steps.variables , jobs.<job_id>.variables , and variables keywords. For more information, see \u201c Workflow syntax for Squash Orchestrator .\u201d steps : - name : Hello world run : echo Hello world $FIRST_NAME $middle_name $Last_Name! variables : FIRST_NAME : Mona middle_name : The Last_Name : Octocat You can also use the set-env workflow command to set an environment variable that the following steps in a workflow can use. The set-env command can be used directly by an action or as a shell command in a workflow file using the run keyword. For more information, see \u201cWorkflow commands for Squash Orchestrator Workflows.\u201d Default environment variables \u00b6 We strongly recommend that actions use environment variables to access the filesystem rather than using hardcoded file paths. Squash Orchestrator sets environment variables for actions to use in all execution environments. Environment variable Description CI Always set to true if started from a CI pipeline. HOME The path to the GitHub home directory used to store user data. For example, /squashtf/home . SQUASHTF_ACTOR The name of the person or app that initiated the workflow. For example, octocat. SQUASHTF_WORKSPACE The Squash Orchestrator workspace directory path. The workspace directory contains a subdirectory with a copy of your repository if your workflow uses the actions/checkout action. If you don\u2019t use the actions/checkout action, the directory will be empty. For example, /home/runner/work/my-repo-name/my-repo-name . SQUASHTF_SERVER_URL Returns the URL of the Squash Orchestrator server. For example: https://squashtf.example.com . Naming conventions for environment variables \u00b6 Note : Squash Orchestrator reserves the SQUASHTF_ environment variable prefix for internal use by Squash Orchestrator. Setting an environment variable or secret with the SQUASHTF_ prefix will result in an error. Any new environment variables you set that point to a location on the filesystem should have a _PATH suffix. The HOME and SQUASHTF_WORKSPACE default variables are exceptions to this convention because the words \u201chome\u201d and \u201cworkspace\u201d already imply a location.","title":"Environment variables"},{"location":"reference/using-environment-variables/#using-environment-variables","text":"Squash Orchestrator sets default environment variables for each Squash Orchestrator workflow run. You can also set custom environment variables in your workflow file.","title":"Using environment variables"},{"location":"reference/using-environment-variables/#about-environment-variables","text":"Squash Orchestrator sets default environment variables that are available to every step in a workflow run. Environment variables are case-sensitive. Commands run in actions or steps can create, read, and modify environment variables. To set custom environment variables, you need to specify the variables in the workflow file. You can define environment variables for a step, job, or entire workflow using the jobs.<job_id>.steps.variables , jobs.<job_id>.variables , and variables keywords. For more information, see \u201c Workflow syntax for Squash Orchestrator .\u201d steps : - name : Hello world run : echo Hello world $FIRST_NAME $middle_name $Last_Name! variables : FIRST_NAME : Mona middle_name : The Last_Name : Octocat You can also use the set-env workflow command to set an environment variable that the following steps in a workflow can use. The set-env command can be used directly by an action or as a shell command in a workflow file using the run keyword. For more information, see \u201cWorkflow commands for Squash Orchestrator Workflows.\u201d","title":"About environment variables"},{"location":"reference/using-environment-variables/#default-environment-variables","text":"We strongly recommend that actions use environment variables to access the filesystem rather than using hardcoded file paths. Squash Orchestrator sets environment variables for actions to use in all execution environments. Environment variable Description CI Always set to true if started from a CI pipeline. HOME The path to the GitHub home directory used to store user data. For example, /squashtf/home . SQUASHTF_ACTOR The name of the person or app that initiated the workflow. For example, octocat. SQUASHTF_WORKSPACE The Squash Orchestrator workspace directory path. The workspace directory contains a subdirectory with a copy of your repository if your workflow uses the actions/checkout action. If you don\u2019t use the actions/checkout action, the directory will be empty. For example, /home/runner/work/my-repo-name/my-repo-name . SQUASHTF_SERVER_URL Returns the URL of the Squash Orchestrator server. For example: https://squashtf.example.com .","title":"Default environment variables"},{"location":"reference/using-environment-variables/#naming-conventions-for-environment-variables","text":"Note : Squash Orchestrator reserves the SQUASHTF_ environment variable prefix for internal use by Squash Orchestrator. Setting an environment variable or secret with the SQUASHTF_ prefix will result in an error. Any new environment variables you set that point to a location on the filesystem should have a _PATH suffix. The HOME and SQUASHTF_WORKSPACE default variables are exceptions to this convention because the words \u201chome\u201d and \u201cworkspace\u201d already imply a location.","title":"Naming conventions for environment variables"},{"location":"reference/workflow-commands/","text":"Workflow commands for Squash Orchestrator Plugins \u00b6 You can use workflow commands when running shell commands in a workflow or in a plugin\u2019s code. About workflow commands \u00b6 Plugins can communicate with the runner machine to set environment variables, output values used by other plugins, add debug messages to the output logs, and other tasks. Most workflow commands use the echo command in a specific format, while others are invoked by writing to a file. For more information, see \u201cEnvironment Files.\u201d echo \"::workflow-command parameter1={data},parameter2={data}::{command value}\" Note : Workflow command and parameter names are not case-sensitive. Warning : If you are using Command Prompt, omit double quote characters ( \" ) when using workflow commands. You can use the set-output command in your workflow to set the same value: - name : Set selected color run : echo '::set-output name=SELECTED_COLOR::green' id : random-color-generator - name : Get color run : echo 'The selected color is' ${{ steps.random-color-generator.outputs.SELECTED_COLOR }} Attach an artifact \u00b6 ::attach::{path} Sets an action\u2019s attachment. Example \u00b6 echo \"::attach::foobar.yaml Setting an output parameter \u00b6 ::set-output name={name}::{value} Sets an action\u2019s output parameter. Optionally, you can also declare output parameters in an plugin\u2019s metadata file. For more information, see \u201c Metadata syntax for Squash Orchestrator Plugins .\u201d Example \u00b6 echo \"::set-output name=action_fruit::strawberry\" Setting a debug message \u00b6 ::debug::{message} Prints a debug message to the log. You must create a secret named ACTIONS_STEP_DEBUG with the value true to see the debug messages set by this command in the log. For more information, see \u201cManaging a workflow run.\u201d Example \u00b6 echo \"::debug::Set the Octocat variable\" Setting a warning message \u00b6 ::warning file={name},line={line},col={col}::{message} Creates a warning message and prints the message to the log. You can optionally provide a filename ( file ), line number ( line ), and column ( col ) number where the warning occurred. Example \u00b6 echo \"::warning file=app.js,line=1,col=5::Missing semicolon\" Setting an error message \u00b6 ::error file={name},line={line},col={col}::{message} Creates an error message and prints the message to the log. You can optionally provide a filename ( file ), line number ( line ), and column ( col ) number where the warning occurred. Example \u00b6 echo \"::error file=app.js,line=10,col=15::Something went wrong\" Masking a value in log \u00b6 ::add-mask::{value} Masking a value prevents a string or variable from being printed in the log. Each masked word separated by whitespace is replaced with the * character. You can use an environment variable or string for the mask\u2019s value . Example masking a string \u00b6 When you print \"Mona The Octocat\" in the log, you\u2019ll see \"***\" . echo \"::add-mask::Mona The Octocat\" Example masking an environment variable \u00b6 When you print the variable MY_NAME or the value \"Mona The Octocat\" in the log, you\u2019ll see \"***\" instead of \"Mona The Octocat\" . MY_NAME = \"Mona The Octocat\" echo \"::add-mask:: $MY_NAME \" Stopping and starting workflow commands \u00b6 ::stop-commands::{endtoken} Stops processing any workflow commands. This special command allows you to log anything without accidentally running a workflow command. For example, you could stop logging to output an entire script that has comments. Example stopping workflow commands \u00b6 echo \"::stop-commands::pause-logging\" To start workflow commands, pass the token that you used to stop workflow commands. ::{endtoken}:: Example starting workflow commands \u00b6 echo \"::pause-logging::\"","title":"Workflow commands"},{"location":"reference/workflow-commands/#workflow-commands-for-squash-orchestrator-plugins","text":"You can use workflow commands when running shell commands in a workflow or in a plugin\u2019s code.","title":"Workflow commands for Squash Orchestrator Plugins"},{"location":"reference/workflow-commands/#about-workflow-commands","text":"Plugins can communicate with the runner machine to set environment variables, output values used by other plugins, add debug messages to the output logs, and other tasks. Most workflow commands use the echo command in a specific format, while others are invoked by writing to a file. For more information, see \u201cEnvironment Files.\u201d echo \"::workflow-command parameter1={data},parameter2={data}::{command value}\" Note : Workflow command and parameter names are not case-sensitive. Warning : If you are using Command Prompt, omit double quote characters ( \" ) when using workflow commands. You can use the set-output command in your workflow to set the same value: - name : Set selected color run : echo '::set-output name=SELECTED_COLOR::green' id : random-color-generator - name : Get color run : echo 'The selected color is' ${{ steps.random-color-generator.outputs.SELECTED_COLOR }}","title":"About workflow commands"},{"location":"reference/workflow-commands/#attach-an-artifact","text":"::attach::{path} Sets an action\u2019s attachment.","title":"Attach an artifact"},{"location":"reference/workflow-commands/#example","text":"echo \"::attach::foobar.yaml","title":"Example"},{"location":"reference/workflow-commands/#setting-an-output-parameter","text":"::set-output name={name}::{value} Sets an action\u2019s output parameter. Optionally, you can also declare output parameters in an plugin\u2019s metadata file. For more information, see \u201c Metadata syntax for Squash Orchestrator Plugins .\u201d","title":"Setting an output parameter"},{"location":"reference/workflow-commands/#example_1","text":"echo \"::set-output name=action_fruit::strawberry\"","title":"Example"},{"location":"reference/workflow-commands/#setting-a-debug-message","text":"::debug::{message} Prints a debug message to the log. You must create a secret named ACTIONS_STEP_DEBUG with the value true to see the debug messages set by this command in the log. For more information, see \u201cManaging a workflow run.\u201d","title":"Setting a debug message"},{"location":"reference/workflow-commands/#example_2","text":"echo \"::debug::Set the Octocat variable\"","title":"Example"},{"location":"reference/workflow-commands/#setting-a-warning-message","text":"::warning file={name},line={line},col={col}::{message} Creates a warning message and prints the message to the log. You can optionally provide a filename ( file ), line number ( line ), and column ( col ) number where the warning occurred.","title":"Setting a warning message"},{"location":"reference/workflow-commands/#example_3","text":"echo \"::warning file=app.js,line=1,col=5::Missing semicolon\"","title":"Example"},{"location":"reference/workflow-commands/#setting-an-error-message","text":"::error file={name},line={line},col={col}::{message} Creates an error message and prints the message to the log. You can optionally provide a filename ( file ), line number ( line ), and column ( col ) number where the warning occurred.","title":"Setting an error message"},{"location":"reference/workflow-commands/#example_4","text":"echo \"::error file=app.js,line=10,col=15::Something went wrong\"","title":"Example"},{"location":"reference/workflow-commands/#masking-a-value-in-log","text":"::add-mask::{value} Masking a value prevents a string or variable from being printed in the log. Each masked word separated by whitespace is replaced with the * character. You can use an environment variable or string for the mask\u2019s value .","title":"Masking a value in log"},{"location":"reference/workflow-commands/#example-masking-a-string","text":"When you print \"Mona The Octocat\" in the log, you\u2019ll see \"***\" . echo \"::add-mask::Mona The Octocat\"","title":"Example masking a string"},{"location":"reference/workflow-commands/#example-masking-an-environment-variable","text":"When you print the variable MY_NAME or the value \"Mona The Octocat\" in the log, you\u2019ll see \"***\" instead of \"Mona The Octocat\" . MY_NAME = \"Mona The Octocat\" echo \"::add-mask:: $MY_NAME \"","title":"Example masking an environment variable"},{"location":"reference/workflow-commands/#stopping-and-starting-workflow-commands","text":"::stop-commands::{endtoken} Stops processing any workflow commands. This special command allows you to log anything without accidentally running a workflow command. For example, you could stop logging to output an entire script that has comments.","title":"Stopping and starting workflow commands"},{"location":"reference/workflow-commands/#example-stopping-workflow-commands","text":"echo \"::stop-commands::pause-logging\" To start workflow commands, pass the token that you used to stop workflow commands. ::{endtoken}::","title":"Example stopping workflow commands"},{"location":"reference/workflow-commands/#example-starting-workflow-commands","text":"echo \"::pause-logging::\"","title":"Example starting workflow commands"},{"location":"reference/workflows/","text":"Workflow syntax for OpenTestFactory orchestrator \u00b6 Note : This workflow syntax closely matches GitHub Actions workflow syntax. If you are new to GitHub actions and want to learn more, see \u201c Workflow syntax for GitHub Actions .\u201d A workflow is a configurable automated process made up of one or more jobs. You must create a YAML file to define your workflow configuration. About YAML syntax for workflows \u00b6 Workflow files use YAML syntax, and must have either a .yml or .yaml file extension. If you\u2019re new to YAML and want to learn more, see \u201c Learn YAML in five minutes .\u201d You may store workflow files anywhere in your repository. Usage limits \u00b6 There are some limits on OpenTestFactory orchestrator workflows. These limits are subject to change. Jobs - There can be a maximum of 1024 jobs per workflow (including generated jobs). Steps - There can be a maximum of 256 steps per job (including generated steps). Job matrix - A job matrix can generate a maximum of 256 jobs per workflow run. metadata.name \u00b6 The name of your workflow. The orchestrator displays the names of your workflows on your repository\u2019s page. variables \u00b6 A map of environment variables that are available to all jobs and steps in the workflow. You can also set environment variables that are only available to a job or step. For more information, see jobs.<job_id>.variables and jobs.<job_id>.steps.variables . When more than one environment variable is defined with the same name, the orchestrator uses the most specific environment variable. For example, an environment variable defined in a step will override job and workflow variables with the same name, while the step executes. A variable defined for a job will override a workflow variable with the same name, while the job executes. Example \u00b6 variables : SERVER : production resources \u00b6 A map of resources that are available to all jobs and steps in the workflow. You can define three kinds of resources: testmanagers , repositories , and files . testmanagers and repositories resources are a way to reuse resources in your workflow. files allows to provide local or external files to your workflow. Example of a repository resource \u00b6 This example defines a myrepo repository: resources : repositories : - name : myrepo type : bitbucket repository : example/my-example-repo endpoint : https://bitbucket.org Example of a file resource \u00b6 This example defines one file. This file will have to be joined when launching the workflow: resources : files : - dataset This action will put a copy of the provided file in the current execution environment, so that the following steps can use it: - uses : actions/put-file@v1 with : data : ${{ resources.files.dataset }} path : dataset.xml defaults \u00b6 A map of default settings that will apply to all jobs in the workflow. You can also set default settings that are only available to a job. For more information, see jobs.<job_id>.defaults . When more than one default setting is defined with the same name, the orchestrator uses the most specific default setting. For example, a default setting defined in a job will override a default setting that has the same name defined in a workflow. defaults.run \u00b6 You can provide default shell and working-directory options for all run steps in a workflow. You can also set default settings for run that are only available to a job. For more information, see jobs.<job_id>.defaults.run . You cannot use contexts or expressions in this keyword. When more than one default setting is defined with the same name, the orchestrator uses the most specific default setting. For example, a default setting defined in a job will override a default setting that has the same name defined in a workflow. Example \u00b6 defaults : run : shell : bash working-directory : scripts jobs \u00b6 A workflow run is made up of one or more jobs. Jobs run in parallel by default. To run jobs sequentially, you can define dependencies on other jobs using the jobs.<job_id>.needs keyword. Each job runs in an execution environment specified by runs-on . You can run an unlimited number of jobs as long as you are within the workflow usage limits. For more information, see \u201c Usage limits .\u201d jobs.<job_id> \u00b6 Each job must have an id to associate with the job. The key job_id is a string and its value is a map of the job\u2019s configuration data. You must replace <job_id> with a string that is unique to the jobs object. The <job_id> must start with a letter or _ and contain only alphanumeric characters, - , or _ . Example \u00b6 jobs : my_first_job : name : My first job my_second_job : name : My second job jobs.<job_id>.name \u00b6 The name of the job displayed by the orchestrator. jobs.<job_id>.needs \u00b6 Identifies any jobs that must complete successfully before this job will run. It can be a string or array of strings. If a job fails, all jobs that need it are skipped unless the jobs use a conditional statement that causes the job to continue. Example \u00b6 jobs : job1 : job2 : needs : job1 job3 : needs : [ job1 , job2 ] In this example, job1 must complete successfully before job2 begins, and job3 waits for both job1 and job2 to complete. The jobs in this example run sequentially: job1 job2 job3 jobs.<job_id>.runs-on \u00b6 Required The environment to run the job on. Examples \u00b6 runs-on : robotframework All execution environments have at least one label, and you can select an execution environment by providing only one label. Alternatively, you can use an array with additional labels, such as labels for a specific execution environment, to select only the execution environment types you specify. runs-on : [ linux , robotframework ] For more information, see \u201cExecution environments.\u201d jobs.<jobs_id>.outputs \u00b6 A map of outputs for a job. Job outputs are available to all downstream jobs that depend on this job. For more information on defining job dependencies, see jobs.<job_id>.needs . Job outputs are strings, and job outputs containing expressions are evaluated at the end of each job. To use job outputs in a dependent job, you can use the needs context. For more information, see \u201c Context and expression syntax for OpenTestFactory workflows .\u201d Example \u00b6 jobs : job1 : runs-on : ubuntu-latest # Map a step output to a job output outputs : output1 : ${{ steps.step1.outputs.test }} output2 : ${{ steps.step2.outputs.test }} steps : - id : step1 run : echo \"::set-output name=test::hello\" - id : step2 run : echo \"::set-output name=test::world\" job2 : runs-on : ubuntu-latest needs : job1 steps : - run : echo ${{needs.job1.outputs.output1}} ${{needs.job1.outputs.output2}} jobs.<job_id>.variables \u00b6 A map of environment variables that are available to all steps in the job. You can also set environment variables for the entire workflow or an individual step. For more information, see variables and jobs.<job_id>.steps.variables . When more than one environment variable is defined with the same name, the orchestrator uses the most specific environment variable. For example, an environment variable defined in a step will override job and workflow variables with the same name, while the step executes. A variable defined for a job will override a workflow variable with the same name, while the job executes. Example \u00b6 jobs : job1 : variables : FIRST_NAME : Mona jobs.<job_id>.defaults \u00b6 A map of default settings that will apply to all steps in the job. You can also set default settings for the entire workflow. For more information, see defaults . When more than one default setting is defined with the same name, the orchestrator uses the most specific default setting. For example, a default setting defined in a job will override a default setting that has the same name defined in a workflow. jobs.<job_id>.defaults.run \u00b6 Provide default shell and working-directory to all run steps in the job. Context and expression are not allowed in this section. You can provide default shell and working-directory options for all run steps in a job. You can also set default settings for run for the entire workflow. For more information, see jobs.defaults.run . You cannot use contexts or expressions in this keyword. When more than one default setting is defined with the same name, the orchestrator uses the most specific default setting. For example, a default setting defined in a job will override a default setting that has the same name defined in a workflow. Example \u00b6 jobs : job1 : runs-on : ubuntu-latest defaults : run : shell : bash working-directory : scripts jobs.<job_id>.if \u00b6 You can use the if conditional to prevent a job from running unless a condition is met. You can use any supported context and expression to create a conditional. When you use expressions in an if conditional, you may omit the expression syntax ( ${{ }} ) because the orchestrator automatically evaluates the if conditional as an expression. For more information, see \u201c Context and expression syntax for OpenTestFactory workflows .\u201d jobs.<job_id>.generator \u00b6 A generator creates a series of jobs that replace the current job. Some generators require inputs that you must set using the with keyword. Review the generator\u2019s README file to determine the inputs required. jobs.<job_id>.with \u00b6 A map of the input parameters defined by the generator plugin. Each input parameter is a key/value pair. Example \u00b6 Defines the three input parameters ( first_name , middle_name , and last_name ) defined by the hello_world generator. jobs : generator : yada@hello_world@v1 with : first_name : Mona middle_name : The last_name : Octocat jobs.<job_id>.steps \u00b6 A job contains a sequence of tasks called steps . Steps can run commands, run setup tasks, or run an action. Not all steps run actions, but all actions run as a step. Each step runs in its own process in the execution environment and has access to the workspace and filesystem. Because steps run in their own process, changes to environment variables are not preserved between steps. The orchestrator provides built-in steps to set up and complete a job. You can run an unlimited number of steps as long as you are within the workflow usage limits. For more information, see \u201c Usage limits .\u201d Example \u00b6 metadata : name : Greeting from Mona jobs : my-job : name : My Job runs-on : ubuntu-latest steps : - name : Print a greeting variables : MY_VAR : Hi there! My name is FIRST_NAME : Mona MIDDLE_NAME : The LAST_NAME : Octocat run : | echo $MY_VAR $FIRST_NAME $MIDDLE_NAME $LAST_NAME. jobs.<job_id>.steps.id \u00b6 A unique identifier for the step. You can use the id to reference the step in contexts. For more information, see \u201c Context and expression syntax for OpenTestFactory workflows .\u201d jobs.<job_id>.steps.if \u00b6 You can use the if conditional to prevent a step from running unless a condition is met. You can use any supported context and expression to create a conditional. When you use expressions in an if conditional, you may omit the expression syntax ( ${{ }} ) because the orchestrator automatically evaluates the if conditional as an expression. For more information, see \u201c Context and expression syntax for OpenTestFactory workflows .\u201d Example using contexts \u00b6 This step only runs when the event type is a pull_request and the event action is unassigned. steps : - name : My first step if : ${{ github.event_name == 'pull_request' && github.event.action == 'unassigned' }} run : echo This event is a pull request that had an assignee removed. Example using status check functions \u00b6 The my backup step only runs when the previous step of a job fails. For more information, see \u201c Context and expression syntax for OpenTestFactory workflows .\u201d steps : - name : My first step uses : monacorp/action-name@v1 - name : My backup step if : ${{ failure() }} uses : actions/heroku@v2 jobs.<job_id>.steps.name \u00b6 A name for your step to display on the orchestrator. jobs.<job_id>.steps.uses \u00b6 Selects a provider to run as part of a step in your job. A provider is a reusable unit of code. We strongly recommend that you include the version of the provider you are using. If you do not specify a version, it could break your workflows or cause unexpected behavior when the provider owner publishes an update. Using the specific major provider version allows you to receive critical fixes and security patches while still maintaining compatibility. It also assures that your workflow should still work. Using the master branch of a provider may be convenient, but if someone releases a new major version with a breaking change, your workflow could break. Some providers require inputs that you must set using the with keyword. Review the provider\u2019s README file to determine the inputs required. Providers are Python files. For more details, see runs-on . Example using versioned providers \u00b6 steps : # Reference the major version of a release - uses : actions/setup-node@v1 # Reference a minor version of a release - uses : actions/setup-node@v1.2 jobs.<job_id>.steps.run \u00b6 Runs command-line programs using the execution environment\u2019s shell. If you do not provide a name , the step name will default to the text specified in the run command. Commands run using non-login shells by default. You can choose a different shell and customize the shell used to run commands. For more information, see \u201cUsing a specific shell.\u201d Each run keyword represents a new process and shell in the execution environment. When you provide multi-line commands, each line runs in the same shell. For example: A single-line command: - name : Install Dependencies run : npm install A multi-line command: - name : Clean install dependencies and build run : | npm ci npm run build Using the working-directory keyword, you can specify the working directory of where to run the command. - name : Clean temp directory run : rm -rf * working-directory : ./temp Using a specific shell \u00b6 You can override the default shell settings in the execution environment using the shell keyword. You can use built-in shell keywords, or you can define a custom set of shell options. Supported platform shell parameter Description Command run internally All bash The default shell on non-Windows platforms with a fallback to sh . When specifying a bash shell on Windows, the bash shell included with Git for Windows is used. bash \u2013noprofile \u2013norc -eo pipefail All python Executes the python command. python Windows cmd The orchestrator appends the extension .cmd to your script name and substitutes for {0} . %ComSpec% /D /E:ON /V:OFF /S /C \u201cCALL \u201c{0}\u201d\u201c. Example running a script using bash \u00b6 steps : - name : Display the path run : echo $PATH shell : bash Example running a script using Windows cmd \u00b6 steps : - name : Display the path run : echo %PATH% shell : cmd Example running a python script \u00b6 steps : - name : Display the path run : | import os print(os.environ['PATH']) shell : python Custom shell \u00b6 You can set the shell value to a template string using command [options...] {0} [more_options...] The orchestrator interprets the first whitespace-delimited word of the string as the command and inserts the file name for the temporary script at {0} . Exit codes and error action preference \u00b6 For built-in shell keywords, we provide the following defaults that are executed by the execution environments. You should use these guidelines when running shell scripts. bash / sh : Fail-fast behavior using set -e o pipefail : Default for bash and built-in shell. It is also the default when you do not provide an option on non-Windows platforms. You can opt out of fail-fast and take full control by providing a template string to the shell options. For example, bash {0} . sh-like shells exit with the exit code of the last command executed in a script, which is also the default behavior for actions. The execution environment will report the status of the step as fail/succeed based on this exit code. cmd There does not seem to be a way to fully opt into fail-fast behavior other than writing your script to check each error code and respond accordingly. Because we can\u2019t actually provide that behavior by default, you need to write this behavior into your script. cmd.exe will exit with the error level of the last program it executed, and it will return the error code to the execution environment. jobs.<job_id>.steps.with \u00b6 A map of the input parameters defined by the action. Each input parameter is a key/value pair. Input parameters are set as environment variables. The variable is prefixed with INPUT_ and converted to upper case. Example \u00b6 Defines the three input parameters ( first_name , middle_name , and last_name ) defined by the hello_world action. These input variables will be accessible to the hello-world action as INPUT_FIRST_NAME , INPUT_MIDDLE_NAME , and INPUT_LAST_NAME environment variables. jobs : my_first_job : steps : - name : My first step uses : actions/hello_world@master with : first_name : Mona middle_name : The last_name : Octocat jobs.<job_id>.steps.variables \u00b6 Sets environment variables for steps to use in the execution environment. You can also set environment variables for the entire workflow or a job. For more information, see variables and jobs.<job_id>.variables . When more than one environment variable is defined with the same name, the orchestrator uses the most specific environment variable. For example, an environment variable defined in a step will override job and workflow variables with the same name, while the step executes. A variable defined for a job will override a workflow variable with the same name, while the job executes. Public actions may specify expected environment variables in the README file. If you are setting a secret in an environment variable, you must set secrets using the secrets context. For more information, see \u201c Using environment variables \u201d and \u201c Context and expression syntax for OpenTestFactory workflows .\u201d Example \u00b6 steps : - name : My first action variables : GITHUB_TOKEN : ${{ secrets.GITHUB_TOKEN }} FIRST_NAME : Mona LAST_NAME : Octocat jobs.<job_id>.steps.continue-on-error \u00b6 Prevents a job from failing when a step fails. Set to true to allow a job to pass when this step fails. jobs.<job_id>.steps.timeout-minutes \u00b6 The maximum number of minutes to run the step before killing the process. jobs.<job_id>.timeout-minutes \u00b6 The maximum number of minutes to let a job run before the orchestrator automatically cancels it. Default: 360 jobs.<job_id>.continue-on-error \u00b6 Prevents a workflow run from failing when a job fails. Set to true to allow a workflow run to pass when this job fails.","title":"Workflow syntax"},{"location":"reference/workflows/#workflow-syntax-for-opentestfactory-orchestrator","text":"Note : This workflow syntax closely matches GitHub Actions workflow syntax. If you are new to GitHub actions and want to learn more, see \u201c Workflow syntax for GitHub Actions .\u201d A workflow is a configurable automated process made up of one or more jobs. You must create a YAML file to define your workflow configuration.","title":"Workflow syntax for OpenTestFactory orchestrator"},{"location":"reference/workflows/#about-yaml-syntax-for-workflows","text":"Workflow files use YAML syntax, and must have either a .yml or .yaml file extension. If you\u2019re new to YAML and want to learn more, see \u201c Learn YAML in five minutes .\u201d You may store workflow files anywhere in your repository.","title":"About YAML syntax for workflows"},{"location":"reference/workflows/#usage-limits","text":"There are some limits on OpenTestFactory orchestrator workflows. These limits are subject to change. Jobs - There can be a maximum of 1024 jobs per workflow (including generated jobs). Steps - There can be a maximum of 256 steps per job (including generated steps). Job matrix - A job matrix can generate a maximum of 256 jobs per workflow run.","title":"Usage limits"},{"location":"reference/workflows/#metadataname","text":"The name of your workflow. The orchestrator displays the names of your workflows on your repository\u2019s page.","title":"metadata.name"},{"location":"reference/workflows/#variables","text":"A map of environment variables that are available to all jobs and steps in the workflow. You can also set environment variables that are only available to a job or step. For more information, see jobs.<job_id>.variables and jobs.<job_id>.steps.variables . When more than one environment variable is defined with the same name, the orchestrator uses the most specific environment variable. For example, an environment variable defined in a step will override job and workflow variables with the same name, while the step executes. A variable defined for a job will override a workflow variable with the same name, while the job executes.","title":"variables"},{"location":"reference/workflows/#example","text":"variables : SERVER : production","title":"Example"},{"location":"reference/workflows/#resources","text":"A map of resources that are available to all jobs and steps in the workflow. You can define three kinds of resources: testmanagers , repositories , and files . testmanagers and repositories resources are a way to reuse resources in your workflow. files allows to provide local or external files to your workflow.","title":"resources"},{"location":"reference/workflows/#example-of-a-repository-resource","text":"This example defines a myrepo repository: resources : repositories : - name : myrepo type : bitbucket repository : example/my-example-repo endpoint : https://bitbucket.org","title":"Example of a repository resource"},{"location":"reference/workflows/#example-of-a-file-resource","text":"This example defines one file. This file will have to be joined when launching the workflow: resources : files : - dataset This action will put a copy of the provided file in the current execution environment, so that the following steps can use it: - uses : actions/put-file@v1 with : data : ${{ resources.files.dataset }} path : dataset.xml","title":"Example of a file resource"},{"location":"reference/workflows/#defaults","text":"A map of default settings that will apply to all jobs in the workflow. You can also set default settings that are only available to a job. For more information, see jobs.<job_id>.defaults . When more than one default setting is defined with the same name, the orchestrator uses the most specific default setting. For example, a default setting defined in a job will override a default setting that has the same name defined in a workflow.","title":"defaults"},{"location":"reference/workflows/#defaultsrun","text":"You can provide default shell and working-directory options for all run steps in a workflow. You can also set default settings for run that are only available to a job. For more information, see jobs.<job_id>.defaults.run . You cannot use contexts or expressions in this keyword. When more than one default setting is defined with the same name, the orchestrator uses the most specific default setting. For example, a default setting defined in a job will override a default setting that has the same name defined in a workflow.","title":"defaults.run"},{"location":"reference/workflows/#example_1","text":"defaults : run : shell : bash working-directory : scripts","title":"Example"},{"location":"reference/workflows/#jobs","text":"A workflow run is made up of one or more jobs. Jobs run in parallel by default. To run jobs sequentially, you can define dependencies on other jobs using the jobs.<job_id>.needs keyword. Each job runs in an execution environment specified by runs-on . You can run an unlimited number of jobs as long as you are within the workflow usage limits. For more information, see \u201c Usage limits .\u201d","title":"jobs"},{"location":"reference/workflows/#jobsjob_id","text":"Each job must have an id to associate with the job. The key job_id is a string and its value is a map of the job\u2019s configuration data. You must replace <job_id> with a string that is unique to the jobs object. The <job_id> must start with a letter or _ and contain only alphanumeric characters, - , or _ .","title":"jobs.&lt;job_id&gt;"},{"location":"reference/workflows/#example_2","text":"jobs : my_first_job : name : My first job my_second_job : name : My second job","title":"Example"},{"location":"reference/workflows/#jobsjob_idname","text":"The name of the job displayed by the orchestrator.","title":"jobs.&lt;job_id&gt;.name"},{"location":"reference/workflows/#jobsjob_idneeds","text":"Identifies any jobs that must complete successfully before this job will run. It can be a string or array of strings. If a job fails, all jobs that need it are skipped unless the jobs use a conditional statement that causes the job to continue.","title":"jobs.&lt;job_id&gt;.needs"},{"location":"reference/workflows/#example_3","text":"jobs : job1 : job2 : needs : job1 job3 : needs : [ job1 , job2 ] In this example, job1 must complete successfully before job2 begins, and job3 waits for both job1 and job2 to complete. The jobs in this example run sequentially: job1 job2 job3","title":"Example"},{"location":"reference/workflows/#jobsjob_idruns-on","text":"Required The environment to run the job on.","title":"jobs.&lt;job_id&gt;.runs-on"},{"location":"reference/workflows/#examples","text":"runs-on : robotframework All execution environments have at least one label, and you can select an execution environment by providing only one label. Alternatively, you can use an array with additional labels, such as labels for a specific execution environment, to select only the execution environment types you specify. runs-on : [ linux , robotframework ] For more information, see \u201cExecution environments.\u201d","title":"Examples"},{"location":"reference/workflows/#jobsjobs_idoutputs","text":"A map of outputs for a job. Job outputs are available to all downstream jobs that depend on this job. For more information on defining job dependencies, see jobs.<job_id>.needs . Job outputs are strings, and job outputs containing expressions are evaluated at the end of each job. To use job outputs in a dependent job, you can use the needs context. For more information, see \u201c Context and expression syntax for OpenTestFactory workflows .\u201d","title":"jobs.&lt;jobs_id&gt;.outputs"},{"location":"reference/workflows/#example_4","text":"jobs : job1 : runs-on : ubuntu-latest # Map a step output to a job output outputs : output1 : ${{ steps.step1.outputs.test }} output2 : ${{ steps.step2.outputs.test }} steps : - id : step1 run : echo \"::set-output name=test::hello\" - id : step2 run : echo \"::set-output name=test::world\" job2 : runs-on : ubuntu-latest needs : job1 steps : - run : echo ${{needs.job1.outputs.output1}} ${{needs.job1.outputs.output2}}","title":"Example"},{"location":"reference/workflows/#jobsjob_idvariables","text":"A map of environment variables that are available to all steps in the job. You can also set environment variables for the entire workflow or an individual step. For more information, see variables and jobs.<job_id>.steps.variables . When more than one environment variable is defined with the same name, the orchestrator uses the most specific environment variable. For example, an environment variable defined in a step will override job and workflow variables with the same name, while the step executes. A variable defined for a job will override a workflow variable with the same name, while the job executes.","title":"jobs.&lt;job_id&gt;.variables"},{"location":"reference/workflows/#example_5","text":"jobs : job1 : variables : FIRST_NAME : Mona","title":"Example"},{"location":"reference/workflows/#jobsjob_iddefaults","text":"A map of default settings that will apply to all steps in the job. You can also set default settings for the entire workflow. For more information, see defaults . When more than one default setting is defined with the same name, the orchestrator uses the most specific default setting. For example, a default setting defined in a job will override a default setting that has the same name defined in a workflow.","title":"jobs.&lt;job_id&gt;.defaults"},{"location":"reference/workflows/#jobsjob_iddefaultsrun","text":"Provide default shell and working-directory to all run steps in the job. Context and expression are not allowed in this section. You can provide default shell and working-directory options for all run steps in a job. You can also set default settings for run for the entire workflow. For more information, see jobs.defaults.run . You cannot use contexts or expressions in this keyword. When more than one default setting is defined with the same name, the orchestrator uses the most specific default setting. For example, a default setting defined in a job will override a default setting that has the same name defined in a workflow.","title":"jobs.&lt;job_id&gt;.defaults.run"},{"location":"reference/workflows/#example_6","text":"jobs : job1 : runs-on : ubuntu-latest defaults : run : shell : bash working-directory : scripts","title":"Example"},{"location":"reference/workflows/#jobsjob_idif","text":"You can use the if conditional to prevent a job from running unless a condition is met. You can use any supported context and expression to create a conditional. When you use expressions in an if conditional, you may omit the expression syntax ( ${{ }} ) because the orchestrator automatically evaluates the if conditional as an expression. For more information, see \u201c Context and expression syntax for OpenTestFactory workflows .\u201d","title":"jobs.&lt;job_id&gt;.if"},{"location":"reference/workflows/#jobsjob_idgenerator","text":"A generator creates a series of jobs that replace the current job. Some generators require inputs that you must set using the with keyword. Review the generator\u2019s README file to determine the inputs required.","title":"jobs.&lt;job_id&gt;.generator"},{"location":"reference/workflows/#jobsjob_idwith","text":"A map of the input parameters defined by the generator plugin. Each input parameter is a key/value pair.","title":"jobs.&lt;job_id&gt;.with"},{"location":"reference/workflows/#example_7","text":"Defines the three input parameters ( first_name , middle_name , and last_name ) defined by the hello_world generator. jobs : generator : yada@hello_world@v1 with : first_name : Mona middle_name : The last_name : Octocat","title":"Example"},{"location":"reference/workflows/#jobsjob_idsteps","text":"A job contains a sequence of tasks called steps . Steps can run commands, run setup tasks, or run an action. Not all steps run actions, but all actions run as a step. Each step runs in its own process in the execution environment and has access to the workspace and filesystem. Because steps run in their own process, changes to environment variables are not preserved between steps. The orchestrator provides built-in steps to set up and complete a job. You can run an unlimited number of steps as long as you are within the workflow usage limits. For more information, see \u201c Usage limits .\u201d","title":"jobs.&lt;job_id&gt;.steps"},{"location":"reference/workflows/#example_8","text":"metadata : name : Greeting from Mona jobs : my-job : name : My Job runs-on : ubuntu-latest steps : - name : Print a greeting variables : MY_VAR : Hi there! My name is FIRST_NAME : Mona MIDDLE_NAME : The LAST_NAME : Octocat run : | echo $MY_VAR $FIRST_NAME $MIDDLE_NAME $LAST_NAME.","title":"Example"},{"location":"reference/workflows/#jobsjob_idstepsid","text":"A unique identifier for the step. You can use the id to reference the step in contexts. For more information, see \u201c Context and expression syntax for OpenTestFactory workflows .\u201d","title":"jobs.&lt;job_id&gt;.steps.id"},{"location":"reference/workflows/#jobsjob_idstepsif","text":"You can use the if conditional to prevent a step from running unless a condition is met. You can use any supported context and expression to create a conditional. When you use expressions in an if conditional, you may omit the expression syntax ( ${{ }} ) because the orchestrator automatically evaluates the if conditional as an expression. For more information, see \u201c Context and expression syntax for OpenTestFactory workflows .\u201d","title":"jobs.&lt;job_id&gt;.steps.if"},{"location":"reference/workflows/#example-using-contexts","text":"This step only runs when the event type is a pull_request and the event action is unassigned. steps : - name : My first step if : ${{ github.event_name == 'pull_request' && github.event.action == 'unassigned' }} run : echo This event is a pull request that had an assignee removed.","title":"Example using contexts"},{"location":"reference/workflows/#example-using-status-check-functions","text":"The my backup step only runs when the previous step of a job fails. For more information, see \u201c Context and expression syntax for OpenTestFactory workflows .\u201d steps : - name : My first step uses : monacorp/action-name@v1 - name : My backup step if : ${{ failure() }} uses : actions/heroku@v2","title":"Example using status check functions"},{"location":"reference/workflows/#jobsjob_idstepsname","text":"A name for your step to display on the orchestrator.","title":"jobs.&lt;job_id&gt;.steps.name"},{"location":"reference/workflows/#jobsjob_idstepsuses","text":"Selects a provider to run as part of a step in your job. A provider is a reusable unit of code. We strongly recommend that you include the version of the provider you are using. If you do not specify a version, it could break your workflows or cause unexpected behavior when the provider owner publishes an update. Using the specific major provider version allows you to receive critical fixes and security patches while still maintaining compatibility. It also assures that your workflow should still work. Using the master branch of a provider may be convenient, but if someone releases a new major version with a breaking change, your workflow could break. Some providers require inputs that you must set using the with keyword. Review the provider\u2019s README file to determine the inputs required. Providers are Python files. For more details, see runs-on .","title":"jobs.&lt;job_id&gt;.steps.uses"},{"location":"reference/workflows/#example-using-versioned-providers","text":"steps : # Reference the major version of a release - uses : actions/setup-node@v1 # Reference a minor version of a release - uses : actions/setup-node@v1.2","title":"Example using versioned providers"},{"location":"reference/workflows/#jobsjob_idstepsrun","text":"Runs command-line programs using the execution environment\u2019s shell. If you do not provide a name , the step name will default to the text specified in the run command. Commands run using non-login shells by default. You can choose a different shell and customize the shell used to run commands. For more information, see \u201cUsing a specific shell.\u201d Each run keyword represents a new process and shell in the execution environment. When you provide multi-line commands, each line runs in the same shell. For example: A single-line command: - name : Install Dependencies run : npm install A multi-line command: - name : Clean install dependencies and build run : | npm ci npm run build Using the working-directory keyword, you can specify the working directory of where to run the command. - name : Clean temp directory run : rm -rf * working-directory : ./temp","title":"jobs.&lt;job_id&gt;.steps.run"},{"location":"reference/workflows/#using-a-specific-shell","text":"You can override the default shell settings in the execution environment using the shell keyword. You can use built-in shell keywords, or you can define a custom set of shell options. Supported platform shell parameter Description Command run internally All bash The default shell on non-Windows platforms with a fallback to sh . When specifying a bash shell on Windows, the bash shell included with Git for Windows is used. bash \u2013noprofile \u2013norc -eo pipefail All python Executes the python command. python Windows cmd The orchestrator appends the extension .cmd to your script name and substitutes for {0} . %ComSpec% /D /E:ON /V:OFF /S /C \u201cCALL \u201c{0}\u201d\u201c.","title":"Using a specific shell"},{"location":"reference/workflows/#example-running-a-script-using-bash","text":"steps : - name : Display the path run : echo $PATH shell : bash","title":"Example running a script using bash"},{"location":"reference/workflows/#example-running-a-script-using-windows-cmd","text":"steps : - name : Display the path run : echo %PATH% shell : cmd","title":"Example running a script using Windows cmd"},{"location":"reference/workflows/#example-running-a-python-script","text":"steps : - name : Display the path run : | import os print(os.environ['PATH']) shell : python","title":"Example running a python script"},{"location":"reference/workflows/#custom-shell","text":"You can set the shell value to a template string using command [options...] {0} [more_options...] The orchestrator interprets the first whitespace-delimited word of the string as the command and inserts the file name for the temporary script at {0} .","title":"Custom shell"},{"location":"reference/workflows/#exit-codes-and-error-action-preference","text":"For built-in shell keywords, we provide the following defaults that are executed by the execution environments. You should use these guidelines when running shell scripts. bash / sh : Fail-fast behavior using set -e o pipefail : Default for bash and built-in shell. It is also the default when you do not provide an option on non-Windows platforms. You can opt out of fail-fast and take full control by providing a template string to the shell options. For example, bash {0} . sh-like shells exit with the exit code of the last command executed in a script, which is also the default behavior for actions. The execution environment will report the status of the step as fail/succeed based on this exit code. cmd There does not seem to be a way to fully opt into fail-fast behavior other than writing your script to check each error code and respond accordingly. Because we can\u2019t actually provide that behavior by default, you need to write this behavior into your script. cmd.exe will exit with the error level of the last program it executed, and it will return the error code to the execution environment.","title":"Exit codes and error action preference"},{"location":"reference/workflows/#jobsjob_idstepswith","text":"A map of the input parameters defined by the action. Each input parameter is a key/value pair. Input parameters are set as environment variables. The variable is prefixed with INPUT_ and converted to upper case.","title":"jobs.&lt;job_id&gt;.steps.with"},{"location":"reference/workflows/#example_9","text":"Defines the three input parameters ( first_name , middle_name , and last_name ) defined by the hello_world action. These input variables will be accessible to the hello-world action as INPUT_FIRST_NAME , INPUT_MIDDLE_NAME , and INPUT_LAST_NAME environment variables. jobs : my_first_job : steps : - name : My first step uses : actions/hello_world@master with : first_name : Mona middle_name : The last_name : Octocat","title":"Example"},{"location":"reference/workflows/#jobsjob_idstepsvariables","text":"Sets environment variables for steps to use in the execution environment. You can also set environment variables for the entire workflow or a job. For more information, see variables and jobs.<job_id>.variables . When more than one environment variable is defined with the same name, the orchestrator uses the most specific environment variable. For example, an environment variable defined in a step will override job and workflow variables with the same name, while the step executes. A variable defined for a job will override a workflow variable with the same name, while the job executes. Public actions may specify expected environment variables in the README file. If you are setting a secret in an environment variable, you must set secrets using the secrets context. For more information, see \u201c Using environment variables \u201d and \u201c Context and expression syntax for OpenTestFactory workflows .\u201d","title":"jobs.&lt;job_id&gt;.steps.variables"},{"location":"reference/workflows/#example_10","text":"steps : - name : My first action variables : GITHUB_TOKEN : ${{ secrets.GITHUB_TOKEN }} FIRST_NAME : Mona LAST_NAME : Octocat","title":"Example"},{"location":"reference/workflows/#jobsjob_idstepscontinue-on-error","text":"Prevents a job from failing when a step fails. Set to true to allow a job to pass when this step fails.","title":"jobs.&lt;job_id&gt;.steps.continue-on-error"},{"location":"reference/workflows/#jobsjob_idstepstimeout-minutes","text":"The maximum number of minutes to run the step before killing the process.","title":"jobs.&lt;job_id&gt;.steps.timeout-minutes"},{"location":"reference/workflows/#jobsjob_idtimeout-minutes","text":"The maximum number of minutes to let a job run before the orchestrator automatically cancels it. Default: 360","title":"jobs.&lt;job_id&gt;.timeout-minutes"},{"location":"reference/workflows/#jobsjob_idcontinue-on-error","text":"Prevents a workflow run from failing when a job fails. Set to true to allow a workflow run to pass when this job fails.","title":"jobs.&lt;job_id&gt;.continue-on-error"},{"location":"services/actionprovider/","text":"ActionProvider Service \u00b6 Handles the actions/... elements in uses step statements. Configuration \u00b6 This module has a configuration file ( actionprovider.yaml by default) that describes the host , port , ssl_context , trusted_authorities , and logfile to use. It can also enable insecure logins. If no configuration file is found it will default to the following values: apiVersion : opentestfactory.org/v1alpha1 kind : ServiceConfig current-context : default contexts : - context : port : 443 host : 127.0.0.1 ssl_context : adhoc eventbus : endpoint : https://127.0.0.1:38368 token : invalid name : default ssl_context is either adhoc , a list of two items (certificate file path and private key file path), or disabled (not recommended, will switch to plain HTTP). A context can also contain a trusted_authorities , which is a list of public key files, used for token validation. A context can also allow for insecure (token-less) logins, if enable_insecure_login is set to true (by default, insecure logins are disabled). Insecure logins, if enabled, are only allowed from a given address ( 127.0.0.1 by default). This can be overridden by specifying insecure_bind_address . Usage \u00b6 python3 -m squashtf.core.actionprovider [ --context context ] [ --config configfile ] Endpoints \u00b6 This module exposes one endpoint: /inbox (POST) Whenever calling this endpoint, a signed token must be specified via the Authorization header. This header will be of form: Authorization : Bearer xxxxxxxx It must be signed with one of the trusted authorities specified in the current context.","title":"Action Provider service"},{"location":"services/actionprovider/#actionprovider-service","text":"Handles the actions/... elements in uses step statements.","title":"ActionProvider Service"},{"location":"services/actionprovider/#configuration","text":"This module has a configuration file ( actionprovider.yaml by default) that describes the host , port , ssl_context , trusted_authorities , and logfile to use. It can also enable insecure logins. If no configuration file is found it will default to the following values: apiVersion : opentestfactory.org/v1alpha1 kind : ServiceConfig current-context : default contexts : - context : port : 443 host : 127.0.0.1 ssl_context : adhoc eventbus : endpoint : https://127.0.0.1:38368 token : invalid name : default ssl_context is either adhoc , a list of two items (certificate file path and private key file path), or disabled (not recommended, will switch to plain HTTP). A context can also contain a trusted_authorities , which is a list of public key files, used for token validation. A context can also allow for insecure (token-less) logins, if enable_insecure_login is set to true (by default, insecure logins are disabled). Insecure logins, if enabled, are only allowed from a given address ( 127.0.0.1 by default). This can be overridden by specifying insecure_bind_address .","title":"Configuration"},{"location":"services/actionprovider/#usage","text":"python3 -m squashtf.core.actionprovider [ --context context ] [ --config configfile ]","title":"Usage"},{"location":"services/actionprovider/#endpoints","text":"This module exposes one endpoint: /inbox (POST) Whenever calling this endpoint, a signed token must be specified via the Authorization header. This header will be of form: Authorization : Bearer xxxxxxxx It must be signed with one of the trusted authorities specified in the current context.","title":"Endpoints"},{"location":"services/arranger/","text":"Arranger Service \u00b6 Configuration \u00b6 This module has a configuration file ( arranger.yaml by default) that describes the host , port , ssl_context , trusted_authorities , and logfile to use. It can also enable insecure logins. If no configuration file is found it will default to the following values: apiVersion : opentestfactory.org/v1alpha1 kind : ServiceConfig current-context : default contexts : - context : port : 443 host : 127.0.0.1 ssl_context : adhoc eventbus : endpoint : https://127.0.0.1:38368 token : invalid name : default ssl_context is either adhoc , a list of two items (certificate file path and private key file path), or disabled (not recommended, will switch to plain HTTP). A context can also contain a trusted_authorities , which is a list of public key files, used for token validation. A context can also allow for insecure (token-less) logins, if enable_insecure_login is set to true (by default, insecure logins are disabled). Insecure logins, if enabled, are only allowed from a given address ( 127.0.0.1 by default). This can be overridden by specifying insecure_bind_address . Usage \u00b6 python3 -m squashtf.core.arranger [ --context context ] [ --config configfile ] Endpoints \u00b6 This module exposes one endpoint: /inbox (POST) Whenever calling this endpoint, a signed token must be specified via the Authorization header. This header will be of form: Authorization : Bearer xxxxxxxx It must be signed with one of the trusted authorities specified in the current context.","title":"Arranger service"},{"location":"services/arranger/#arranger-service","text":"","title":"Arranger Service"},{"location":"services/arranger/#configuration","text":"This module has a configuration file ( arranger.yaml by default) that describes the host , port , ssl_context , trusted_authorities , and logfile to use. It can also enable insecure logins. If no configuration file is found it will default to the following values: apiVersion : opentestfactory.org/v1alpha1 kind : ServiceConfig current-context : default contexts : - context : port : 443 host : 127.0.0.1 ssl_context : adhoc eventbus : endpoint : https://127.0.0.1:38368 token : invalid name : default ssl_context is either adhoc , a list of two items (certificate file path and private key file path), or disabled (not recommended, will switch to plain HTTP). A context can also contain a trusted_authorities , which is a list of public key files, used for token validation. A context can also allow for insecure (token-less) logins, if enable_insecure_login is set to true (by default, insecure logins are disabled). Insecure logins, if enabled, are only allowed from a given address ( 127.0.0.1 by default). This can be overridden by specifying insecure_bind_address .","title":"Configuration"},{"location":"services/arranger/#usage","text":"python3 -m squashtf.core.arranger [ --context context ] [ --config configfile ]","title":"Usage"},{"location":"services/arranger/#endpoints","text":"This module exposes one endpoint: /inbox (POST) Whenever calling this endpoint, a signed token must be specified via the Authorization header. This header will be of form: Authorization : Bearer xxxxxxxx It must be signed with one of the trusted authorities specified in the current context.","title":"Endpoints"},{"location":"services/eventbus/","text":"EventBus Service \u00b6 Can work as a standalone service, but may be replaced by a facade to an existing eventbus/mq manager. Configuration \u00b6 This module has a configuration file ( eventbus.yaml by default) that describes the host , port , ssl_context , trusted_authorities , and logfile to use. It can also enable insecure logins. If no configuration file is found it will default to the following values: apiVersion : opentestfactory.org/v1alpha1 kind : EventBusConfig current-context : default contexts : - context : port : 38368 host : 127.0.0.1 ssl_context : adhoc name : default ssl_context is either adhoc , a list of two items (certificate file path and private key file path), or disabled (not recommended, will switch to plain HTTP). A context can also contain a trusted_authorities , which is a list of public key files, used for token validation. A context can also allow for insecure (token-less) logins, if enable_insecure_login is set to true (by default, insecure logins are disabled). Insecure logins, if enabled, are only allowed from a given address ( 127.0.0.1 by default). This can be overridden by specifying insecure_bind_address . Usage \u00b6 python3 -m squashtf.core.eventbus [ --context context ] [ --config configfile ] Endpoints \u00b6 This module exposes three endpoints: /subscriptions (GET, POST) /subscriptions/{uuid} (DELETE) /publications (POST) Whenever calling those endpoints, a signed token must be specified via the Authorization header. This header will be of form: Authorization : Bearer xxxxxxxx It must be signed with one of the trusted authorities specified in the current context. On successful subscription a subscription ID is returned (in .details.uuid ). On successful publication, the publication is dispatched to all corresponding subscriptions, asynchronously. If there is no corresponding subscription, the response code will be 200, but its message part will be: 'Publication received, but no matching subscription' If there are corresponding subscriptions, the publication will be posted to their endpoints. There will be an X-Subscription-ID header containing the subscription ID: X-Subscription-ID: uuid There will also be a X-Publication-ID header containing the publication ID, to help disambiguate duplicates: X-Publication-ID: uuid No attempt is made to resubmit a publication to a given subscription if the first attempt failed. A failure to submit a publication to a given subscription has no effect on other subscriptions: they will receive the publication.","title":"Event Bus"},{"location":"services/eventbus/#eventbus-service","text":"Can work as a standalone service, but may be replaced by a facade to an existing eventbus/mq manager.","title":"EventBus Service"},{"location":"services/eventbus/#configuration","text":"This module has a configuration file ( eventbus.yaml by default) that describes the host , port , ssl_context , trusted_authorities , and logfile to use. It can also enable insecure logins. If no configuration file is found it will default to the following values: apiVersion : opentestfactory.org/v1alpha1 kind : EventBusConfig current-context : default contexts : - context : port : 38368 host : 127.0.0.1 ssl_context : adhoc name : default ssl_context is either adhoc , a list of two items (certificate file path and private key file path), or disabled (not recommended, will switch to plain HTTP). A context can also contain a trusted_authorities , which is a list of public key files, used for token validation. A context can also allow for insecure (token-less) logins, if enable_insecure_login is set to true (by default, insecure logins are disabled). Insecure logins, if enabled, are only allowed from a given address ( 127.0.0.1 by default). This can be overridden by specifying insecure_bind_address .","title":"Configuration"},{"location":"services/eventbus/#usage","text":"python3 -m squashtf.core.eventbus [ --context context ] [ --config configfile ]","title":"Usage"},{"location":"services/eventbus/#endpoints","text":"This module exposes three endpoints: /subscriptions (GET, POST) /subscriptions/{uuid} (DELETE) /publications (POST) Whenever calling those endpoints, a signed token must be specified via the Authorization header. This header will be of form: Authorization : Bearer xxxxxxxx It must be signed with one of the trusted authorities specified in the current context. On successful subscription a subscription ID is returned (in .details.uuid ). On successful publication, the publication is dispatched to all corresponding subscriptions, asynchronously. If there is no corresponding subscription, the response code will be 200, but its message part will be: 'Publication received, but no matching subscription' If there are corresponding subscriptions, the publication will be posted to their endpoints. There will be an X-Subscription-ID header containing the subscription ID: X-Subscription-ID: uuid There will also be a X-Publication-ID header containing the publication ID, to help disambiguate duplicates: X-Publication-ID: uuid No attempt is made to resubmit a publication to a given subscription if the first attempt failed. A failure to submit a publication to a given subscription has no effect on other subscriptions: they will receive the publication.","title":"Endpoints"},{"location":"services/killswitch/","text":"Killswitch Service \u00b6 A killswitch service can be used to cancel a currently executing workflow . Due to the asynchronous nature of workflow executions, some actions may still occur on execution environments and SUT when a workflow is canceled. Configuration \u00b6 This module has a configuration file ( killswitch.yaml by default) that describes the host , port , ssl_context , trusted_authorities , and logfile to use. It can also enable insecure logins. If no configuration file is found it will default to the following values: apiVersion : opentestfactory.org/v1alpha1 kind : SSHServiceConfig current-context : default contexts : - context : port : 443 host : 127.0.0.1 ssl_context : adhoc eventbus : endpoint : https://127.0.0.1:38368 token : invalid name : default ssl_context is either adhoc , a list of two items (certificate file path and private key file path), or disabled (not recommended, will switch to plain HTTP). A context can also contain a trusted_authorities , which is a list of public key files, used for token validation. A context can also allow for insecure (token-less) logins, if enable_insecure_login is set to true (by default, insecure logins are disabled). Insecure logins, if enabled, are only allowed from a given address ( 127.0.0.1 by default). This can be overridden by specifying insecure_bind_address . Usage \u00b6 The following command starts the killswitch service: python3 -m squashtf.core.killswitch [ --context context ] [ --config configfile ] Endpoints \u00b6 This module exposes one endpoint: /workflows/{uuid} (DELETE) Whenever calling this endpoint, a signed token must be specified via the Authorization header. This header will be of form: Authorization : Bearer xxxxxxxx It must be signed with one of the trusted authorities specified in the current context.","title":"Killswitch service"},{"location":"services/killswitch/#killswitch-service","text":"A killswitch service can be used to cancel a currently executing workflow . Due to the asynchronous nature of workflow executions, some actions may still occur on execution environments and SUT when a workflow is canceled.","title":"Killswitch Service"},{"location":"services/killswitch/#configuration","text":"This module has a configuration file ( killswitch.yaml by default) that describes the host , port , ssl_context , trusted_authorities , and logfile to use. It can also enable insecure logins. If no configuration file is found it will default to the following values: apiVersion : opentestfactory.org/v1alpha1 kind : SSHServiceConfig current-context : default contexts : - context : port : 443 host : 127.0.0.1 ssl_context : adhoc eventbus : endpoint : https://127.0.0.1:38368 token : invalid name : default ssl_context is either adhoc , a list of two items (certificate file path and private key file path), or disabled (not recommended, will switch to plain HTTP). A context can also contain a trusted_authorities , which is a list of public key files, used for token validation. A context can also allow for insecure (token-less) logins, if enable_insecure_login is set to true (by default, insecure logins are disabled). Insecure logins, if enabled, are only allowed from a given address ( 127.0.0.1 by default). This can be overridden by specifying insecure_bind_address .","title":"Configuration"},{"location":"services/killswitch/#usage","text":"The following command starts the killswitch service: python3 -m squashtf.core.killswitch [ --context context ] [ --config configfile ]","title":"Usage"},{"location":"services/killswitch/#endpoints","text":"This module exposes one endpoint: /workflows/{uuid} (DELETE) Whenever calling this endpoint, a signed token must be specified via the Authorization header. This header will be of form: Authorization : Bearer xxxxxxxx It must be signed with one of the trusted authorities specified in the current context.","title":"Endpoints"},{"location":"services/observer/","text":"Observer Service \u00b6 A service that is used to observe the progress of a workflow . Configuration \u00b6 This module has a configuration file ( observer.yaml by default) that describes the host , port , ssl_context , trusted_authorities , and logfile to use. It can also enable insecure logins. If no configuration file is found it will default to the following values: apiVersion : opentestfactory.org/v1alpha1 kind : ServiceConfig current-context : default contexts : - context : port : 443 host : 127.0.0.1 ssl_context : adhoc eventbus : endpoint : https://127.0.0.1:38368 token : invalid name : default ssl_context is either adhoc , a list of two items (certificate file path and private key file path), or disabled (not recommended, will switch to plain HTTP). A context can also contain a trusted_authorities , which is a list of public key files, used for token validation. A context can also allow for insecure (token-less) logins, if enable_insecure_login is set to true (by default, insecure logins are disabled). Insecure logins, if enabled, are only allowed from a given address ( 127.0.0.1 by default). This can be overridden by specifying insecure_bind_address . Usage \u00b6 python3 -m squashtf.core.observer [ --context context ] [ --config configfile ] Endpoints \u00b6 This module exposes two endpoints: /workflow/{workflow_id}/status (GET) /inbox (POST) Whenever calling those endpoints, a signed token must be specified via the Authorization header. This header will be of form: Authorization : Bearer xxxxxxxx It must be signed with one of the trusted authorities specified in the current context.","title":"Observer service"},{"location":"services/observer/#observer-service","text":"A service that is used to observe the progress of a workflow .","title":"Observer Service"},{"location":"services/observer/#configuration","text":"This module has a configuration file ( observer.yaml by default) that describes the host , port , ssl_context , trusted_authorities , and logfile to use. It can also enable insecure logins. If no configuration file is found it will default to the following values: apiVersion : opentestfactory.org/v1alpha1 kind : ServiceConfig current-context : default contexts : - context : port : 443 host : 127.0.0.1 ssl_context : adhoc eventbus : endpoint : https://127.0.0.1:38368 token : invalid name : default ssl_context is either adhoc , a list of two items (certificate file path and private key file path), or disabled (not recommended, will switch to plain HTTP). A context can also contain a trusted_authorities , which is a list of public key files, used for token validation. A context can also allow for insecure (token-less) logins, if enable_insecure_login is set to true (by default, insecure logins are disabled). Insecure logins, if enabled, are only allowed from a given address ( 127.0.0.1 by default). This can be overridden by specifying insecure_bind_address .","title":"Configuration"},{"location":"services/observer/#usage","text":"python3 -m squashtf.core.observer [ --context context ] [ --config configfile ]","title":"Usage"},{"location":"services/observer/#endpoints","text":"This module exposes two endpoints: /workflow/{workflow_id}/status (GET) /inbox (POST) Whenever calling those endpoints, a signed token must be specified via the Authorization header. This header will be of form: Authorization : Bearer xxxxxxxx It must be signed with one of the trusted authorities specified in the current context.","title":"Endpoints"},{"location":"services/receptionist/","text":"Receptionist Service \u00b6 A receptionist service is what is used to start a workflow . Configuration \u00b6 This module has a configuration file ( receptionist.yaml by default) that describes the host , port , ssl_context , trusted_authorities , and logfile to use. It can also enable insecure logins. If no configuration file is found it will default to the following values: apiVersion : opentestfactory.org/v1alpha1 kind : ServiceConfig current-context : default contexts : - context : port : 7774 host : 127.0.0.1 ssl_context : adhoc eventbus : endpoint : https://127.0.0.1:38368 token : invalid name : default ssl_context is either adhoc , a list of two items (certificate file path and private key file path), or disabled (not recommended, will switch to plain HTTP). A context can also contain a trusted_authorities , which is a list of public key files, used for token validation. A context can also allow for insecure (token-less) logins, if enable_insecure_login is set to true (by default, insecure logins are disabled). Insecure logins, if enabled, are only allowed from a given address ( 127.0.0.1 by default). This can be overridden by specifying insecure_bind_address . Usage \u00b6 The following command starts the receptionist service: python3 -m squashtf.core.receptionist [ --context context ] [ --config configfile ] Endpoints \u00b6 This module exposes one endpoint: /workflows (POST) Whenever calling this endpoint, a signed token must be specified via the Authorization header. This header will be of form: Authorization : Bearer xxxxxxxx It must be signed with one of the trusted authorities specified in the current context. On success, a workflow ID is returned (in details.workflow_id ).","title":"Receptionist service"},{"location":"services/receptionist/#receptionist-service","text":"A receptionist service is what is used to start a workflow .","title":"Receptionist Service"},{"location":"services/receptionist/#configuration","text":"This module has a configuration file ( receptionist.yaml by default) that describes the host , port , ssl_context , trusted_authorities , and logfile to use. It can also enable insecure logins. If no configuration file is found it will default to the following values: apiVersion : opentestfactory.org/v1alpha1 kind : ServiceConfig current-context : default contexts : - context : port : 7774 host : 127.0.0.1 ssl_context : adhoc eventbus : endpoint : https://127.0.0.1:38368 token : invalid name : default ssl_context is either adhoc , a list of two items (certificate file path and private key file path), or disabled (not recommended, will switch to plain HTTP). A context can also contain a trusted_authorities , which is a list of public key files, used for token validation. A context can also allow for insecure (token-less) logins, if enable_insecure_login is set to true (by default, insecure logins are disabled). Insecure logins, if enabled, are only allowed from a given address ( 127.0.0.1 by default). This can be overridden by specifying insecure_bind_address .","title":"Configuration"},{"location":"services/receptionist/#usage","text":"The following command starts the receptionist service: python3 -m squashtf.core.receptionist [ --context context ] [ --config configfile ]","title":"Usage"},{"location":"services/receptionist/#endpoints","text":"This module exposes one endpoint: /workflows (POST) Whenever calling this endpoint, a signed token must be specified via the Authorization header. This header will be of form: Authorization : Bearer xxxxxxxx It must be signed with one of the trusted authorities specified in the current context. On success, a workflow ID is returned (in details.workflow_id ).","title":"Endpoints"},{"location":"services/sshee/","text":"SSH channel plugin \u00b6 This channel plugin manages communications with execution environments that are accessed via SSH. Configuration \u00b6 This plugin has a configuration file ( sshee.yaml by default) that describes the host , port , ssl_context , trusted_authorities , and logfile to use. It can also enable insecure logins. The configuration file also specifies pools . If no configuration file is found it will default to the following values: apiVersion : opentestfactory.org/v1alpha1 kind : SSHServiceConfig current-context : default contexts : - name : default context : port : 443 host : 127.0.0.1 ssl_context : adhoc eventbus : endpoint : https://127.0.0.1:38368 token : invalid-token targets : [] pools : Contexts \u00b6 contexts lists the available contexts . Contexts have a name and a definition, context . Each context is described by the following items: host (required): the host the service will bind to (a hostname or an IP address). port (required): the port the service will listen to (a number). ssl_context (required): either adhoc , a list of two items (certificate file path and private key file path), or disabled (not recommended, will switch to plain HTTP). trusted_authorities (optional): a list of public key files and/or directories containing public key files, used for token validation. enable_insecure_login (optional): allow for insecure (token-less) logins, if set to true (by default, insecure logins are disabled). insecure_bind_address (optional, only used if enable_insecure_login is set to true ): insecure logins, if enabled, are only allowed from a given address ( 127.0.0.1 by default). eventbus (required): the event bus to use. A dictionary with two required entries, endpoint and token , and an optional entry, insecure-skip-tls-verify . targets (required): a list of targets which are groups specified in pools . Current context \u00b6 current-context specifies which context entry to use. There can be any number of contexts specified in the configuration file, but there must be one and only one current context. The current context must be in the list of contexts. Pools \u00b6 pools lists the available groups of execution environments. Each group (a pool ) describes one or more execution environment. Each execution environment is described by the following (possibly optional) items: Either host or hosts : a host name (a string) or a list of host names (a list of strings), required. username : a username on the execution environment (a string), required. tags : what capabilities it provides (a list of strings), required. Amongst those capabilities, there must be one and only one of linux , macos , or windows . If that is not the case, the execution environment will be ignored. port : a port (a number), 22 by default. script_path : where to put temporary scripts (a string), '/tmp' by default for linux and macos execution environments, %TEMP% for windows environments. key_filename : a file path (a string), required if using key authentification. passphrase : a passphrase (a string), required if using key authentification and key has a passphrase. password : a password (a string), required if using password authentification. ssh_host_keys : a file path (a string), required if there is a list of allowed incoming hosts (typically specified if using the default missing host key policy, which is to reject unknown incoming connections). missing_host_key_policy : a string, either auto-add or reject ( reject by default). You cannot specify both key_filename and password at the same time. Execution environments with an incorrect specification are ignored. Example \u00b6 A SSH channel requires targets and pools entries in order to prepare and dispatch execution commands. In the following example the current context, default , can handle execution requests with a runs-on statement that is either ssh , linux , [ssh, linux] , or [linux, ssh] . It cannot handle execution requests with other tags. It cannot handle runs-on statements that are either windows , [ssh, windows] , or [windows, ssh] due to the fact that the demo_target group is not specified in the context\u2019s targets. apiVersion : opentestfactory.org/v1alpha1 kind : SSHServiceConfig current-context : default contexts : - name : default context : port : 443 host : 127.0.0.1 ssl_context : adhoc logfile : sshee.log eventbus : endpoint : https://127.0.0.1:38368 token : invalid-token targets : [ ssh_targets , ssh2_targets ] pools : demo_target : - host : demo.example.com username : demo password : 1234 tags : [ ssh , windows ] ssh_targets : - host : dummy.example.com username : user password : secret tags : [ ssh , linux ] ssh2_targets : - host : host.example.com port : 22 username : alice ssh_host_keys : /data/ssh/known_hosts key_filename : /data/ssh/example.pem missing_host_key_policy : reject tags : [ ssh , linux ] - hosts : [ foo.example.com , bar.example.com ] port : 22 username : bob ssh_host_keys : /data/ssh/known_hosts key_filename : /data/ssh/secret.pem passphrase : secret missing_host_key_policy : auto-add tags : [ ssh , linux ] Usage \u00b6 python3 -m squashtf.plugins.sshee [ --context context ] [ --config configfile ] Endpoints \u00b6 This module exposes one endpoint: /inbox (POST) Whenever calling this endpoint, a signed token must be specified via the Authorization header. This header will be of form: Authorization : Bearer xxxxxxxx It must be signed with one of the trusted authorities specified in the current context.","title":"SSH Channel service"},{"location":"services/sshee/#ssh-channel-plugin","text":"This channel plugin manages communications with execution environments that are accessed via SSH.","title":"SSH channel plugin"},{"location":"services/sshee/#configuration","text":"This plugin has a configuration file ( sshee.yaml by default) that describes the host , port , ssl_context , trusted_authorities , and logfile to use. It can also enable insecure logins. The configuration file also specifies pools . If no configuration file is found it will default to the following values: apiVersion : opentestfactory.org/v1alpha1 kind : SSHServiceConfig current-context : default contexts : - name : default context : port : 443 host : 127.0.0.1 ssl_context : adhoc eventbus : endpoint : https://127.0.0.1:38368 token : invalid-token targets : [] pools :","title":"Configuration"},{"location":"services/sshee/#contexts","text":"contexts lists the available contexts . Contexts have a name and a definition, context . Each context is described by the following items: host (required): the host the service will bind to (a hostname or an IP address). port (required): the port the service will listen to (a number). ssl_context (required): either adhoc , a list of two items (certificate file path and private key file path), or disabled (not recommended, will switch to plain HTTP). trusted_authorities (optional): a list of public key files and/or directories containing public key files, used for token validation. enable_insecure_login (optional): allow for insecure (token-less) logins, if set to true (by default, insecure logins are disabled). insecure_bind_address (optional, only used if enable_insecure_login is set to true ): insecure logins, if enabled, are only allowed from a given address ( 127.0.0.1 by default). eventbus (required): the event bus to use. A dictionary with two required entries, endpoint and token , and an optional entry, insecure-skip-tls-verify . targets (required): a list of targets which are groups specified in pools .","title":"Contexts"},{"location":"services/sshee/#current-context","text":"current-context specifies which context entry to use. There can be any number of contexts specified in the configuration file, but there must be one and only one current context. The current context must be in the list of contexts.","title":"Current context"},{"location":"services/sshee/#pools","text":"pools lists the available groups of execution environments. Each group (a pool ) describes one or more execution environment. Each execution environment is described by the following (possibly optional) items: Either host or hosts : a host name (a string) or a list of host names (a list of strings), required. username : a username on the execution environment (a string), required. tags : what capabilities it provides (a list of strings), required. Amongst those capabilities, there must be one and only one of linux , macos , or windows . If that is not the case, the execution environment will be ignored. port : a port (a number), 22 by default. script_path : where to put temporary scripts (a string), '/tmp' by default for linux and macos execution environments, %TEMP% for windows environments. key_filename : a file path (a string), required if using key authentification. passphrase : a passphrase (a string), required if using key authentification and key has a passphrase. password : a password (a string), required if using password authentification. ssh_host_keys : a file path (a string), required if there is a list of allowed incoming hosts (typically specified if using the default missing host key policy, which is to reject unknown incoming connections). missing_host_key_policy : a string, either auto-add or reject ( reject by default). You cannot specify both key_filename and password at the same time. Execution environments with an incorrect specification are ignored.","title":"Pools"},{"location":"services/sshee/#example","text":"A SSH channel requires targets and pools entries in order to prepare and dispatch execution commands. In the following example the current context, default , can handle execution requests with a runs-on statement that is either ssh , linux , [ssh, linux] , or [linux, ssh] . It cannot handle execution requests with other tags. It cannot handle runs-on statements that are either windows , [ssh, windows] , or [windows, ssh] due to the fact that the demo_target group is not specified in the context\u2019s targets. apiVersion : opentestfactory.org/v1alpha1 kind : SSHServiceConfig current-context : default contexts : - name : default context : port : 443 host : 127.0.0.1 ssl_context : adhoc logfile : sshee.log eventbus : endpoint : https://127.0.0.1:38368 token : invalid-token targets : [ ssh_targets , ssh2_targets ] pools : demo_target : - host : demo.example.com username : demo password : 1234 tags : [ ssh , windows ] ssh_targets : - host : dummy.example.com username : user password : secret tags : [ ssh , linux ] ssh2_targets : - host : host.example.com port : 22 username : alice ssh_host_keys : /data/ssh/known_hosts key_filename : /data/ssh/example.pem missing_host_key_policy : reject tags : [ ssh , linux ] - hosts : [ foo.example.com , bar.example.com ] port : 22 username : bob ssh_host_keys : /data/ssh/known_hosts key_filename : /data/ssh/secret.pem passphrase : secret missing_host_key_policy : auto-add tags : [ ssh , linux ]","title":"Example"},{"location":"services/sshee/#usage","text":"python3 -m squashtf.plugins.sshee [ --context context ] [ --config configfile ]","title":"Usage"},{"location":"services/sshee/#endpoints","text":"This module exposes one endpoint: /inbox (POST) Whenever calling this endpoint, a signed token must be specified via the Authorization header. This header will be of form: Authorization : Bearer xxxxxxxx It must be signed with one of the trusted authorities specified in the current context.","title":"Endpoints"}]}